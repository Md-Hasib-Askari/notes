## âœ… 7.5 **Evaluation Frameworks for AI Agents**

These tools are purpose-built to support **LLM application development**, **prompt testing**, and **performance monitoring** with reproducibility and visibility in mind.

---

### ðŸ”¹ 1. **LangSmith** (by LangChain)

> ðŸ§  Best for: Observability and debugging of LangChain apps (also works outside LangChain)

#### âœ… Key Features:

* **Prompt playgrounds** and prompt versioning
* **Trace visualizer**: See step-by-step agent/tool interactions
* **Evaluation runs** with comparison support
* **Integration with LangChain, OpenAI, and manual logging**

#### ðŸ”§ Example Use Cases:

* Debug long agent chains
* Analyze token usage per step
* View function calls in your agent loop

#### ðŸ› ï¸ Setup:

```bash
pip install langsmith
```

```python
from langsmith.run_helpers import traceable

@traceable
def my_tool(input: str):
    return "Processed: " + input
```

> Use it with LangChainâ€™s built-in `evaluate_chain()` to automatically log prompt output and test metrics.

---

### ðŸ”¹ 2. **Ragas** (by Exploding Gradients)

> ðŸ§  Best for: **RAG evaluation** â€” retrieval quality, answer grounding, and faithfulness

#### âœ… Key Features:

* **RAG-specific metrics**:

  * Context Precision / Recall
  * Faithfulness (does the output match the context?)
  * Answer Relevance
  * Context-Answer Similarity
* Plug-and-play for LlamaIndex and LangChain outputs

#### ðŸ“Š Ideal For:

* Evaluating document Q\&A agents
* Comparing RAG models with different vector stores or chunking strategies

#### ðŸ”§ Example:

```python
from ragas.metrics import faithfulness, answer_relevancy
from ragas.evaluation import evaluate

results = evaluate(
    questions, contexts, answers,
    metrics=[faithfulness(), answer_relevancy()]
)
```

> Combine this with human feedback loops for robust fine-tuning of your RAG setup.

---

### ðŸ”¹ 3. **DeepEval**

> ðŸ§  Best for: **Custom metric design** + out-of-the-box LLM evaluation

#### âœ… Key Features:

* Run **evaluation suites** on prompts, chains, or tools
* Define **custom metrics** using LLMs or logic
* Compare prompt versions side-by-side
* Integrates with LangChain, OpenAI, and manual codebases

#### ðŸ’¡ Highlights:

* Track history of prompt and tool changes
* Add subjective/human-style metrics like "Confidence" or "Humor"
* CLI or SDK-based usage

#### ðŸ”§ Example:

```python
from deepeval import assert_output

assert_output(
    input="What is AI?",
    expected="Artificial Intelligence is...",
    actual="AI is a branch of computer science...",
    metrics=["similarity", "relevance"]
)
```

---

### ðŸ§© Comparison Table

| Framework     | Best For                          | Key Strength                       |
| ------------- | --------------------------------- | ---------------------------------- |
| **LangSmith** | Debugging, tracing, observability | Visual, granular trace explorer    |
| **Ragas**     | RAG output evaluation             | RAG-specific, grounded metrics     |
| **DeepEval**  | Custom + holistic eval pipelines  | Prompt versioning & flexible tests |

---

### ðŸš€ Recommended Flow:

If you're building an LLM Agent:

1. **Use LangSmith** to debug and visualize agent/tool flows
2. **Use Ragas** if your agent uses vector search / RAG
3. **Use DeepEval** to benchmark prompt quality and regression test changes
