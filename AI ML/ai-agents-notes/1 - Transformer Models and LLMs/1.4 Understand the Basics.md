## ðŸ§  **1.4 Understand the Basics**

---

### ðŸ”¹ **Streamed vs Unstreamed Responses**

| Type           | Description                                                                  |
| -------------- | ---------------------------------------------------------------------------- |
| **Unstreamed** | The model processes the full input and then returns the full output.         |
| **Streamed**   | The model sends output **token-by-token** or in chunks as it generates them. |

#### âœ… Use Cases for Streaming:

* Chatbots (faster UX)
* Real-time applications (e.g., assistants)
* Long outputs where responsiveness matters

> Most modern APIs (like OpenAIâ€™s and Anthropicâ€™s) support streaming via websockets or SSE (Server-Sent Events).

---

### ðŸ”¹ **Reasoning vs Standard Models**

| Model Type           | Description                                                        |
| -------------------- | ------------------------------------------------------------------ |
| **Standard Models**  | Predict next token based on patterns learned during training       |
| **Reasoning Models** | Fine-tuned or structured to **solve logical, multi-step problems** |

#### Examples of Reasoning Enhancements:

* Chain of Thought prompting (step-by-step thinking)
* Tool usage for math/code
* Memory or scratchpad strategies
* Instruction-tuned LLMs (e.g., GPT-4, Claude 3 Opus)

> "Reasoning" is not inherentâ€”it emerges from training and prompting.

---

### ðŸ”¹ **Fine-tuning vs Prompt Engineering**

| Technique              | Description                                       | When to Use                               |
| ---------------------- | ------------------------------------------------- | ----------------------------------------- |
| **Prompt Engineering** | Crafting effective prompts to get desired outputs | Fast iteration, no training needed        |
| **Fine-tuning**        | Updating model weights with new labeled data      | Consistent outputs, domain-specific tasks |

#### Prompt Engineering Examples:

* Add system instructions
* Use few-shot examples
* Specify format/output style

#### Fine-tuning Examples:

* Domain-specific legal summarizer
* Custom chatbot trained on internal docs

> **Prompting is zero-shot** customization. Fine-tuning is more durable but expensive.

---

### ðŸ”¹ **Embeddings and Vector Search**

**Embeddings:**
Numerical vector representations of text that capture **semantic meaning**. Similar meanings â†’ similar vectors.

**Vector Search:**
Enables **semantic similarity search** in embedding space (using cosine or dot-product similarity).

#### Example Use Case:

* Convert documents to embeddings
* Store in vector DB (like Pinecone, Weaviate, FAISS)
* Search for most relevant chunks to feed into LLM (RAG!)

---

### ðŸ”¹ **Understand the Basics of RAG (Retrieval-Augmented Generation)**

**Definition:**
RAG is an architecture that **retrieves external context (from documents or databases)** and feeds it into the LLM to **generate better, fact-based responses**.

#### Basic Flow:

1. User Input
2. Embed query & retrieve documents
3. Inject retrieved text into prompt
4. LLM generates grounded answer

#### Benefits:

* Reduces hallucination
* Supports large knowledge bases
* Keeps LLM stateless (no memory required)

---

### ðŸ’° **Pricing of Common Models (as of 2024â€“2025)**

| Model                    | Context Length | Prompt Cost /1K tokens | Output Cost /1K tokens |
| ------------------------ | -------------- | ---------------------- | ---------------------- |
| GPT-4-turbo              | 128k           | \$0.01                 | \$0.03                 |
| GPT-3.5-turbo            | 16k            | \$0.001                | \$0.002                |
| Claude 3 Opus            | 200k           | \~\$0.015              | \~\$0.075              |
| Gemini 1.5 Pro           | 1M             | Varies (subscription)  | -                      |
| Mistral 7B (self-hosted) | â€”              | Free (infra only)      | Free                   |

> **Token Efficiency Tip:** Use summarization, compression, and relevant chunking to reduce token use.


