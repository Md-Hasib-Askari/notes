Great! Letâ€™s now dive into the **Model Mechanisms** under ## ðŸ”§ **Model Mechanisms**

---

### ðŸ”¹ 1. **Tokenization**

**Definition:**
Tokenization is the process of breaking down raw text into smaller pieces (tokens) that the model can understand. These tokens are usually subwords, words, or even characters.

#### Common Tokenization Strategies:

* **Byte-Pair Encoding (BPE):** Merges frequent pairs of characters/subwords (used in GPT-2, GPT-3).
* **Unigram Language Model:** Assigns probabilities to all possible token splits and picks the best (used in SentencePiece, like in T5).
* **Character-level:** Treats every character as a token (rare in LLMs, used in early models).
* **WordPiece:** A variant of BPE used in BERT.

#### Example:

```text
Input: "ChatGPT rocks!"
Tokens (BPE): ["Chat", "G", "PT", " rocks", "!"]
```

**Note:** The model doesn't understand words, only numeric **token IDs** mapped from these pieces.

---

### ðŸ”¹ 2. **Context Windows**

**Definition:**
The context window refers to the **maximum number of tokens** an LLM can process in a single input/output interaction.

#### Key Points:

* Larger context windows allow the model to consider more information (longer documents, threads, etc.).
* Once the limit is reached, older tokens are dropped (unless a memory strategy is in place).

#### Examples:

| Model          | Context Window (Tokens) |
| -------------- | ----------------------- |
| GPT-3          | 2048                    |
| GPT-3.5 Turbo  | 4096 â€“ 16,384           |
| GPT-4-turbo    | Up to 128,000           |
| Claude 3 Opus  | 200,000+                |
| Gemini 1.5 Pro | 1,000,000+              |

> **1 token â‰ˆ 0.75 words (average)**

---

### ðŸ”¹ 3. **Token-Based Pricing**

**Definition:**
Token-based pricing refers to charging users based on the number of tokens used during:

* **Input (prompt tokens)**
* **Output (completion tokens)**

#### Why It Matters:

* Pricing is tightly tied to token consumption.
* Efficient prompt design can save cost.

#### Example (GPT-4-turbo as of 2024):

| Usage Type   | Price per 1,000 tokens |
| ------------ | ---------------------- |
| Prompt Input | \$0.01                 |
| Output       | \$0.03                 |

So a single chat with 1,000 input + 500 output tokens:

```
Total cost = (1 x $0.01) + (0.5 x $0.03) = $0.025
```

#### Cost Optimization Tips:

* Compress context intelligently.
* Summarize previous history.
* Avoid verbose prompts.
