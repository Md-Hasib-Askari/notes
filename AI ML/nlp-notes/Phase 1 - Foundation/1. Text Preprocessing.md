# Text Preprocessing in NLP

Text preprocessing is the foundation of all NLP tasks, transforming raw text into structured formats for analysis.

## Tokenization
The process of splitting text into meaningful units (tokens):
- **Word tokenization**: Splits text into individual words
- **Sentence tokenization**: Divides text into separate sentences
- **Subword tokenization**: Creates tokens that are parts of words (common in modern NLP)

Example:
```python
# NLTK tokenization
from nltk.tokenize import word_tokenize
tokens = word_tokenize("Natural language processing is fascinating!")
```

## Stemming
Reduces words to their root/stem form by removing affixes, often using heuristic rules:
- Example: "running", "runs", "runner" → "run"
- Fast but imprecise, may produce non-words

## Lemmatization
Converts words to their dictionary base form (lemma) using vocabulary and morphological analysis:
- Example: "better" → "good", "was" → "be"
- More accurate but computationally intensive than stemming

Preprocessing decisions significantly impact downstream performance in NLP systems. Choose techniques based on your specific task requirements.
