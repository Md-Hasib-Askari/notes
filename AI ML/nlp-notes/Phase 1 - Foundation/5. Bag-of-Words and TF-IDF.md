# Bag-of-Words and TF-IDF Representations

These are fundamental techniques for converting text into numerical features for machine learning.

## Bag-of-Words (BoW)
A simple representation that:
- Creates a vocabulary of all unique words in the corpus
- Represents each document as a vector of word counts
- Ignores word order and grammar

**Example**:
For documents:
- D1: "I love natural language processing"
- D2: "I love deep learning"

Vocabulary: {"I", "love", "natural", "language", "processing", "deep", "learning"}
- D1 vector: [1, 1, 1, 1, 1, 0, 0]
- D2 vector: [1, 1, 0, 0, 0, 1, 1]

## Term Frequency-Inverse Document Frequency (TF-IDF)
Addresses BoW's limitation of treating all words equally:

- **Term Frequency (TF)**: How often a word appears in a document
  - TF(t,d) = (occurrences of term t in document d) / (total terms in d)

- **Inverse Document Frequency (IDF)**: Measures how informative a word is
  - IDF(t) = log(total documents / documents containing term t)

- **TF-IDF** = TF Ã— IDF
  - Downweights common words (like "the", "is")
  - Upweights distinctive terms specific to certain documents

These representations serve as input features for text classification, clustering, and information retrieval.
