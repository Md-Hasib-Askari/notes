# N-grams and Language Models

N-grams are contiguous sequences of n items (typically words or characters) from a text.

## Types of N-grams
- **Unigrams**: Single words (e.g., "natural", "language", "processing")
- **Bigrams**: Pairs of adjacent words (e.g., "natural language", "language processing")
- **Trigrams**: Three consecutive words (e.g., "natural language processing")

## Statistical Language Models
N-grams form the basis of statistical language models that predict the probability of a sequence of words:

- **Markov Assumption**: The probability of a word depends only on the n-1 previous words
- **Maximum Likelihood Estimation**: P(word|context) = count(context, word) / count(context)

## Applications
- **Text prediction**: Autocomplete, predictive text
- **Spelling correction**: Identifying likely corrections
- **Machine translation**: Evaluating fluency of translations
- **Text generation**: Creating new content based on patterns

## Limitations
- **Sparsity problem**: Many valid word combinations never appear in training data
- **Memory requirements**: Higher n values require exponentially more storage
- **Limited context**: Struggles with long-range dependencies

Modern approaches use smoothing techniques (Laplace, Good-Turing) or neural networks to address these limitations.
