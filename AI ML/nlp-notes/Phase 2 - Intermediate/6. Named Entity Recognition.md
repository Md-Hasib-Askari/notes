# Named Entity Recognition (NER)

Named Entity Recognition (NER) is the task of identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, time expressions, quantities, monetary values, and more.

## Core Concepts

### What are Named Entities?
Named entities are real-world objects with proper names, including:
- **People**: Barack Obama, Marie Curie
- **Organizations**: Google, United Nations
- **Locations**: Paris, Mount Everest
- **Dates and Times**: January 1, 2023, next Monday
- **Quantities**: 10 miles, 50 kilograms
- **Monetary Values**: $100, â‚¬50
- **Products**: iPhone, Tesla Model 3
- **Events**: World War II, Olympic Games

### NER as a Sequence Labeling Task
NER is typically framed as a sequence labeling problem where each token is assigned a tag:
- **BIO Tagging**: Beginning, Inside, Outside
  - B-PER: Beginning of person name
  - I-PER: Inside person name
  - O: Outside any entity
- **BILOU Tagging**: Beginning, Inside, Last, Outside, Unit
  - Adds L-tags for last tokens and U-tags for single-token entities

## Traditional NER Approaches

### Rule-based Systems
- Pattern matching with regular expressions
- Gazetteers (predefined lists of entities)
- Handcrafted linguistic rules

```python
import re

# Simple rule-based NER for dates
date_pattern = r'\b(?:January|February|March|...) \d{1,2}, \d{4}\b'
dates = re.findall(date_pattern, text)
```

### Statistical Methods
- Hidden Markov Models (HMMs)
- Conditional Random Fields (CRFs)
- Maximum Entropy Markov Models (MEMMs)

```python
import sklearn_crfsuite
from sklearn_crfsuite import metrics

# Extract features for CRF
def word_features(sentence, i):
    word = sentence[i]
    features = {
        'word.lower()': word.lower(),
        'word[-3:]': word[-3:],
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit()
    }
    if i > 0:
        prev_word = sentence[i-1]
        features.update({
            'prev_word.lower()': prev_word.lower(),
            'prev_word.istitle()': prev_word.istitle()
        })
    else:
        features['BOS'] = True  # Beginning of sentence
    return features

# Train CRF model
crf = sklearn_crfsuite.CRF(algorithm='lbfgs', max_iterations=100)
crf.fit(X_train, y_train)
```

## Modern NER Approaches

### Deep Learning Methods
- **BiLSTM-CRF**: Bidirectional LSTM with CRF layer
- **Transformer-based**: BERT, RoBERTa, etc.

### Using spaCy for NER
```python
import spacy

# Load pre-trained model
nlp = spacy.load("en_core_web_sm")

# Process text
text = "Apple is looking at buying U.K. startup for $1 billion"
doc = nlp(text)

# Extract entities
for ent in doc.ents:
    print(f"{ent.text} ({ent.label_})")
# Output:
# Apple (ORG)
# U.K. (GPE)
# $1 billion (MONEY)
```

### Using Hugging Face Transformers
```python
from transformers import pipeline

# Load NER pipeline
ner = pipeline("ner")

# Process text
text = "My name is Sarah and I work at Microsoft in Seattle."
entities = ner(text)

# Print results
for entity in entities:
    print(f"{entity['word']} ({entity['entity']})")
```

## Customizing NER Systems

### Fine-tuning Pre-trained Models
```python
import spacy
from spacy.training.example import Example

# Load existing model
nlp = spacy.load("en_core_web_sm")

# Training data
TRAIN_DATA = [
    ("Uber is acquiring Postmates for $2.65 billion", 
     {"entities": [(0, 4, "ORG"), (19, 28, "ORG"), (33, 45, "MONEY")]}),
    # More examples...
]

# Create training examples
examples = []
for text, annotations in TRAIN_DATA:
    examples.append(Example.from_dict(nlp.make_doc(text), annotations))

# Fine-tune model
optimizer = nlp.create_optimizer()
for _ in range(10):
    losses = {}
    for example in examples:
        nlp.update([example], drop=0.5, losses=losses)
    print(losses)
```

### Building Domain-Specific NER
- Focus on entities relevant to your domain
- Create domain-specific training data
- Consider custom entity types

## Evaluation Metrics

### Standard Metrics
- **Precision**: Percentage of extracted entities that are correct
- **Recall**: Percentage of entities in the text that were found
- **F1 Score**: Harmonic mean of precision and recall

```python
from seqeval.metrics import classification_report

# Example true and predicted labels
y_true = [['O', 'B-PER', 'I-PER', 'O', 'B-ORG']]
y_pred = [['O', 'B-PER', 'I-PER', 'O', 'B-LOC']]

# Print metrics
print(classification_report(y_true, y_pred))
```

### Span-Based Evaluation
- Considers complete entity spans rather than token-level predictions
- More representative of real-world performance

## Common Challenges

### Boundary Detection
- Determining where entities start and end
- Resolving ambiguous boundaries

### Entity Type Ambiguity
- Same entity can have different types in different contexts
- Example: "Apple" as company vs. fruit

### Nested Entities
- Entities contained within other entities
- Example: "Bank of America" (ORG) contains "America" (LOC)

### Out-of-Vocabulary Entities
- Handling previously unseen entities
- Leveraging context for identification

## Applications of NER

### Information Extraction
- Extracting structured information from unstructured text
- Building knowledge graphs

### Question Answering
- Identifying entities in questions and potential answers
- Supporting fact retrieval

### Document Summarization
- Focusing on key entities in a document
- Entity-centric summarization

### Content Recommendation
- Matching content based on entity preferences
- Entity-based user profiling

### Search Enhancement
- Entity-aware search indexing
- Query understanding through entity recognition

NER serves as a fundamental building block for many advanced NLP applications, bridging the gap between unstructured text and structured, actionable information.
