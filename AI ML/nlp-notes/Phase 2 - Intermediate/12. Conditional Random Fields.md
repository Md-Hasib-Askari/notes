# Conditional Random Fields in NLP

Conditional Random Fields (CRFs) are discriminative probabilistic models for sequence labeling that overcome many limitations of Hidden Markov Models (HMMs). They're particularly effective for tasks like named entity recognition, part-of-speech tagging, and information extraction.

## Fundamentals of Conditional Random Fields

### Key Concepts

#### Discriminative vs. Generative Models
- **HMMs** (generative): Model joint probability P(X,Y) of observations X and labels Y
- **CRFs** (discriminative): Model conditional probability P(Y|X) directly

#### Advantages over HMMs
1. Can incorporate arbitrary, overlapping features
2. Avoid the label bias problem
3. Model entire sequence globally rather than state-by-state
4. Don't make strong independence assumptions about observations

### Mathematical Foundation

CRFs define the conditional probability of a label sequence Y given an observation sequence X:

P(Y|X) = (1/Z(X)) * exp(∑ᵢ ∑ⱼ λⱼfⱼ(yᵢ₋₁, yᵢ, X, i))

Where:
- Z(X) is a normalization factor
- λⱼ are weights learned during training
- fⱼ are feature functions that map (label pair, observation, position) to a real value
- i indexes positions in the sequence

## Linear-Chain CRFs

The most common form of CRFs used in NLP is the linear-chain CRF, where dependencies between labels are restricted to adjacent positions:

```python
import sklearn_crfsuite
from sklearn_crfsuite import metrics

# Feature extraction for CRF
def word_to_features(sentence, i):
    word = sentence[i][0]
    
    # Basic features
    features = {
        'bias': 1.0,
        'word.lower()': word.lower(),
        'word[-3:]': word[-3:],
        'word[-2:]': word[-2:],
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit()
    }
    
    # Features for previous word
    if i > 0:
        prev_word = sentence[i-1][0]
        features.update({
            '-1:word.lower()': prev_word.lower(),
            '-1:word.istitle()': prev_word.istitle(),
            '-1:word.isupper()': prev_word.isupper()
        })
    else:
        features['BOS'] = True  # Beginning of sentence
    
    # Features for next word
    if i < len(sentence) - 1:
        next_word = sentence[i+1][0]
        features.update({
            '+1:word.lower()': next_word.lower(),
            '+1:word.istitle()': next_word.istitle(),
            '+1:word.isupper()': next_word.isupper()
        })
    else:
        features['EOS'] = True  # End of sentence
    
    return features

# Convert a sentence to features
def sentence_to_features(sentence):
    return [word_to_features(sentence, i) for i in range(len(sentence))]

# Extract labels from a sentence
def sentence_to_labels(sentence):
    return [label for token, label in sentence]

# Prepare training data (formatted as [[(word1, label1), (word2, label2), ...], ...])
X_train = [sentence_to_features(s) for s in train_data]
y_train = [sentence_to_labels(s) for s in train_data]

# Train the CRF model
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,  # L1 regularization
    c2=0.1,  # L2 regularization
    max_iterations=100,
    all_possible_transitions=True
)
crf.fit(X_train, y_train)
```

## Training CRFs

### Objective Function
CRFs are typically trained to maximize the conditional log-likelihood of the training data:

L(θ) = ∑ᵢ log P(Yᵢ|Xᵢ; θ) - (λ₁||θ₁||₁ + λ₂||θ₂||₂²)

Where the second term represents regularization to prevent overfitting.

### Optimization Methods
- **L-BFGS**: Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm
- **Stochastic Gradient Descent**: For large-scale applications

### Inference Algorithms
- **Viterbi**: Find most likely label sequence
- **Forward-Backward**: Calculate marginal probabilities

## Applications in NLP

### Named Entity Recognition
CRFs excel at NER tasks by capturing contextual dependencies:

```python
# Example: Predict entities with trained CRF
def predict_entities(sentence, crf):
    # Convert sentence to features
    sentence_features = [word_to_features([(w, '') for w in sentence], i) 
                         for i in range(len(sentence))]
    
    # Predict labels
    predicted_labels = crf.predict([sentence_features])[0]
    
    # Combine words with predicted labels
    return list(zip(sentence, predicted_labels))

# Example usage
text = "Barack Obama was born in Hawaii and was the 44th president of the United States".split()
entities = predict_entities(text, crf)
print(entities)
# Output: [('Barack', 'B-PER'), ('Obama', 'I-PER'), ('was', 'O'), ...]
```

### Part-of-Speech Tagging
CRFs achieve state-of-the-art performance for traditional POS tagging:

```python
# Feature function for POS tagging (simplified)
def pos_features(sentence, i):
    word = sentence[i]
    
    features = {
        'word': word,
        'suffix3': word[-3:] if len(word) > 3 else word,
        'suffix2': word[-2:] if len(word) > 2 else word,
        'prefix3': word[:3] if len(word) > 3 else word,
        'prefix2': word[:2] if len(word) > 2 else word,
        'is_capitalized': word[0].isupper(),
        'is_numeric': word.isdigit(),
        'has_hyphen': '-' in word
    }
    
    # Add context features
    if i > 0:
        features['prev_word'] = sentence[i-1]
    if i < len(sentence) - 1:
        features['next_word'] = sentence[i+1]
    
    return features
```

### Shallow Parsing (Chunking)
Identifying non-overlapping phrases in text:

```python
# Example chunks with IOB notation:
# B-NP: Beginning of noun phrase
# I-NP: Inside noun phrase
# B-VP: Beginning of verb phrase
# I-VP: Inside verb phrase
# O: Outside any chunk

# (Use similar feature extraction as for NER and POS tagging)
```

### Information Extraction
Extracting structured information from unstructured text:

```python
# Example: Extract fields from citations
# Fields: author, title, journal, year, etc.
# (Use similar approach with domain-specific features)
```

## Evaluating CRF Models

### Metrics
- **Token-level accuracy**: Percentage of correctly labeled tokens
- **Sequence-level accuracy**: Percentage of completely correct sequences
- **F1 score**: Harmonic mean of precision and recall (for each label class)

```python
from sklearn_crfsuite import metrics

# Predict on test data
X_test = [sentence_to_features(s) for s in test_data]
y_test = [sentence_to_labels(s) for s in test_data]
y_pred = crf.predict(X_test)

# Calculate metrics
print(metrics.flat_f1_score(y_test, y_pred, 
                           average='weighted', 
                           labels=crf.classes_))

# Print detailed report
print(metrics.flat_classification_report(
    y_test, y_pred, labels=crf.classes_, digits=3
))
```

## Advanced Topics

### Semi-CRFs
Model segmentation and labeling simultaneously by working with segments rather than individual tokens.

### Higher-Order CRFs
Extend the Markov assumption to consider more than just the previous label:
- Second-order CRF: P(yᵢ|yᵢ₋₁, yᵢ₋₂, X)
- Trade-off between expressiveness and computational complexity

### Neural CRFs
Combining neural networks with CRFs:
- **BiLSTM-CRF**: Use bidirectional LSTM to generate features for CRF
- **Transformer-CRF**: Transformer encoders with CRF output layer

```python
# Conceptual BiLSTM-CRF architecture
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,
                            num_layers=1, bidirectional=True)
        self.hidden2tag = nn.Linear(hidden_dim, len(tag_to_ix))
        self.transitions = nn.Parameter(torch.randn(len(tag_to_ix), len(tag_to_ix)))
        # ... CRF implementation details ...
```

CRFs remain a powerful tool for sequence labeling in NLP, either as standalone models or combined with neural networks in hybrid architectures. Their ability to model sequential dependencies and incorporate rich feature sets makes them particularly well-suited for structured prediction tasks.
