# Unsupervised Learning: Clustering and Topic Modeling

Unsupervised learning techniques extract patterns and structure from text data without requiring labeled examples. These methods are invaluable for exploring large text collections and discovering hidden themes.

## Text Clustering

### Clustering Fundamentals for Text Data
Clustering groups similar documents together based on content similarity, allowing for:
- Organizing large document collections
- Identifying similar content
- Discovering natural groupings in text data
- Enhancing search and recommendation systems

### K-Means Clustering for Text
The most widely used clustering algorithm for text:

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np

# Prepare text data
documents = ["Machine learning is fascinating", 
             "NLP techniques process human language",
             "Deep learning uses neural networks",
             "Language models predict text sequences",
             "Clustering groups similar documents"]

# Create TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(documents)

# Apply K-means clustering
k = 2  # Number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(X)
clusters = kmeans.labels_

# Get cluster centers and top terms per cluster
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names_out()

for i in range(k):
    print(f"Cluster {i}:")
    for ind in order_centroids[i, :5]:  # Print top 5 terms
        print(f" - {terms[ind]}")
```

### Hierarchical Clustering
Creates a tree-like structure (dendrogram) of nested clusters:

```python
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Apply hierarchical clustering
hc = AgglomerativeClustering(n_clusters=k, linkage='ward')
hc.fit(X.toarray())

# Visualize dendrogram
linkage_matrix = linkage(X.toarray(), 'ward')
plt.figure(figsize=(10, 7))
dendrogram(linkage_matrix, labels=documents)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Document')
plt.ylabel('Distance')
plt.show()
```

### DBSCAN for Text Clustering
Density-based clustering for discovering clusters of arbitrary shape:

```python
from sklearn.cluster import DBSCAN

# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')
dbscan.fit(X)
```

### Evaluating Text Clustering
Without ground truth labels:
- **Silhouette Score**: Measures how similar documents are to their own cluster vs. other clusters
- **Davies-Bouldin Index**: Ratio of within-cluster scatter to between-cluster separation
- **Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster dispersion

```python
from sklearn.metrics import silhouette_score

silhouette_avg = silhouette_score(X, kmeans.labels_, metric='cosine')
print(f"Silhouette Score: {silhouette_avg}")
```

With ground truth labels:
- **Adjusted Rand Index**
- **Normalized Mutual Information**

## Topic Modeling

### Latent Dirichlet Allocation (LDA)
The most popular probabilistic topic modeling technique:

```python
from sklearn.decomposition import LatentDirichletAllocation

# Apply LDA
n_topics = 3
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(X)

# Print top words for each topic
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda.components_):
    top_words_idx = topic.argsort()[:-10 - 1:-1]  # Top 10 words
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"Topic #{topic_idx}: {' '.join(top_words)}")
    
# Get topic distribution for a document
topic_distribution = lda.transform(X[0])
```

### Non-negative Matrix Factorization (NMF)
Another effective technique for topic modeling:

```python
from sklearn.decomposition import NMF

# Apply NMF
nmf = NMF(n_components=n_topics, random_state=42)
nmf.fit(X)

# Print top words for each topic
for topic_idx, topic in enumerate(nmf.components_):
    top_words_idx = topic.argsort()[:-10 - 1:-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"Topic #{topic_idx}: {' '.join(top_words)}")
```

### Latent Semantic Analysis (LSA)
Uses singular value decomposition to identify topics:

```python
from sklearn.decomposition import TruncatedSVD

# Apply LSA/LSI
lsa = TruncatedSVD(n_components=n_topics, random_state=42)
lsa.fit(X)

# Print top words for each topic
for topic_idx, topic in enumerate(lsa.components_):
    top_words_idx = topic.argsort()[:-10 - 1:-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"Topic #{topic_idx}: {' '.join(top_words)}")
```

### Gensim Implementation
Using the popular Gensim library for more advanced topic modeling:

```python
import gensim
from gensim.corpora import Dictionary

# Prepare corpus
tokenized_docs = [doc.split() for doc in documents]
dictionary = Dictionary(tokenized_docs)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# Train LDA model
lda_model = gensim.models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=3,
    passes=10,
    alpha='auto',
    eta='auto'
)

# Print topics
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic #{idx}: {topic}")
    
# Topic coherence
from gensim.models.coherencemodel import CoherenceModel
coherence_model = CoherenceModel(model=lda_model, texts=tokenized_docs, 
                                dictionary=dictionary, coherence='c_v')
coherence_score = coherence_model.get_coherence()
print(f"Coherence Score: {coherence_score}")
```

### Topic Model Evaluation
- **Perplexity**: How well the model predicts held-out data
- **Topic Coherence**: Measures semantic similarity between high-scoring words in topics
- **Human Evaluation**: Manual assessment of topic quality

```python
# Perplexity in sklearn
print(f"Perplexity: {lda.perplexity(X)}")
```

### Visualizing Topic Models
Using pyLDAvis for interactive visualization:

```python
import pyLDAvis
import pyLDAvis.sklearn

# Prepare visualization
vis_data = pyLDAvis.sklearn.prepare(lda, X, vectorizer)
pyLDAvis.display(vis_data)
```

## Advanced Techniques

### Dynamic Topic Modeling
Tracking how topics evolve over time:

```python
from gensim.models.ldaseqmodel import LdaSeqModel

# Group documents by time periods
time_slices = [3, 4, 5]  # Number of documents in each time slice

# Train dynamic topic model
ldaseq = LdaSeqModel(corpus=corpus, id2word=dictionary, 
                     time_slice=time_slices, num_topics=3)
```

### Hierarchical Topic Modeling
Discovering topic hierarchies:

```python
from gensim.models.hdpmodel import HdpModel

# Train hierarchical Dirichlet process model
hdp = HdpModel(corpus=corpus, id2word=dictionary)
```

### Contextualized Topic Models
Using pre-trained language models for better topic coherence:

```python
from contextualized_topic_models.models.ctm import CombinedTM

# Prepare training data
training_dataset = CTMDataset(X, document_embeddings)

# Train model
ctm = CombinedTM(bow_size=len(vectorizer.get_feature_names_out()),
                 contextual_size=768, num_topics=5)
ctm.fit(training_dataset)
```

## Practical Applications

### Document Organization
- Automatically categorizing documents
- Creating hierarchical document structures
- Building faceted navigation systems

### Content Recommendation
- Finding similar documents based on topics
- Diversifying content recommendations
- User interest modeling

### Trend Analysis
- Identifying emerging topics in news/social media
- Tracking changes in topic prevalence over time
- Detecting shifts in discourse

### Information Extraction
- Using topics as features for downstream tasks
- Summarizing document collections
- Extracting key themes from large text corpora

## Best Practices

1. **Preprocessing considerations**:
   - Remove stopwords, punctuation, and common/rare terms
   - Apply stemming/lemmatization
   - Consider domain-specific preprocessing

2. **Determining optimal number of topics**:
   - Use coherence scores
   - Apply grid search with cross-validation
   - Elbow method with perplexity/coherence

3. **Interpretability techniques**:
   - Label topics manually based on top words
   - Use word clouds for visualization
   - Evaluate with domain experts

4. **Handling large document collections**:
   - Online/incremental learning
   - Dimensionality reduction before clustering
   - Sampling techniques

Unsupervised learning techniques offer powerful ways to explore and organize text data, revealing hidden structures and themes without the need for labeled examples.
