# Transformers Library Basics

The Hugging Face Transformers library provides state-of-the-art pre-trained models for NLP tasks. This library simplifies working with transformer-based architectures like BERT, GPT, and T5, making advanced NLP accessible to everyone.

## Installation and Setup

```python
# Install the Transformers library
# pip install transformers

# Basic imports
import transformers
from transformers import AutoTokenizer, AutoModel

# Check installed version
print(f"Transformers version: {transformers.__version__}")
```

## Core Concepts

### Pre-trained Models

The library is built around pre-trained models that can be fine-tuned for specific tasks:

- **BERT** (Bidirectional Encoder Representations from Transformers)
- **GPT** (Generative Pre-trained Transformer)
- **RoBERTa** (Robustly Optimized BERT Pretraining Approach)
- **T5** (Text-to-Text Transfer Transformer)
- **ALBERT, DistilBERT** (Lightweight BERT variants)
- Many others, constantly being updated

### Pipeline API for Quick Tasks

The simplest way to use the library is through high-level pipelines:

```python
from transformers import pipeline

# Sentiment analysis
sentiment_analyzer = pipeline("sentiment-analysis")
result = sentiment_analyzer("I really enjoyed using this library!")
print(f"Sentiment: {result}")
# Output: [{'label': 'POSITIVE', 'score': 0.9998}]

# Text generation
text_generator = pipeline("text-generation")
generated_text = text_generator("Transformers library is", max_length=50, do_sample=True)
print(f"Generated: {generated_text[0]['generated_text']}")

# Named entity recognition
ner = pipeline("ner")
entities = ner("Hugging Face was founded in Paris, France")
print(f"Entities: {entities}")

# Question answering
qa = pipeline("question-answering")
answer = qa(
    question="What is a transformer model?",
    context="Transformer models are neural networks that use self-attention mechanisms to process sequential data."
)
print(f"Answer: {answer['answer']}")
```

## Working with Tokenizers

Tokenizers convert text into tokens that models can process:

```python
from transformers import AutoTokenizer

# Load tokenizer (bert-base-uncased as example)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenize text
text = "Transformers handle NLP tasks efficiently."
tokens = tokenizer.tokenize(text)
print(f"Tokens: {tokens}")

# Convert tokens to IDs
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print(f"Input IDs: {input_ids}")

# All-in-one encoding
encoded = tokenizer(
    text,
    padding=True,      # Add padding
    truncation=True,   # Truncate if too long
    max_length=20,     # Maximum sequence length
    return_tensors="pt" # Return PyTorch tensors ('tf' for TensorFlow)
)
print(f"Encoded: {encoded}")

# Decode back to text
decoded = tokenizer.decode(encoded["input_ids"][0], skip_special_tokens=True)
print(f"Decoded: {decoded}")
```

## Using Pre-trained Models

### Loading and Using Base Models

```python
from transformers import AutoModel, AutoTokenizer
import torch

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# Prepare input
text = "Let's extract embeddings from this sentence."
inputs = tokenizer(text, return_tensors="pt")

# Get embeddings
with torch.no_grad():
    outputs = model(**inputs)

# Access last hidden states (embeddings)
last_hidden_state = outputs.last_hidden_state
print(f"Embedding shape: {last_hidden_state.shape}")
# Output: torch.Size([1, sequence_length, hidden_size])

# Using CLS token as sentence embedding
sentence_embedding = last_hidden_state[:, 0, :]
print(f"Sentence embedding shape: {sentence_embedding.shape}")
```

### Task-Specific Models

```python
from transformers import AutoModelForSequenceClassification

# Load model for text classification
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english"
)
tokenizer = AutoTokenizer.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english"
)

# Prepare input
text = "I'm really happy with the results!"
inputs = tokenizer(text, return_tensors="pt")

# Get classification logits
with torch.no_grad():
    outputs = model(**inputs)
    
logits = outputs.logits
print(f"Logits: {logits}")

# Convert to probabilities
import torch.nn.functional as F
probs = F.softmax(logits, dim=-1)
print(f"Probabilities: {probs}")
```

## Fine-tuning Transformers

### Text Classification Example

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import Trainer, TrainingArguments
import torch
import numpy as np
from datasets import load_dataset

# Load dataset (IMDB as example)
dataset = load_dataset("imdb")

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=2
)

# Tokenize datasets
def tokenize_function(examples):
    return tokenizer(
        examples["text"], 
        padding="max_length", 
        truncation=True, 
        max_length=512
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    learning_rate=5e-5,
    weight_decay=0.01,
    evaluation_strategy="epoch",
)

# Define metrics for evaluation
import evaluate
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# Create Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    compute_metrics=compute_metrics,
)

# Fine-tune model
trainer.train()
```

## Transformers for Generation Tasks

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Add special tokens if not present
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Generate text
prompt = "In natural language processing, transformers are"
inputs = tokenizer(prompt, return_tensors="pt")

# Generate with beam search
beam_outputs = model.generate(
    inputs["input_ids"],
    max_length=100,
    num_beams=5,
    no_repeat_ngram_size=2,
    early_stopping=True
)
print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))

# Generate with sampling
sample_outputs = model.generate(
    inputs["input_ids"],
    max_length=100,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.7,
    num_return_sequences=3
)

print("Generated samples:")
for i, sample in enumerate(sample_outputs):
    print(f"{i}: {tokenizer.decode(sample, skip_special_tokens=True)}")
```

## Saving and Loading Fine-tuned Models

```python
# Save model and tokenizer
model.save_pretrained("./my-finetuned-model")
tokenizer.save_pretrained("./my-finetuned-model")

# Load the saved model and tokenizer
loaded_model = AutoModelForSequenceClassification.from_pretrained("./my-finetuned-model")
loaded_tokenizer = AutoTokenizer.from_pretrained("./my-finetuned-model")
```

## Handling Longer Texts

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# Long text example
long_text = "..." # Long document here

# Chunk the text
max_length = model.config.max_position_embeddings - 2  # Account for [CLS] and [SEP]
words = long_text.split()
chunks = []
current_chunk = []

for word in words:
    current_chunk.append(word)
    if len(tokenizer.tokenize(" ".join(current_chunk))) > max_length:
        current_chunk.pop()  # Remove last word
        chunks.append(" ".join(current_chunk))
        current_chunk = [word]  # Start new chunk with the removed word

# Add the last chunk
if current_chunk:
    chunks.append(" ".join(current_chunk))

# Process each chunk
all_embeddings = []
for chunk in chunks:
    inputs = tokenizer(chunk, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    all_embeddings.append(outputs.last_hidden_state[:, 0, :])  # CLS token embeddings

# Combine chunk embeddings (average, max, or other pooling strategy)
document_embedding = torch.mean(torch.stack(all_embeddings), dim=0)
```

The Transformers library has revolutionized NLP by making powerful pre-trained models accessible and easy to use, enabling developers to quickly implement state-of-the-art solutions for various language tasks.
