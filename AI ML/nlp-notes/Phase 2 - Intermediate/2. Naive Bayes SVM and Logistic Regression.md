# Naive Bayes, SVM, and Logistic Regression for NLP

These three classical machine learning algorithms form the backbone of many NLP applications due to their efficiency, interpretability, and strong performance on text data.

## Naive Bayes

### Theory and Principles
Naive Bayes is a probabilistic classifier based on Bayes' theorem with the "naive" assumption of conditional independence between features.

**Bayes' Theorem**:
P(class|features) = P(features|class) × P(class) / P(features)

For text classification:
- P(class|document) = P(document|class) × P(class) / P(document)
- With independence assumption: P(document|class) = P(word₁|class) × P(word₂|class) × ... × P(wordₙ|class)

### Variants for NLP
1. **Multinomial Naive Bayes**: Models word frequencies; suitable for document classification
2. **Bernoulli Naive Bayes**: Models word presence/absence; good for short texts
3. **Gaussian Naive Bayes**: For continuous features (rarely used directly for text)

### Implementation Example
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# Create pipeline
nb_pipeline = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('classifier', MultinomialNB(alpha=1.0))  # alpha is smoothing parameter
])

# Train model
nb_pipeline.fit(X_train, y_train)

# Predict and evaluate
y_pred = nb_pipeline.predict(X_test)
```

### Advantages and Limitations
**Advantages**:
- Fast training and prediction
- Works well with small datasets
- Effective for text classification
- Handles high-dimensional feature spaces

**Limitations**:
- Independence assumption often violated
- May be outperformed by more complex models
- Poor probability estimates

## Support Vector Machines (SVM)

### Theory and Principles
SVM finds the optimal hyperplane that maximizes the margin between classes. For text data, it works well because:
1. Text problems are often linearly separable in high-dimensional space
2. SVMs handle high dimensionality efficiently
3. They're effective with sparse feature vectors (common in text)

### SVM Kernels for Text
- **Linear kernel**: Most common for text; efficient for high-dimensional data
- **RBF kernel**: Sometimes useful for semantic analysis
- **String kernels**: Specialized for text; measures sequence similarity

### Implementation Example
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV

# Create pipeline
svm_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(min_df=5, max_df=0.8)),
    ('classifier', CalibratedClassifierCV(LinearSVC()))  # For probability estimates
])

# Train and evaluate
svm_pipeline.fit(X_train, y_train)
y_pred = svm_pipeline.predict(X_test)
```

### Advantages and Limitations
**Advantages**:
- Excellent performance on text classification
- Effective in high-dimensional spaces
- Robust against overfitting
- Handles non-linear boundaries with kernels

**Limitations**:
- Slower training than Naive Bayes
- Less interpretable
- Sensitive to parameter tuning
- Memory-intensive for large datasets

## Logistic Regression

### Theory and Principles
Logistic Regression models the probability of a class using the logistic function:
P(class=1|x) = 1 / (1 + e^(-wx))

For text classification, it:
1. Learns weights for each feature (word/n-gram)
2. Combines them linearly
3. Applies logistic function to get probability

### Regularization for Text
- **L1 regularization (Lasso)**: Creates sparse models; performs feature selection
- **L2 regularization (Ridge)**: Prevents overfitting with smoother solutions
- **Elastic Net**: Combines L1 and L2 regularization

### Implementation Example
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Create pipeline
lr_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('classifier', LogisticRegression(C=1.0, penalty='l2', solver='liblinear'))
])

# Train and evaluate
lr_pipeline.fit(X_train, y_train)
y_pred = lr_pipeline.predict(X_test)

# Feature importance
feature_names = lr_pipeline.named_steps['tfidf'].get_feature_names_out()
important_features = sorted(zip(lr_pipeline.named_steps['classifier'].coef_[0], feature_names))
```

### Advantages and Limitations
**Advantages**:
- Provides probability estimates
- Highly interpretable feature weights
- Efficient for large datasets
- Handles regularization well

**Limitations**:
- Assumes linear decision boundary
- May underperform with complex relationships
- Can be prone to overfitting without regularization

## Comparative Analysis

### Performance Comparison
- **Naive Bayes**: Fast but less accurate; good baseline
- **SVM**: Often highest accuracy; resource intensive
- **Logistic Regression**: Good balance of speed and accuracy

### When to Use Each
- **Naive Bayes**: Small datasets, real-time applications, baseline models
- **SVM**: When accuracy is paramount, medium-sized datasets
- **Logistic Regression**: When interpretability matters, large datasets

### Ensemble Approaches
Combining these classifiers often yields better results:
- Voting classifiers
- Stacking
- Weighted averaging of predictions

## Practical Tips for NLP Applications

1. **Feature engineering matters**:
   - Word n-grams (typically 1-3)
   - Character n-grams for robustness against misspellings
   - TF-IDF often outperforms raw counts

2. **Hyperparameter tuning**:
   - NB: Alpha smoothing parameter
   - SVM: C parameter, kernel selection
   - LR: Regularization strength and type

3. **Handling class imbalance**:
   - Class weights
   - Resampling techniques
   - Threshold adjustment

These classical algorithms remain competitive and often outperform more complex approaches, especially when data is limited or interpretability is required.
