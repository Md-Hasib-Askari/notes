# Text Summarization Methods

Text summarization is the process of creating a concise and coherent version of a longer document while preserving its key information and meaning. Summarization techniques enable users to quickly understand the essence of large volumes of text.

## Types of Summarization

### Extractive Summarization
Selects and combines existing sentences or phrases from the original text:
- Identifies important sentences based on statistical/linguistic features
- Maintains original wording from source document
- Generally easier to implement and more reliable

### Abstractive Summarization
Generates new text that captures the meaning of the original document:
- Creates novel sentences not present in the source
- May paraphrase and restructure information
- More similar to human-written summaries, but more challenging

### Hybrid Approaches
Combines extractive and abstractive techniques:
- Extract key sentences first, then paraphrase/rewrite them
- Compress extracted content before generating abstract

## Extractive Summarization Techniques

### Statistical Methods

#### Frequency-Based Approaches
```python
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize

def frequency_summarizer(text, num_sentences=3):
    # Tokenize the text into sentences and words
    sentences = sent_tokenize(text)
    words = word_tokenize(text.lower())
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.isalnum() and word not in stop_words]
    
    # Count word frequencies
    word_frequencies = Counter(words)
    
    # Calculate sentence scores based on word frequencies
    sentence_scores = {}
    for i, sentence in enumerate(sentences):
        for word in word_tokenize(sentence.lower()):
            if word in word_frequencies:
                if i in sentence_scores:
                    sentence_scores[i] += word_frequencies[word]
                else:
                    sentence_scores[i] = word_frequencies[word]
    
    # Get top sentences
    top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
    top_sentences = sorted(top_sentences, key=lambda x: x[0])  # Sort by position
    
    # Combine sentences into summary
    summary = [sentences[i] for i, _ in top_sentences]
    return ' '.join(summary)
```

#### TF-IDF Weighting
```python
from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf_summarizer(text, num_sentences=3):
    sentences = sent_tokenize(text)
    
    # Create TF-IDF matrix
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(sentences)
    
    # Calculate sentence scores as sum of TF-IDF values
    sentence_scores = [sum(row) for row in tfidf_matrix.toarray()]
    
    # Get top sentences
    top_sentence_indices = sorted(range(len(sentence_scores)), 
                                 key=lambda i: sentence_scores[i], 
                                 reverse=True)[:num_sentences]
    top_sentence_indices.sort()  # Sort by position
    
    # Create summary
    summary = [sentences[i] for i in top_sentence_indices]
    return ' '.join(summary)
```

### Graph-Based Methods

#### TextRank Algorithm
Inspired by PageRank, models text as a graph where sentences are nodes:

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def textrank_summarizer(text, num_sentences=3):
    sentences = sent_tokenize(text)
    
    # Create sentence embeddings (simple bag-of-words for illustration)
    vectorizer = TfidfVectorizer(stop_words='english')
    sentence_vectors = vectorizer.fit_transform(sentences)
    
    # Create similarity matrix
    similarity_matrix = cosine_similarity(sentence_vectors)
    
    # Apply PageRank-like algorithm
    scores = np.ones(len(sentences)) / len(sentences)
    damping = 0.85
    iterations = 10
    
    for _ in range(iterations):
        new_scores = np.ones(len(sentences)) * (1 - damping) / len(sentences)
        for i in range(len(sentences)):
            for j in range(len(sentences)):
                if i != j and similarity_matrix[j, i] > 0:
                    new_scores[i] += damping * scores[j] * similarity_matrix[j, i] / sum(similarity_matrix[j, :])
        scores = new_scores
    
    # Get top sentences
    top_sentence_indices = sorted(range(len(scores)), 
                                 key=lambda i: scores[i], 
                                 reverse=True)[:num_sentences]
    top_sentence_indices.sort()  # Sort by position
    
    # Create summary
    summary = [sentences[i] for i in top_sentence_indices]
    return ' '.join(summary)
```

### Machine Learning Methods

#### Supervised Approaches
- Train classification models to determine if a sentence should be in summary
- Features: position, length, TF-IDF scores, named entities, etc.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# Assuming we have labeled data with sentences and binary inclusion labels
def train_extractive_model(documents, labels):
    # Extract features (simplified example)
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform(documents)
    
    # Train classifier
    scaler = StandardScaler(with_mean=False)
    X_scaled = scaler.fit_transform(X)
    model = SVC(kernel='linear')
    model.fit(X_scaled, labels)
    
    return vectorizer, scaler, model

# Using the trained model
def ml_summarizer(text, vectorizer, scaler, model, num_sentences=3):
    sentences = sent_tokenize(text)
    
    # Extract features
    X = vectorizer.transform(sentences)
    X_scaled = scaler.transform(X)
    
    # Predict importance scores
    scores = model.decision_function(X_scaled)
    
    # Get top sentences
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]
    top_indices.sort()  # Sort by position
    
    # Create summary
    summary = [sentences[i] for i in top_indices]
    return ' '.join(summary)
```

## Abstractive Summarization Techniques

### Sequence-to-Sequence Models
Neural networks that encode the source document and decode it into a summary:

```python
from transformers import pipeline

def abstractive_summarizer(text):
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    result = summarizer(text, max_length=130, min_length=30, do_sample=False)
    return result[0]['summary_text']
```

### Transformer-Based Approaches
State-of-the-art models using attention mechanisms:

```python
from transformers import BartForConditionalGeneration, BartTokenizer

def bart_summarizer(text):
    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
    
    inputs = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True)
    summary_ids = model.generate(inputs['input_ids'], 
                                 num_beams=4, 
                                 min_length=30, 
                                 max_length=100, 
                                 early_stopping=True)
    
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary
```

### Reinforcement Learning Methods
- Use policy gradient methods to optimize for ROUGE scores
- Overcome exposure bias in traditional seq2seq training

## Evaluation Metrics

### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
The standard metric for summarization evaluation:

```python
from rouge import Rouge

def evaluate_summary(reference, candidate):
    rouge = Rouge()
    scores = rouge.get_scores(candidate, reference)
    
    # Print ROUGE scores
    print(f"ROUGE-1: {scores[0]['rouge-1']}")
    print(f"ROUGE-2: {scores[0]['rouge-2']}")
    print(f"ROUGE-L: {scores[0]['rouge-l']}")
    
    return scores
```

- **ROUGE-N**: N-gram overlap (ROUGE-1, ROUGE-2)
- **ROUGE-L**: Longest common subsequence
- **ROUGE-SU**: Skip-bigram plus unigram

### BLEU (Bilingual Evaluation Understudy)
Originally for machine translation, sometimes used for summarization:

```python
from nltk.translate.bleu_score import sentence_bleu

def bleu_score(reference, candidate):
    reference_tokens = [reference.split()]
    candidate_tokens = candidate.split()
    return sentence_bleu(reference_tokens, candidate_tokens)
```

### Human Evaluation
- **Coherence**: Logical flow and readability
- **Relevance**: Coverage of important information
- **Fluency**: Grammaticality and natural language
- **Informativeness**: Amount of meaningful content

## Applications

### News Summarization
- Condensing news articles
- Creating headlines and snippets
- Multi-document summarization for news aggregation

### Scientific Literature
- Research paper summarization
- Abstract generation
- Review article compilation

### Legal Document Summarization
- Contract summarization
- Case law digests
- Legal brief preparation

### Meeting/Conversation Summarization
- Meeting minutes generation
- Customer service call summarization
- Email thread summarization

## Advanced Topics

### Query-Based Summarization
Generate summaries focused on specific information needs:

```python
def query_based_summarizer(text, query, num_sentences=3):
    sentences = sent_tokenize(text)
    
    # Create TF-IDF vectors
    vectorizer = TfidfVectorizer(stop_words='english')
    sentence_vectors = vectorizer.fit_transform(sentences)
    
    # Create query vector
    query_vector = vectorizer.transform([query])
    
    # Calculate similarity with query
    similarities = cosine_similarity(query_vector, sentence_vectors)[0]
    
    # Get top sentences
    top_indices = sorted(range(len(similarities)), 
                        key=lambda i: similarities[i], 
                        reverse=True)[:num_sentences]
    top_indices.sort()  # Sort by position
    
    # Create summary
    summary = [sentences[i] for i in top_indices]
    return ' '.join(summary)
```

### Multi-Document Summarization
Summarizing information from multiple sources:
- Redundancy removal
- Information fusion
- Timeline generation

### Update Summarization
Generating summaries that focus on new information:
- Comparing with previous documents
- Highlighting changes and developments

Text summarization remains an active research area with applications growing as the volume of digital text continues to expand, making efficient information consumption increasingly important.
