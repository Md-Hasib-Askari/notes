# Gensim for Topic Modeling and Word Embeddings

Gensim is a robust Python library designed for unsupervised topic modeling and natural language processing, with a focus on memory efficiency and large corpus processing. It excels in document similarity analysis, topic extraction, and word embedding operations.

## Installation and Setup

```python
# Install Gensim
# pip install gensim

import gensim
from gensim import corpora, models
import gensim.downloader as api
```

## Document Preprocessing and Vectorization

### Creating a Dictionary and Corpus

```python
# Sample documents
documents = [
    "Machine learning algorithms can automatically learn and improve from experience",
    "Natural language processing deals with interactions between computers and human language",
    "Topic modeling discovers abstract topics in document collections",
    "Word embeddings capture semantic meaning in vector space"
]

# Tokenize and preprocess
tokenized_docs = [[word.lower() for word in doc.split()] for doc in documents]

# Create dictionary (mapping words to IDs)
dictionary = corpora.Dictionary(tokenized_docs)
print(f"Vocabulary size: {len(dictionary)}")
print(f"Word to ID mapping: {dictionary.token2id}")

# Convert documents to bag-of-words representation
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]
print(f"First document BoW: {corpus[0]}")
```

## Topic Modeling with Gensim

### Latent Dirichlet Allocation (LDA)

```python
# Train LDA model
lda_model = models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=3,  # Number of topics
    passes=10,     # Multiple passes for better convergence
    alpha='auto',  # Learn the prior distributions
    eta='auto'     # Learn the topic-word density
)

# Display topics
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic #{idx}: {topic}")

# Infer topics for a new document
new_doc = "Deep learning uses neural networks for language understanding"
new_bow = dictionary.doc2bow(new_doc.lower().split())
doc_topics = lda_model[new_bow]
print(f"Topic distribution: {doc_topics}")
```

### Topic Model Evaluation

```python
# Compute topic coherence
from gensim.models.coherencemodel import CoherenceModel

coherence_model = CoherenceModel(
    model=lda_model, 
    texts=tokenized_docs, 
    dictionary=dictionary, 
    coherence='c_v'
)
coherence_score = coherence_model.get_coherence()
print(f"Coherence score: {coherence_score}")

# Find optimal number of topics
def compute_coherence_values(corpus, dictionary, texts, limit, start=2, step=1):
    coherence_values = []
    for num_topics in range(start, limit, step):
        model = models.LdaModel(
            corpus=corpus, 
            id2word=dictionary, 
            num_topics=num_topics, 
            passes=10
        )
        coherence_model = CoherenceModel(
            model=model, 
            texts=texts, 
            dictionary=dictionary, 
            coherence='c_v'
        )
        coherence_values.append(coherence_model.get_coherence())
    return coherence_values
```

### Other Topic Models

```python
# Latent Semantic Indexing/Analysis (LSI/LSA)
lsi_model = models.LsiModel(corpus, id2word=dictionary, num_topics=3)

# Non-Negative Matrix Factorization (NMF)
nmf_model = models.Nmf(corpus, id2word=dictionary, num_topics=3)

# Hierarchical Dirichlet Process (HDP)
hdp_model = models.HdpModel(corpus, id2word=dictionary)
```

## Word Embeddings with Gensim

### Using Pre-trained Word Vectors

```python
# List available pre-trained models
available_models = api.info()['models']
print(f"Available models: {list(available_models.keys())[:5]}...")

# Load pre-trained Word2Vec model
word2vec_model = api.load('word2vec-google-news-300')

# Word similarity
similarity = word2vec_model.similarity('computer', 'laptop')
print(f"Similarity between 'computer' and 'laptop': {similarity}")

# Find similar words
similar_words = word2vec_model.most_similar('python', topn=5)
print(f"Words similar to 'python': {similar_words}")

# Word arithmetic
result = word2vec_model.most_similar(positive=['king', 'woman'], negative=['man'])
print(f"king - man + woman = {result[0][0]}")
```

### Training Word2Vec from Scratch

```python
# Train a Word2Vec model on your corpus
word2vec = models.Word2Vec(
    sentences=tokenized_docs,
    vector_size=100,   # Embedding dimension
    window=5,          # Context window size
    min_count=1,       # Ignore words with frequency below this
    workers=4,         # Number of threads
    sg=1               # Skip-gram (1) or CBOW (0)
)

# Save and load models
word2vec.save('word2vec.model')
loaded_model = models.Word2Vec.load('word2vec.model')

# Get word vector
vector = word2vec.wv['learning']
print(f"Vector shape: {vector.shape}")
```

### Other Word Embedding Models

```python
# FastText (handles out-of-vocabulary words)
fasttext_model = models.FastText(
    sentences=tokenized_docs,
    vector_size=100,
    window=5,
    min_count=1
)

# Doc2Vec (document embeddings)
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

tagged_docs = [TaggedDocument(words=doc, tags=[str(i)]) 
               for i, doc in enumerate(tokenized_docs)]

doc2vec_model = Doc2Vec(
    documents=tagged_docs,
    vector_size=100,
    window=5,
    min_count=1,
    workers=4,
    epochs=20
)

# Get document vector
doc_vector = doc2vec_model.infer_vector(['natural', 'language', 'processing'])
```

## Document Similarity and Search

```python
# Create similarity index
from gensim.similarities import MatrixSimilarity

lsi_index = MatrixSimilarity(lsi_model[corpus])

# Query processing
query = "machine learning for language processing"
query_bow = dictionary.doc2bow(query.lower().split())
query_lsi = lsi_model[query_bow]

# Get document similarities
similarities = lsi_index[query_lsi]
sorted_sims = sorted(enumerate(similarities), key=lambda item: -item[1])

print("Document similarities:")
for doc_id, similarity in sorted_sims:
    print(f"Doc #{doc_id}: {similarity:.4f} - {documents[doc_id][:50]}...")
```

## Advanced Gensim Techniques

### Phrase Detection

```python
# Detect phrases (bigrams)
from gensim.models.phrases import Phrases, Phraser

bigram = Phrases(tokenized_docs, min_count=1, threshold=1)
bigram_phraser = Phraser(bigram)
bigram_docs = [bigram_phraser[doc] for doc in tokenized_docs]

# Example result
print(f"Original: {tokenized_docs[0]}")
print(f"With phrases: {bigram_docs[0]}")
```

### Text Preprocessing Pipeline

```python
# Complete preprocessing pipeline
from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation

CUSTOM_FILTERS = [strip_tags, strip_punctuation, lambda x: x.lower()]

def preprocess(docs):
    # Apply preprocessing
    processed_docs = [preprocess_string(doc, CUSTOM_FILTERS) for doc in docs]
    
    # Detect bigrams
    bigram = Phrases(processed_docs, min_count=1, threshold=10)
    bigram_phraser = Phraser(bigram)
    
    # Apply bigram detection
    processed_docs = [bigram_phraser[doc] for doc in processed_docs]
    
    return processed_docs
```

Gensim is a powerful library for working with unstructured text, particularly for discovering hidden structures in document collections through topic modeling and leveraging semantic relationships through word embeddings.
