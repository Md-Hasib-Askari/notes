# Feature Engineering for Text Data

Feature engineering transforms raw text into meaningful numerical representations that machine learning algorithms can process. The quality of these features often determines the success of NLP applications.

## Text Representation Fundamentals

### Basic Approaches
1. **One-Hot Encoding**: Each word gets a binary vector
   - Sparse, high-dimensional
   - Loses word relationships

2. **Bag-of-Words (BoW)**: Count-based word representation
   - Simple but effective
   - Ignores word order and context
   ```python
   from sklearn.feature_extraction.text import CountVectorizer
   
   vectorizer = CountVectorizer()
   X = vectorizer.fit_transform(["Text feature engineering is important"])
   ```

3. **TF-IDF**: Term Frequency-Inverse Document Frequency
   - Downweights common words, emphasizes distinctive terms
   - Better than raw counts for many tasks
   ```python
   from sklearn.feature_extraction.text import TfidfVectorizer
   
   tfidf = TfidfVectorizer()
   X_tfidf = tfidf.fit_transform(corpus)
   ```

4. **Word Embeddings**: Dense vector representations
   - Captures semantic relationships
   - Pre-trained (Word2Vec, GloVe, FastText) or task-specific
   ```python
   import gensim.downloader
   
   # Load pre-trained embeddings
   word2vec = gensim.downloader.load('word2vec-google-news-300')
   
   # Get vector for a word
   vector = word2vec['computer']  # 300-dimensional vector
   ```

## Advanced Feature Engineering Techniques

### N-gram Features
```python
# Word n-grams
ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))  # Unigrams, bigrams, trigrams

# Character n-grams
char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 5))
```

Benefits:
- Captures local word order
- Handles out-of-vocabulary words (character n-grams)
- Robust to spelling variations

### Syntactic Features
- **Part-of-Speech (POS) tags**: Grammatical information
  ```python
  import spacy
  nlp = spacy.load("en_core_web_sm")
  
  def extract_pos_features(text):
      doc = nlp(text)
      return [token.pos_ for token in doc]
  ```
- **Dependency relations**: Grammatical structure
- **Parse tree features**: Hierarchical sentence structure

### Lexical Features
- **Named entities**: People, organizations, locations
- **Sentiment lexicons**: Positive/negative word dictionaries
  ```python
  from nltk.sentiment.vader import SentimentIntensityAnalyzer
  
  sid = SentimentIntensityAnalyzer()
  sentiment_scores = sid.polarity_scores("This is a great product!")
  ```
- **Topic signatures**: Words strongly associated with topics

### Statistical Features
- **Document length**: Character/word/sentence counts
- **Readability scores**: Flesch-Kincaid, SMOG, etc.
- **Type-token ratio**: Lexical diversity measure
- **Word frequency statistics**: Average frequency, variance

### Contextual Features
- **Surrounding words**: Window of context
- **Document position**: Location of terms in document
- **Discourse markers**: Transition words, conjunctions

## Dimensionality Reduction

### Feature Selection
- **Chi-squared test**: Statistical relevance of features
  ```python
  from sklearn.feature_selection import SelectKBest, chi2
  
  selector = SelectKBest(chi2, k=1000)
  X_selected = selector.fit_transform(X_tfidf, y)
  ```
- **Mutual information**: Information gain from features
- **Recursive feature elimination**: Iteratively removing features

### Feature Extraction
- **Principal Component Analysis (PCA)**: Linear dimensionality reduction
- **Latent Semantic Analysis (LSA/LSI)**: Using SVD on document-term matrix
  ```python
  from sklearn.decomposition import TruncatedSVD
  
  svd = TruncatedSVD(n_components=100)
  X_lsa = svd.fit_transform(X_tfidf)
  ```
- **Non-negative Matrix Factorization (NMF)**: For topic modeling

## Document Embedding Techniques

### Combining Word Embeddings
- **Average pooling**: Mean of all word vectors
  ```python
  def document_vector(doc, model):
      words = [word for word in doc.split() if word in model]
      return np.mean([model[word] for word in words], axis=0)
  ```
- **TF-IDF weighted average**: Weight by importance
- **Max/min pooling**: Element-wise maximum/minimum

### Specialized Document Embeddings
- **Doc2Vec**: Paragraph Vector model
  ```python
  from gensim.models.doc2vec import Doc2Vec, TaggedDocument
  
  documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(corpus)]
  model = Doc2Vec(documents, vector_size=100)
  ```
- **Universal Sentence Encoder**: Transformer-based embeddings
- **SBERT**: Sentence-BERT for sentence similarity

## Domain-Specific Feature Engineering

### Text Classification Features
- Content-based features: Words, phrases, topics
- Style-based features: Formality, readability, complexity

### Information Retrieval Features
- BM25 scores: Probabilistic relevance framework
- Query term proximity: Distance between query terms

### Sentiment Analysis Features
- Negation handling: "not good" vs "good"
- Intensifiers/diminishers: "very good" vs "somewhat good"
- Emoticons and emoji: ðŸ˜Š, ðŸ˜¢, etc.

## Feature Engineering Pipeline

### Preprocessing Steps
1. Text cleaning: Remove noise, normalize
2. Tokenization: Break into meaningful units
3. Stopword removal: Filter common words
4. Stemming/lemmatization: Normalize word forms

### Feature Extraction
1. Create base features (BoW, TF-IDF, embeddings)
2. Add specialized features (POS, entities, etc.)
3. Apply transformations (normalization, scaling)

### Feature Selection/Reduction
1. Remove irrelevant features
2. Reduce dimensionality
3. Handle multicollinearity

## Evaluation and Iteration

### Feature Importance Analysis
- Model-specific methods (coefficients, feature importance)
- Permutation importance: Shuffling feature values
- SHAP values: Game-theoretic approach to feature contribution

### Ablation Studies
Systematically removing feature groups to measure impact

### Best Practices
1. Start simple (BoW, TF-IDF) before adding complexity
2. Use domain knowledge to guide feature creation
3. Validate features with cross-validation
4. Monitor for overfitting with increasing feature count
5. Document feature engineering decisions for reproducibility

Feature engineering remains crucial even in the deep learning era, particularly for tasks with limited data or when model interpretability is required.
