# Cross-validation and Model Evaluation in NLP

Proper evaluation is critical for developing robust NLP systems. This note covers key techniques for validating and assessing model performance in text-based applications.

## Fundamentals of Cross-validation

### Why Cross-validation is Essential for NLP
- Text data often exhibits high variability
- Models may memorize specific phrases rather than learn patterns
- Distribution shifts between training and test data can be severe
- Helps detect overfitting to particular linguistic patterns

### Basic Cross-validation Techniques

**K-Fold Cross-validation**
```python
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=42)
for train_index, test_index in kf.split(documents):
    X_train, X_test = [documents[i] for i in train_index], [documents[i] for i in test_index]
    y_train, y_test = [labels[i] for i in train_index], [labels[i] for i in test_index]
    # Train and evaluate model
```

**Stratified K-Fold**
Maintains class distribution in each fold - critical for imbalanced text datasets:
```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for train_index, test_index in skf.split(documents, labels):
    # Train and evaluate model
```

**Leave-One-Out Cross-validation**
Useful for very small text datasets:
```python
from sklearn.model_selection import LeaveOneOut

loo = LeaveOneOut()
for train_index, test_index in loo.split(documents):
    # Train and evaluate model
```

### NLP-Specific Cross-validation Considerations

**Document-Level vs. Sentence-Level**
- Ensure validation splits respect document boundaries
- Avoid information leakage between related texts

**Temporal Splitting**
For time-sensitive text data (news, social media):
```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(time_ordered_documents):
    # Train and evaluate model
```

**Cross-Topic Validation**
Evaluating generalization across different topics:
1. Group documents by topic
2. Hold out entire topics for testing

**Cross-Domain Validation**
Testing on text from different sources/domains to assess transferability

## Classification Metrics for NLP Tasks

### Basic Classification Metrics
```python
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Overall accuracy
accuracy = accuracy_score(y_true, y_pred)

# Class-wise metrics
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)

# Micro/macro averages
macro_f1 = precision_recall_fscore_support(y_true, y_pred, average='macro')[2]
```

### Handling Class Imbalance in Text Data
- **Precision**: Fraction of predicted positives that are correct
- **Recall**: Fraction of actual positives identified
- **F1 Score**: Harmonic mean of precision and recall
- **Area Under ROC Curve (AUC)**: Threshold-independent metric

```python
from sklearn.metrics import roc_auc_score, average_precision_score

# For binary classification
auc = roc_auc_score(y_true, y_prob)
ap = average_precision_score(y_true, y_prob)  # Average precision
```

### Multi-label Text Classification
```python
from sklearn.metrics import hamming_loss, jaccard_score

# Hamming loss
hl = hamming_loss(y_true, y_pred)

# Jaccard index
js = jaccard_score(y_true, y_pred, average='samples')
```

### Sequence Labeling Metrics (NER, POS tagging)
```python
from seqeval.metrics import classification_report

# For sequence labeling tasks
print(classification_report(y_true, y_pred))
```

## Beyond Standard Metrics: NLP-Specific Evaluation

### Text Generation Evaluation
- **BLEU Score**: n-gram precision (machine translation, summarization)
- **ROUGE**: Recall-oriented metrics for summarization
- **METEOR**: Considers synonyms and stemming
- **BERTScore**: Contextual similarity using BERT embeddings

```python
from nltk.translate.bleu_score import sentence_bleu
bleu = sentence_bleu([reference], hypothesis)

# For ROUGE
from rouge import Rouge
rouge = Rouge()
scores = rouge.get_scores(hypothesis, reference)
```

### Semantic Similarity Evaluation
- **Cosine similarity**: Between document embeddings
- **Word Mover's Distance**: Semantic transportation cost
- **BERTScore**: Contextual token similarity

### Human Evaluation Considerations
- Inter-annotator agreement (Cohen's Kappa, Fleiss' Kappa)
- Qualitative error analysis categories
- A/B testing in production environments

## Diagnostic Evaluation

### Error Analysis Techniques
- **Confusion Matrix Visualization**
```python
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_true, y_pred)
```

- **Error Classification**
  - Misclassified examples by length, complexity, topic
  - Systematic error patterns
  - Out-of-vocabulary impacts

- **Feature Importance Analysis**
```python
import eli5
from eli5.sklearn import explain_weights

explanation = explain_weights(classifier, vec=vectorizer)
```

### Learning Curves
Detecting overfitting/underfitting in text models:
```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    estimator, X, y, cv=5, scoring='f1_macro', 
    train_sizes=np.linspace(0.1, 1.0, 10))
```

### Bias and Fairness Evaluation
- Testing for gender, racial, or cultural biases in text models
- Counterfactual data augmentation for bias testing
- Disaggregated performance across demographic groups

## Model Comparison Strategies

### Statistical Significance Testing
- **McNemar's Test**: For comparing classifiers on the same test set
```python
from statsmodels.stats.contingency_tables import mcnemar

# Create contingency table
table = [[both_correct, model1_correct_only],
         [model2_correct_only, both_incorrect]]
         
result = mcnemar(table, exact=True)
```

- **Wilcoxon Signed-Rank Test**: For comparing metrics across multiple datasets
```python
from scipy.stats import wilcoxon

# Compare F1 scores across multiple folds/datasets
p_value = wilcoxon(model1_f1_scores, model2_f1_scores).pvalue
```

### Ensemble Evaluation
- Diversity analysis of ensemble components
- Error correlation between models
- Contribution analysis of individual models

## Practical Implementation Tips

### Evaluation Pipelines
1. **Nested Cross-validation**: For hyperparameter tuning
```python
from sklearn.model_selection import GridSearchCV, cross_val_score

# Outer CV loop
cv_scores = []
for train_idx, test_idx in outer_cv.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    
    # Inner CV loop for hyperparameter tuning
    grid_search = GridSearchCV(model, param_grid, cv=inner_cv)
    grid_search.fit(X_train, y_train)
    
    # Evaluate best model on test set
    best_model = grid_search.best_estimator_
    cv_scores.append(best_model.score(X_test, y_test))
```

2. **Evaluation Tracking**: Log metrics, parameters, and artifacts
3. **Reproducibility**: Fixed random seeds, documented preprocessing

### Setting Realistic Baselines
- **Majority class classifier**: Simple frequency baseline
- **Keyword/rule-based system**: Domain knowledge baseline
- **Classical ML model**: Before complex deep learning

### Production Monitoring
- Performance drift detection
- Data distribution shifts
- A/B testing framework for model updates

Rigorous evaluation is the cornerstone of reliable NLP systems. The methods outlined here provide a comprehensive framework for assessing and improving text processing models across various applications and domains.
