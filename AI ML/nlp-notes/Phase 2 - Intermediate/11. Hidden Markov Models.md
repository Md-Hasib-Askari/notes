# Hidden Markov Models in NLP

Hidden Markov Models (HMMs) are statistical models that represent a system where states are not directly observable (hidden), but outputs dependent on those states are observable. In NLP, HMMs provide a principled framework for sequence labeling tasks.

## Fundamentals of Hidden Markov Models

### Core Components
1. **Hidden States**: Typically represent linguistic categories (e.g., POS tags)
2. **Observations**: The words or tokens in a sequence
3. **Transition Probabilities**: Probability of moving from one state to another
4. **Emission Probabilities**: Probability of observing a particular output given a state
5. **Initial State Probabilities**: Probability distribution of starting in each state

### Formal Definition
An HMM is defined by the tuple λ = (A, B, π) where:
- **A**: Transition probability matrix where A[i,j] = P(state j at t+1 | state i at t)
- **B**: Emission probability matrix where B[i,j] = P(observation j | state i)
- **π**: Initial state probability distribution

## HMMs for Part-of-Speech Tagging

In POS tagging, the hidden states are the POS tags, and the observations are the words:

```python
import numpy as np
from hmmlearn import hmm

# Simplified example with numerical representation
# Assume we've mapped words and tags to indices

# Define transition probabilities (between tags)
trans_prob = np.array([
    [0.3, 0.4, 0.3],  # From tag 0 (e.g., DET) to others
    [0.2, 0.5, 0.3],  # From tag 1 (e.g., NOUN) to others
    [0.6, 0.3, 0.1]   # From tag 2 (e.g., VERB) to others
])

# Define emission probabilities (tags to words)
emit_prob = np.array([
    [0.7, 0.1, 0.1, 0.1],  # Tag 0 -> words
    [0.1, 0.6, 0.2, 0.1],  # Tag 1 -> words
    [0.1, 0.1, 0.2, 0.6]   # Tag 2 -> words
])

# Define initial state probabilities
start_prob = np.array([0.5, 0.3, 0.2])

# Create and train HMM
model = hmm.MultinomialHMM(n_components=3)
model.startprob_ = start_prob
model.transmat_ = trans_prob
model.emissionprob_ = emit_prob
```

## Key Algorithms for HMMs

### Forward Algorithm
Computes P(observations | model) efficiently using dynamic programming:

```python
def forward(obs_seq, states, start_p, trans_p, emit_p):
    # Initialize forward probabilities
    fwd = np.zeros((len(obs_seq), len(states)))
    
    # Initialization (t=0)
    for s in range(len(states)):
        fwd[0, s] = start_p[s] * emit_p[s, obs_seq[0]]
    
    # Forward recursion
    for t in range(1, len(obs_seq)):
        for s in range(len(states)):
            fwd[t, s] = 0
            for s2 in range(len(states)):
                fwd[t, s] += fwd[t-1, s2] * trans_p[s2, s]
            fwd[t, s] *= emit_p[s, obs_seq[t]]
    
    # Return total probability summing over final states
    return np.sum(fwd[-1, :])
```

### Viterbi Algorithm
Finds the most likely sequence of hidden states given observations:

```python
def viterbi(obs_seq, states, start_p, trans_p, emit_p):
    V = [{}]  # Viterbi matrix
    path = {}
    
    # Initialize base cases (t=0)
    for s in range(len(states)):
        V[0][s] = start_p[s] * emit_p[s, obs_seq[0]]
        path[s] = [s]
    
    # Run Viterbi for t > 0
    for t in range(1, len(obs_seq)):
        V.append({})
        new_path = {}
        
        for s in range(len(states)):
            max_prob = -1
            best_state = -1
            
            for s2 in range(len(states)):
                prob = V[t-1][s2] * trans_p[s2, s] * emit_p[s, obs_seq[t]]
                if prob > max_prob:
                    max_prob = prob
                    best_state = s2
            
            V[t][s] = max_prob
            new_path[s] = path[best_state] + [s]
        
        path = new_path
    
    # Find the best path
    max_prob = -1
    best_state = -1
    for s in range(len(states)):
        if V[len(obs_seq)-1][s] > max_prob:
            max_prob = V[len(obs_seq)-1][s]
            best_state = s
    
    return path[best_state]
```

### Baum-Welch Algorithm
An expectation-maximization algorithm for estimating HMM parameters from unlabeled data:

```python
# Using hmmlearn library for Baum-Welch
from hmmlearn import hmm

# Initialize model with random parameters
model = hmm.MultinomialHMM(n_components=3, n_iter=100)

# Train model with observations
model.fit(obs_sequences)  # This implements Baum-Welch

# Get learned parameters
learned_trans_prob = model.transmat_
learned_emit_prob = model.emissionprob_
learned_start_prob = model.startprob_
```

## Applications in NLP

### POS Tagging
Using HMMs to determine the grammatical category of words in context:

```python
# Example: POS tagging with HMM
def pos_tag_with_hmm(sentence, model, word_to_idx, idx_to_tag):
    # Convert words to indices
    obs_seq = [word_to_idx.get(word.lower(), 0) for word in sentence.split()]
    
    # Use Viterbi to find most likely tag sequence
    tag_indices = model.decode(np.array([obs_seq]).T)[1]
    
    # Convert indices back to tags
    tags = [idx_to_tag[idx] for idx in tag_indices]
    
    return list(zip(sentence.split(), tags))
```

### Named Entity Recognition
HMMs can identify entity types in text, typically using BIO tagging:

```python
# Simplified NER with HMM (conceptual)
entity_types = ["O", "B-PER", "I-PER", "B-LOC", "I-LOC", "B-ORG", "I-ORG"]
# (Initialize and train HMM with entity types as states)
```

### Speech Recognition
HMMs were historically dominant in speech recognition:
- States represent phonemes or sub-phonetic units
- Observations are acoustic features

## Limitations of HMMs

1. **Independence assumptions**: States depend only on previous state
2. **Limited context**: Cannot easily incorporate wider context
3. **Feature engineering**: Struggle with overlapping features
4. **Label bias problem**: Prefers transitions to states with fewer outgoing transitions

## Enhancements to Basic HMMs

### Higher-Order HMMs
Extend state dependency beyond just the previous state:
- Second-order HMM: P(state_t | state_{t-1}, state_{t-2})
- Trade-off between expressiveness and computational complexity

### Hierarchical HMMs
Organize states in a hierarchical structure:
- Higher-level states correspond to broader linguistic units
- Lower-level states handle more specific patterns

HMMs provide a strong statistical foundation for sequence modeling in NLP, although they have been largely superseded by neural approaches for many tasks. Understanding HMMs remains valuable for grasping the fundamentals of probabilistic sequence modeling.
