# Recurrent Neural Networks (RNNs, LSTMs, GRUs)

Recurrent Neural Networks (RNNs) are specialized neural network architectures designed for sequential data processing. Unlike traditional feedforward networks, RNNs maintain internal memory through recurrent connections, making them particularly suitable for NLP tasks where context and sequence matter.

## Basic RNN Architecture

The fundamental RNN design processes inputs sequentially, maintaining a hidden state that's updated at each step:

```
h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)
y_t = W_hy * h_t + b_y
```

Where:
- `x_t`: Input at time step t
- `h_t`: Hidden state at time step t
- `y_t`: Output at time step t
- `W_xh`, `W_hh`, `W_hy`: Weight matrices
- `b_h`, `b_y`: Bias terms

## The Vanishing/Exploding Gradient Problem

Basic RNNs suffer from vanishing or exploding gradients during backpropagation through time, making them unable to capture long-range dependencies. This limitation led to the development of more sophisticated architectures.

## Long Short-Term Memory (LSTM)

LSTMs address the vanishing gradient problem by introducing a cell state and various gates to control information flow:

1. **Forget Gate**: Decides what information to discard from the cell state
2. **Input Gate**: Decides what new information to store in the cell state
3. **Output Gate**: Decides what parts of the cell state to output

```
f_t = σ(W_f · [h_{t-1}, x_t] + b_f)  # Forget gate
i_t = σ(W_i · [h_{t-1}, x_t] + b_i)  # Input gate
C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)  # Candidate cell state
C_t = f_t * C_{t-1} + i_t * C̃_t  # Updated cell state
o_t = σ(W_o · [h_{t-1}, x_t] + b_o)  # Output gate
h_t = o_t * tanh(C_t)  # Hidden state
```

## Gated Recurrent Unit (GRU)

GRUs simplify the LSTM architecture by combining the forget and input gates into a single "update gate" and merging the cell state and hidden state:

```
z_t = σ(W_z · [h_{t-1}, x_t] + b_z)  # Update gate
r_t = σ(W_r · [h_{t-1}, x_t] + b_r)  # Reset gate
h̃_t = tanh(W · [r_t * h_{t-1}, x_t] + b)  # Candidate hidden state
h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t  # Updated hidden state
```

## Implementing RNNs with PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Simple LSTM for text classification
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=1, 
                 bidirectional=False, dropout=0.5):
        super().__init__()
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # LSTM layer
        self.lstm = nn.LSTM(embedding_dim, 
                          hidden_dim, 
                          num_layers=n_layers, 
                          bidirectional=bidirectional, 
                          dropout=dropout if n_layers > 1 else 0,
                          batch_first=True)
        
        # Fully connected layer
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
        
        # Dropout layer
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, text):
        # text shape: [batch size, sentence length]
        
        # Embedded shape: [batch size, sentence length, embedding dim]
        embedded = self.embedding(text)
        
        # LSTM output shape: [batch size, sentence length, hidden dim * num directions]
        # LSTM hidden shape: [num layers * num directions, batch size, hidden dim]
        output, (hidden, cell) = self.lstm(embedded)
        
        # If bidirectional, concatenate the final forward and backward hidden states
        if self.lstm.bidirectional:
            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        else:
            hidden = hidden[-1,:,:]
        
        # Apply dropout
        hidden = self.dropout(hidden)
        
        # Pass through fully connected layer
        return self.fc(hidden)

# Example usage
vocab_size = 10000
embedding_dim = 100
hidden_dim = 256
output_dim = 3  # e.g., positive, negative, neutral sentiment
model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, 
                       bidirectional=True)

# Create a dummy input (batch of 5 sentences, each with 10 tokens)
dummy_input = torch.randint(0, vocab_size, (5, 10))
prediction = model(dummy_input)
print(f"Prediction shape: {prediction.shape}")  # Should be [5, 3]
```

## RNN Variants Comparison

| Feature | Basic RNN | LSTM | GRU |
|---------|-----------|------|-----|
| Parameters | Fewest | Most | Moderate |
| Training speed | Fastest | Slowest | Moderate |
| Long-term dependencies | Poor | Excellent | Good |
| Computational efficiency | High | Low | Moderate |
| Vanishing gradient | Severe | Minimal | Minimal |

## Applications in NLP

1. **Text Generation**: Character or word-level language models
2. **Sentiment Analysis**: Capturing sequential sentiment cues
3. **Named Entity Recognition**: Tagging words in context
4. **Machine Translation**: Encoding source language sentences
5. **Speech Recognition**: Converting audio to text

## Best Practices

1. **Bidirectional RNNs**: Use for tasks where future context matters
2. **Stacked RNNs**: Multiple layers for complex tasks
3. **Gradient Clipping**: Prevent exploding gradients
4. **Layer Normalization**: Stabilize training
5. **Residual Connections**: Help with very deep RNN architectures
6. **Dropout**: Apply recurrent dropout for regularization
7. **Choose Wisely**: 
   - LSTM for capturing precise long-term dependencies
   - GRU for smaller datasets or when computational efficiency matters

## Limitations

1. **Sequential computation**: Cannot be parallelized effectively
2. **Still struggles with very long sequences**: Even LSTMs have limits
3. **Computational complexity**: Slower than alternatives like Transformers
4. **Fixed-size representations**: Information bottleneck in the hidden state

RNNs, particularly LSTMs and GRUs, were revolutionary for NLP before the Transformer era, and they remain valuable for many sequence modeling tasks, especially with limited computational resources or when explicit modeling of sequential information is needed.
