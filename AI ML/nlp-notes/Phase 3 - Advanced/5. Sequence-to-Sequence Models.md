# Sequence-to-Sequence Models

Sequence-to-sequence (seq2seq) models are neural network architectures designed to transform an input sequence into an output sequence, where the lengths of input and output may differ. These models revolutionized tasks like machine translation, text summarization, and dialogue systems.

## Core Architecture

A standard seq2seq model consists of two main components:

1. **Encoder**: Processes the input sequence and compresses it into a context vector
2. **Decoder**: Generates the output sequence based on the context vector

```
Input Sequence → Encoder → Context Vector → Decoder → Output Sequence
```

## Encoding Process

The encoder reads the input sequence token by token, updating its hidden state at each step. After processing the entire input, the final hidden state becomes the context vector that encapsulates the input information.

## Decoding Process

The decoder is initialized with the context vector and generates the output sequence one token at a time. At each step, it takes the previously generated token and its current hidden state to predict the next token.

## Teacher Forcing

During training, a technique called "teacher forcing" is often used. Rather than feeding the decoder's own predictions back as inputs, the ground truth tokens are provided. This speeds up training and improves stability.

## Implementation with PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random

class Encoder(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers=1, dropout=0.5):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        # src shape: [batch_size, src_len]
        
        embedded = self.dropout(self.embedding(src))
        # embedded shape: [batch_size, src_len, embedding_dim]
        
        outputs, hidden = self.rnn(embedded)
        # outputs shape: [batch_size, src_len, hidden_dim]
        # hidden shape: [n_layers, batch_size, hidden_dim]
        
        return outputs, hidden

class Decoder(nn.Module):
    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers=1, dropout=0.5):
        super().__init__()
        
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(output_dim, embedding_dim)
        self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden):
        # input shape: [batch_size, 1]
        # hidden shape: [n_layers, batch_size, hidden_dim]
        
        input = input.unsqueeze(1)  # Add sequence length dimension
        # input shape: [batch_size, 1]
        
        embedded = self.dropout(self.embedding(input))
        # embedded shape: [batch_size, 1, embedding_dim]
        
        output, hidden = self.rnn(embedded, hidden)
        # output shape: [batch_size, 1, hidden_dim]
        # hidden shape: [n_layers, batch_size, hidden_dim]
        
        prediction = self.fc_out(output.squeeze(1))
        # prediction shape: [batch_size, output_dim]
        
        return prediction, hidden

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
    def forward(self, src, tgt, teacher_forcing_ratio=0.5):
        # src shape: [batch_size, src_len]
        # tgt shape: [batch_size, tgt_len]
        
        batch_size = tgt.shape[0]
        tgt_len = tgt.shape[1]
        tgt_vocab_size = self.decoder.output_dim
        
        # Tensor to store decoder outputs
        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)
        
        # Encode the source sequence
        _, hidden = self.encoder(src)
        
        # First input to the decoder is the <sos> token
        input = tgt[:, 0]
        
        for t in range(1, tgt_len):
            # Get decoder output for current step
            output, hidden = self.decoder(input, hidden)
            
            # Store prediction
            outputs[:, t, :] = output
            
            # Teacher forcing: use ground truth or model prediction
            teacher_force = random.random() < teacher_forcing_ratio
            
            # Get the highest predicted token
            top1 = output.argmax(1)
            
            # Next input is either ground truth or predicted token
            input = tgt[:, t] if teacher_force else top1
        
        return outputs

# Example usage
input_dim = 5000  # Source vocabulary size
output_dim = 6000  # Target vocabulary size
embedding_dim = 256
hidden_dim = 512
n_layers = 2
dropout = 0.5

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

encoder = Encoder(input_dim, embedding_dim, hidden_dim, n_layers, dropout)
decoder = Decoder(output_dim, embedding_dim, hidden_dim, n_layers, dropout)
model = Seq2Seq(encoder, decoder, device).to(device)

# Create dummy data (batch of 3 sequences)
src = torch.randint(1, input_dim, (3, 10)).to(device)  # 3 sequences of length 10
tgt = torch.randint(1, output_dim, (3, 12)).to(device)  # 3 sequences of length 12

# Forward pass
output = model(src, tgt)
print(f"Output shape: {output.shape}")  # Should be [3, 12, 6000]
```

## Inference Process

During inference (after training), the model works differently:

1. Encode the input sequence to get the context vector
2. Initialize the decoder with the context vector and a <START> token
3. Generate the first output token
4. Feed this generated token back to the decoder for the next step
5. Repeat until an <END> token is generated or maximum length is reached

## Beam Search Decoding

Instead of greedily selecting the most probable token at each step, beam search maintains a beam of k most likely partial sequences:

```python
def beam_search_decode(model, src, beam_width=3, max_length=50):
    model.eval()
    
    # Encode input
    _, hidden = model.encoder(src)
    
    # Start with <sos> token
    input = torch.tensor([sos_token_idx]).to(device)
    
    # Initial sequence has just the <sos> token
    sequences = [(input, 0.0, hidden)]  # (sequence, score, hidden_state)
    
    for _ in range(max_length):
        all_candidates = []
        
        # Expand each current candidate
        for seq, score, hidden in sequences:
            last_token = seq[-1].unsqueeze(0)
            
            # If <eos> token, keep sequence as is
            if last_token.item() == eos_token_idx:
                all_candidates.append((seq, score, hidden))
                continue
            
            # Get predictions
            output, new_hidden = model.decoder(last_token, hidden)
            
            # Get top k tokens
            probs, indices = output.topk(beam_width)
            
            for i in range(beam_width):
                new_token = indices[0][i].unsqueeze(0)
                new_score = score + probs[0][i].item()
                new_seq = torch.cat([seq, new_token])
                
                all_candidates.append((new_seq, new_score, new_hidden))
        
        # Select k best candidates
        sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]
        
        # Check if all sequences end with <eos>
        if all(seq[0][-1].item() == eos_token_idx for seq in sequences):
            break
    
    # Return the highest scoring sequence
    return sequences[0][0]
```

## Applications

1. **Machine Translation**: Translating text between languages
2. **Text Summarization**: Generating concise summaries from longer texts
3. **Dialogue Systems**: Creating conversational agents
4. **Speech Recognition**: Converting speech to text
5. **Code Generation**: Translating natural language to code

## Limitations

1. **Information bottleneck**: The fixed-size context vector struggles with long sequences
2. **Vanishing context**: Information from early input tokens gets diluted
3. **Lack of parallelization**: Sequential processing is computationally inefficient
4. **Exposure bias**: Discrepancy between training (teacher forcing) and inference

These limitations led to the development of attention mechanisms and eventually Transformer architectures, which addressed many of these issues. However, seq2seq models remain conceptually important and are still used in many applications, especially with attention enhancement.
