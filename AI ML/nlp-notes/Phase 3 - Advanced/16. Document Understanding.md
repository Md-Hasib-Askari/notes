# Document Understanding

Document understanding combines multiple NLP techniques to comprehend, analyze, and extract information from documents of various types. This field bridges the gap between unstructured text and structured, actionable knowledge.

## Core Components

Document understanding typically involves:
1. **Document parsing**: Converting documents to machine-readable text
2. **Structure analysis**: Identifying sections, headers, lists, tables
3. **Entity extraction**: Recognizing key entities and concepts
4. **Semantic analysis**: Understanding meanings and relationships
5. **Knowledge integration**: Connecting document information with existing knowledge

## Document Parsing

The first step is to extract text from various document formats:

```python
import fitz  # PyMuPDF
import docx
import pandas as pd
from bs4 import BeautifulSoup
import requests
import pytesseract
from PIL import Image

class DocumentParser:
    """Parse different document types into plain text"""
    
    def parse_pdf(self, pdf_path):
        """Extract text from PDF"""
        text = ""
        doc = fitz.open(pdf_path)
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text += page.get_text()
        
        return text
    
    def parse_docx(self, docx_path):
        """Extract text from DOCX"""
        doc = docx.Document(docx_path)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text
    
    def parse_html(self, html_path=None, url=None):
        """Extract text from HTML file or URL"""
        if url:
            response = requests.get(url)
            html_content = response.text
        else:
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.extract()
        
        # Get text
        text = soup.get_text(separator='\n')
        
        # Clean up whitespace
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return text
    
    def parse_image(self, image_path, lang='eng'):
        """Extract text from image using OCR"""
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang=lang)
        return text
    
    def parse_csv(self, csv_path):
        """Extract text from CSV"""
        df = pd.read_csv(csv_path)
        text = df.to_string()
        return text

# Example usage
parser = DocumentParser()
pdf_text = parser.parse_pdf("document.pdf")
docx_text = parser.parse_docx("document.docx")
html_text = parser.parse_html(url="https://example.com")
```

## Document Structure Analysis

Analyzing document structure helps understand its organization:

```python
import re
import spacy
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

class DocumentStructureAnalyzer:
    def __init__(self):
        self.nlp = spacy.load("en_core_web_sm")
        
        # Load section classifier
        self.tokenizer = AutoTokenizer.from_pretrained("jjzha/sci-doc-classification")
        self.model = AutoModelForTokenClassification.from_pretrained("jjzha/sci-doc-classification")
        self.id2label = {
            0: "TITLE",
            1: "ABSTRACT",
            2: "INTRODUCTION",
            3: "METHODS",
            4: "RESULTS",
            5: "DISCUSSION",
            6: "CONCLUSION",
            7: "REFERENCES",
            8: "OTHER"
        }
    
    def extract_sections(self, text):
        """Extract sections from document text"""
        # Split text into paragraphs
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        
        # Identify section headers
        section_pattern = re.compile(r'^([0-9.]*\s*[A-Z][A-Za-z\s]+)(?:\n|:)')
        sections = []
        current_section = {"title": "INTRODUCTION", "content": ""}
        
        for paragraph in paragraphs:
            # Check if paragraph is a section header
            header_match = section_pattern.match(paragraph)
            if header_match:
                # Save previous section
                if current_section["content"]:
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    "title": header_match.group(1).strip(),
                    "content": paragraph[header_match.end():].strip()
                }
            else:
                # Add to current section
                current_section["content"] += "\n" + paragraph
        
        # Add the last section
        if current_section["content"]:
            sections.append(current_section)
        
        return sections
    
    def classify_sections(self, sections):
        """Classify sections into predefined categories"""
        classified_sections = []
        
        for section in sections:
            # Tokenize section title and first paragraph
            sample_text = section["title"] + "\n" + section["content"][:200]
            inputs = self.tokenizer(sample_text, return_tensors="pt", truncation=True)
            
            # Predict section type
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.argmax(outputs.logits, dim=2)
                section_type_id = predictions[0][0].item()  # Take prediction for first token
            
            # Get section type label
            section_type = self.id2label.get(section_type_id, "OTHER")
            
            classified_sections.append({
                "title": section["title"],
                "content": section["content"],
                "type": section_type
            })
        
        return classified_sections
    
    def extract_tables(self, text):
        """Extract table-like structures from text"""
        # Simple heuristic: Look for consecutive lines with similar structure
        lines = text.split('\n')
        tables = []
        current_table = []
        
        for i, line in enumerate(lines):
            # Check if line has table-like structure (multiple spaces or pipe characters)
            if '|' in line or re.search(r'\s{3,}', line):
                current_table.append(line)
            elif current_table and i > 0 and ('|' in lines[i-1] or re.search(r'\s{3,}', lines[i-1])):
                # Empty line after table
                if len(current_table) > 2:  # Ensure it's a real table (more than 2 rows)
                    tables.append("\n".join(current_table))
                current_table = []
        
        # Add the last table if exists
        if len(current_table) > 2:
            tables.append("\n".join(current_table))
        
        return tables

# Example usage
analyzer = DocumentStructureAnalyzer()
sections = analyzer.extract_sections(pdf_text)
classified_sections = analyzer.classify_sections(sections)
tables = analyzer.extract_tables(pdf_text)

for section in classified_sections:
    print(f"Section: {section['title']} (Type: {section['type']})")
    print(f"Content preview: {section['content'][:100]}...")
    print()
```

## Semantic Document Analysis

Extract key information and meaning from documents:

```python
from transformers import pipeline
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

class DocumentAnalyzer:
    def __init__(self):
        # Load NLP pipelines
        self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        self.ner = pipeline("ner", model="dslim/bert-base-NER")
        self.classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
        
        # For keyword extraction
        self.vectorizer = TfidfVectorizer(max_df=0.85, min_df=2, stop_words='english')
    
    def summarize(self, text, max_length=150, min_length=50):
        """Generate a summary of the document"""
        # Split into chunks if text is too long
        max_chunk_size = 1024
        chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        
        summaries = []
        for chunk in chunks:
            if len(chunk) < 50:  # Skip very short chunks
                continue
                
            summary = self.summarizer(chunk, max_length=max_length//len(chunks), 
                                     min_length=min_length//len(chunks))
            summaries.append(summary[0]['summary_text'])
        
        return " ".join(summaries)
    
    def extract_entities(self, text):
        """Extract named entities from text"""
        # Process in chunks to handle long documents
        chunk_size = 512
        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
        
        all_entities = []
        for chunk in chunks:
            entities = self.ner(chunk)
            
            # Merge adjacent entities of same type
            merged_entities = []
            current_entity = None
            
            for entity in entities:
                if current_entity and entity['entity'].startswith('I-') and \
                   current_entity['entity'].replace('B-', '') == entity['entity'].replace('I-', ''):
                    # Extend current entity
                    current_entity['word'] += entity['word'].replace('##', '')
                    current_entity['end'] = entity['end']
                else:
                    # Save previous entity
                    if current_entity:
                        merged_entities.append(current_entity)
                    
                    # Start new entity
                    if entity['entity'].startswith('B-'):
                        current_entity = {
                            'entity': entity['entity'],
                            'word': entity['word'],
                            'start': entity['start'],
                            'end': entity['end'],
                            'score': entity['score']
                        }
                    else:
                        current_entity = None
            
            if current_entity:
                merged_entities.append(current_entity)
            
            all_entities.extend(merged_entities)
        
        # Group by entity type
        grouped_entities = {}
        for entity in all_entities:
            entity_type = entity['entity'].replace('B-', '').replace('I-', '')
            if entity_type not in grouped_entities:
                grouped_entities[entity_type] = []
            grouped_entities[entity_type].append(entity['word'])
        
        return grouped_entities
    
    def extract_keywords(self, text, top_n=10):
        """Extract keywords using TF-IDF"""
        # Split into sentences
        sentences = [sent.strip() for sent in re.split(r'[.!?]', text) if sent.strip()]
        
        # Calculate TF-IDF
        try:
            tfidf_matrix = self.vectorizer.fit_transform(sentences)
            feature_names = self.vectorizer.get_feature_names_out()
            
            # Get top terms for each sentence
            keywords = []
            for i in range(len(sentences)):
                tfidf_row = tfidf_matrix[i].toarray()[0]
                top_indices = tfidf_row.argsort()[-5:][::-1]  # Top 5 terms per sentence
                keywords.extend([feature_names[idx] for idx in top_indices])
            
            # Count keyword occurrences and get top_n
            from collections import Counter
            keyword_counts = Counter(keywords)
            top_keywords = keyword_counts.most_common(top_n)
            
            return top_keywords
        except ValueError:
            # Fallback if TF-IDF fails (e.g., not enough documents)
            return []
    
    def analyze_sentiment(self, text):
        """Analyze sentiment of document sections"""
        # Split into paragraphs
        paragraphs = [p for p in text.split('\n\n') if len(p.strip()) > 50]
        
        # Analyze sentiment of each paragraph
        sentiments = []
        for paragraph in paragraphs:
            result = self.classifier(paragraph)
            sentiments.append({
                'text': paragraph[:100] + '...',
                'label': result[0]['label'],
                'score': result[0]['score']
            })
        
        # Calculate overall sentiment
        positive_count = sum(1 for s in sentiments if s['label'] == 'POSITIVE')
        negative_count = len(sentiments) - positive_count
        
        return {
            'overall': 'POSITIVE' if positive_count > negative_count else 'NEGATIVE',
            'positive_ratio': positive_count / len(sentiments) if sentiments else 0,
            'detailed': sentiments
        }
    
    def create_entity_graph(self, entities, text):
        """Create a graph of entity relationships"""
        G = nx.Graph()
        
        # Add entities as nodes
        for entity_type, entity_list in entities.items():
            for entity in set(entity_list):  # Remove duplicates
                G.add_node(entity, type=entity_type)
        
        # Add edges based on co-occurrence
        sentences = [sent.strip() for sent in re.split(r'[.!?]', text) if sent.strip()]
        
        for sentence in sentences:
            sentence_entities = []
            for node in G.nodes():
                if node in sentence:
                    sentence_entities.append(node)
            
            # Create edges between entities in same sentence
            for i in range(len(sentence_entities)):
                for j in range(i+1, len(sentence_entities)):
                    if G.has_edge(sentence_entities[i], sentence_entities[j]):
                        # Increment weight if edge exists
                        G[sentence_entities[i]][sentence_entities[j]]['weight'] += 1
                    else:
                        # Create new edge
                        G.add_edge(sentence_entities[i], sentence_entities[j], weight=1)
        
        return G

# Example usage
analyzer = DocumentAnalyzer()
summary = analyzer.summarize(pdf_text)
entities = analyzer.extract_entities(pdf_text)
keywords = analyzer.extract_keywords(pdf_text)
sentiment = analyzer.analyze_sentiment(pdf_text)
entity_graph = analyzer.create_entity_graph(entities, pdf_text)

print(f"Summary: {summary}")
print(f"Top Keywords: {keywords}")
print(f"Overall Sentiment: {sentiment['overall']} ({sentiment['positive_ratio']:.2f} positive)")
print("Entities:")
for entity_type, entity_list in entities.items():
    print(f"  {entity_type}: {', '.join(set(entity_list[:5]))}")
```

## Applications

1. **Legal document analysis**: Contract review, clause extraction
2. **Scientific literature review**: Research summarization, knowledge extraction
3. **Business intelligence**: Market reports, competitor analysis
4. **Customer feedback analysis**: Review understanding, sentiment extraction
5. **Financial document processing**: SEC filings, earnings reports, risk assessment

## Challenges and Best Practices

1. **Multi-format handling**: Process different document types (PDFs, Word, HTML)
2. **Layout understanding**: Interpret tables, figures, and complex structures
3. **Domain adaptation**: Customize analysis for specific industries or domains
4. **Long document processing**: Handle lengthy documents efficiently
5. **Error handling**: Manage OCR errors and formatting issues

Document understanding represents a holistic approach to text analysis, combining multiple NLP techniques to extract structured knowledge from unstructured documents, enabling deeper insights and more effective information management.
