# Word2Vec

Word2Vec is a widely used word embedding technique that represents words as dense vectors in a continuous vector space, capturing semantic relationships between words.

## Core Concept

Word2Vec transforms words into numerical vectors where similar words cluster together in the vector space. The key insight is that words appearing in similar contexts tend to have similar meanings.

## Two Main Architectures

1. **Continuous Bag of Words (CBOW)**: Predicts the target word from context words
2. **Skip-gram**: Predicts context words from the target word

Skip-gram generally works better for infrequent words and smaller datasets, while CBOW is faster and better for frequent words.

## Properties of Word2Vec Embeddings

- Words with similar meanings have vectors close to each other
- Vector arithmetic captures semantic relationships:
  - `vector("king") - vector("man") + vector("woman") â‰ˆ vector("queen")`
- Typical dimensionality: 100-300 dimensions
- Unsupervised learning (uses only raw text)

## Implementation with Gensim

```python
import gensim.downloader as api
from gensim.models import Word2Vec
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Method 1: Train from scratch
sentences = [
    ["I", "love", "natural", "language", "processing"],
    ["word", "embeddings", "are", "powerful", "for", "nlp", "tasks"],
    # Add more sentences...
]

# Train model
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Save and load
model.save("word2vec.model")
model = Word2Vec.load("word2vec.model")

# Method 2: Use pre-trained models
pretrained_model = api.load("word2vec-google-news-300")

# Find similar words
similar_words = pretrained_model.most_similar("python", topn=5)
print(similar_words)

# Word analogies
result = pretrained_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
print(f"king - man + woman = {result}")

# Get vector for a word
word_vector = pretrained_model["computer"]  # numpy array of size 300

# Visualize word embeddings in 2D
def plot_embeddings(model, words):
    # Extract word vectors
    word_vectors = np.array([model[word] for word in words])
    
    # Reduce to 2D using PCA
    pca = PCA(n_components=2)
    result = pca.fit_transform(word_vectors)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.scatter(result[:, 0], result[:, 1], c='blue')
    
    # Add word labels
    for i, word in enumerate(words):
        plt.annotate(word, xy=(result[i, 0], result[i, 1]))
    
    plt.title("Word Embeddings Visualization")
    plt.show()

# Example words to visualize
words_to_plot = ["king", "queen", "man", "woman", "paris", "france", "berlin", "germany"]
plot_embeddings(pretrained_model, words_to_plot)
```

## Applications

1. **Text Classification**: Feature engineering for ML models
2. **Named Entity Recognition**: Improved recognition with contextual understanding
3. **Machine Translation**: Initial approach to translate words across languages
4. **Document Similarity**: Compare documents through their aggregated word vectors
5. **Recommendation Systems**: Find similar items through text descriptions

## Limitations

- Cannot handle out-of-vocabulary words
- Single representation per word (doesn't capture polysemy)
- Contextual meaning is limited
- Requires significant training data for high-quality embeddings

## Best Practices

- Use pre-trained models for small datasets
- Fine-tune embeddings for specific domains
- Consider context window size (larger captures more topical similarity, smaller captures more functional similarity)
- Normalize vectors when using for distance calculations
- Consider subword information for handling rare words

Word2Vec revolutionized NLP by introducing dense vector representations, paving the way for more advanced embedding techniques and deep learning models in NLP.
