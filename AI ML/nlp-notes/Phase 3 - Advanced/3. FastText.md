# FastText

FastText is an extension of Word2Vec developed by Facebook Research in 2016 that treats each word as a bag of character n-grams. This approach allows the model to generate embeddings for out-of-vocabulary words and better handle morphologically rich languages.

## Core Concept

While Word2Vec and GloVe learn vectors for whole words, FastText breaks words into character n-grams and learns representations for these subword units. The final word representation is the sum of its constituent n-gram vectors.

For example, with n-grams of length 3-6, the word "apple" would be represented as:
- Special boundary markers: \<apple\>
- Character n-grams: \<ap, app, appl, apple, ppl, pple, ple, le\>
- Word itself: apple

## Key Features

1. **Subword information**: Captures morphological structure of languages
2. **Out-of-vocabulary handling**: Can generate embeddings for unseen words
3. **Better for rare words**: Shares subword information across the vocabulary
4. **Language agnostic**: Works well for languages with complex morphology (Finnish, Turkish, etc.)
5. **Efficiency**: Fast training despite modeling subword units

## Implementation with Python

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from gensim.models import FastText
import gensim.downloader as api

# Method 1: Train from scratch
sentences = [
    ["I", "love", "natural", "language", "processing"],
    ["fasttext", "handles", "out-of-vocabulary", "words", "well"],
    ["it", "works", "great", "for", "morphologically", "rich", "languages"],
    # Add more sentences...
]

# Training with subword information
model = FastText(sentences, vector_size=100, window=5, min_count=1, 
                 min_n=3, max_n=6, epochs=10, workers=4)

# Save and load
model.save("fasttext.model")
model = FastText.load("fasttext.model")

# Method 2: Use pre-trained models
wiki_model = api.load("fasttext-wiki-news-subwords-300")

# Find similar words
similar_words = model.wv.most_similar("processing", topn=5)
print("Words similar to 'processing':", similar_words)

# Word analogies (works like Word2Vec)
result = wiki_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
print(f"king - man + woman = {result}")

# Get vector for a word (even for OOV words)
word_vector = model.wv["pythonic"]  # Works even if 'pythonic' wasn't in training
print("Vector shape:", word_vector.shape)

# Visualize embeddings
def plot_fasttext_embeddings(model, words):
    vectors = []
    valid_words = []
    
    for word in words:
        try:
            vectors.append(model.wv[word])
            valid_words.append(word)
        except KeyError:
            print(f"Word '{word}' not in vocabulary but FastText can still generate a vector!")
            vectors.append(model.wv[word])  # FastText can generate vectors for OOV words
            valid_words.append(word + "*")  # Mark OOV words with asterisk
    
    # Reduce to 2D
    pca = PCA(n_components=2)
    result = pca.fit_transform(np.array(vectors))
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.scatter(result[:, 0], result[:, 1], c='green')
    
    # Add word labels
    for i, word in enumerate(valid_words):
        plt.annotate(word, xy=(result[i, 0], result[i, 1]))
    
    plt.title("FastText Word Embeddings")
    plt.grid(True)
    plt.show()

# Example usage - includes an OOV word 'unhappiness'
words_to_plot = ["happy", "unhappy", "unhappiness", "good", "bad", "terrible", "excellent"]
plot_fasttext_embeddings(model, words_to_plot)

# Demonstrate OOV capabilities
def test_oov_capabilities(model, known_word, derived_words):
    """Test FastText's ability to handle derived words"""
    print(f"Testing OOV capabilities starting from '{known_word}':")
    
    # Get vector for known word
    base_vector = model.wv[known_word]
    
    for derived in derived_words:
        # Calculate similarity even for words not seen during training
        similarity = model.wv.similarity(known_word, derived)
        print(f"Similarity between '{known_word}' and '{derived}': {similarity:.4f}")

# Example
test_oov_capabilities(model, "process", ["processing", "preprocessed", "reprocessing"])
```

## Applications

1. **Morphologically rich languages**: Superior performance for Finnish, Turkish, etc.
2. **Spelling error tolerance**: Robust to typos and spelling variations
3. **Domain-specific terminology**: Generates reasonable vectors for technical terms
4. **Social media text**: Handles misspellings and informal language
5. **Small training datasets**: More effective than Word2Vec with limited data

## Comparison with Word2Vec and GloVe

| Feature | FastText | Word2Vec | GloVe |
|---------|----------|----------|-------|
| OOV handling | Yes | No | No |
| Morphology awareness | High | None | None |
| Training speed | Slower | Fast | Fast |
| Memory usage | Higher | Medium | Medium |
| Performance on rare words | Better | Poor | Poor |
| Semantic relationships | Good | Good | Very good |

## Limitations

1. **Increased model size**: Storing n-gram vectors increases memory requirements
2. **Training time**: Slower than Word2Vec due to subword processing
3. **May lose word-level semantics**: Sometimes too focused on morphology
4. **Not ideal for ideographic languages**: Less effective for Chinese, Japanese

## Best Practices

1. **N-gram selection**: Adjust min_n and max_n based on language morphology (3-6 is standard)
2. **Preprocessing**: Less preprocessing needed than Word2Vec (can keep case, punctuation)
3. **Vector dimension**: 100-300 dimensions work well for most applications
4. **Model selection**: Use pre-trained models (wiki-news-300d-1M) for general language
5. **Fine-tuning**: Consider fine-tuning on domain-specific data

FastText represents a significant advancement in word embeddings by incorporating subword information, making it particularly valuable for morphologically rich languages and handling out-of-vocabulary words. Its ability to generate embeddings for previously unseen words addresses a major limitation of earlier embedding techniques.
