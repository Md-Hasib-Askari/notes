# GloVe (Global Vectors for Word Representation)

GloVe is a word embedding technique that combines the benefits of two major methods: matrix factorization and local context window methods. Developed by Stanford researchers in 2014, it addresses some limitations of Word2Vec.

## Core Concept

GloVe leverages global statistics of word co-occurrences in a corpus, while also capturing the local context similar to Word2Vec. It trains on global word-word co-occurrence counts rather than individual local contexts.

## How GloVe Works

1. **Build co-occurrence matrix**: Count how often each word appears in the context of every other word
2. **Apply weighted factorization**: Factorize the logarithm of the co-occurrence matrix
3. **Optimize word vectors**: Minimize the difference between dot products of word vectors and the logarithm of their co-occurrence probability

The model uses this objective function:
J(θ) = Σ f(Xᵢⱼ)(wᵢᵀw̃ⱼ + bᵢ + b̃ⱼ - log(Xᵢⱼ))²

Where:
- Xᵢⱼ is the co-occurrence count between words i and j
- f(Xᵢⱼ) is a weighting function that prevents rare co-occurrences from being overweighted
- wᵢ and w̃ⱼ are word vectors
- bᵢ and b̃ⱼ are bias terms

## Implementation with Python

```python
import numpy as np
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import os
import requests
from zipfile import ZipFile

# Download and prepare GloVe embeddings
def download_glove(dimension=100):
    """Download and prepare GloVe embeddings"""
    # URLs for different dimensions
    urls = {
        50: "https://nlp.stanford.edu/data/glove.6B.50d.zip",
        100: "https://nlp.stanford.edu/data/glove.6B.100d.zip",
        200: "https://nlp.stanford.edu/data/glove.6B.200d.zip",
        300: "https://nlp.stanford.edu/data/glove.6B.300d.zip"
    }
    
    os.makedirs('glove', exist_ok=True)
    zip_path = f'glove/glove.6B.{dimension}d.zip'
    glove_path = f'glove/glove.6B.{dimension}d.txt'
    word2vec_path = f'glove/glove.6B.{dimension}d.word2vec.txt'
    
    # Download if not exists
    if not os.path.exists(glove_path):
        print(f"Downloading GloVe {dimension}d embeddings...")
        response = requests.get(urls[dimension])
        with open(zip_path, 'wb') as f:
            f.write(response.content)
        
        # Extract
        with ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall('glove')
    
    # Convert to word2vec format if needed
    if not os.path.exists(word2vec_path):
        glove2word2vec(glove_path, word2vec_path)
    
    return word2vec_path

# Load pre-trained GloVe embeddings
glove_path = download_glove(dimension=100)
glove_model = KeyedVectors.load_word2vec_format(glove_path)

# Find similar words
similar_words = glove_model.most_similar("computer", topn=5)
print("Words similar to 'computer':", similar_words)

# Word analogies
result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
print(f"king - man + woman = {result}")

# Visualize word embeddings
def plot_embeddings(model, words):
    word_vectors = np.array([model[word] for word in words if word in model])
    valid_words = [word for word in words if word in model]
    
    # Reduce dimensions with PCA
    pca = PCA(n_components=2)
    result = pca.fit_transform(word_vectors)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.scatter(result[:, 0], result[:, 1], c='red')
    
    # Add word labels
    for i, word in enumerate(valid_words):
        plt.annotate(word, xy=(result[i, 0], result[i, 1]))
    
    plt.title("GloVe Word Embeddings")
    plt.grid(True)
    plt.show()

# Visualize relationships
words_to_plot = ["king", "queen", "man", "woman", "paris", "france", "london", "england"]
plot_embeddings(glove_model, words_to_plot)
```

## Advantages of GloVe

1. **Efficient use of statistics**: Leverages global co-occurrence statistics
2. **Computational efficiency**: Trains faster than Word2Vec on large corpora
3. **Performance**: Often captures semantic relationships better than pure window-based methods
4. **Flexibility**: Allows different weighting of co-occurrences
5. **Space efficiency**: The co-occurrence matrix can be built once and reused

## Limitations

1. **Memory intensive**: Building the co-occurrence matrix requires significant memory for large corpora
2. **Static embeddings**: Like Word2Vec, GloVe provides one vector per word, not capturing polysemy
3. **No out-of-vocabulary handling**: Cannot generate embeddings for unseen words
4. **Requires sufficient corpus size**: Performance degrades with small training corpora

## Best Practices

1. **Dimension selection**: 100-300 dimensions usually provides good performance
2. **Pre-trained vs custom**: Use pre-trained vectors unless you have domain-specific needs
3. **Text preprocessing**: Consistent tokenization between training and application
4. **Vector normalization**: Normalize vectors before similarity calculations
5. **Ensemble approach**: Consider combining with other embedding techniques for robust performance

## Applications

1. **Semantic similarity**: Document clustering and retrieval
2. **Text classification**: Feature generation for ML models
3. **Information extraction**: Named entity recognition and relation extraction
4. **Machine translation**: Input representations for neural translation models
5. **Question answering**: Understanding query semantics

GloVe's ability to capture both global statistics and local context information makes it a powerful tool for many NLP tasks, offering a good balance between computational efficiency and performance.
