# GPT Models and Autoregressive Generation

Generative Pre-trained Transformer (GPT) models represent a family of autoregressive language models that have revolutionized text generation capabilities. Unlike bidirectional models like BERT, GPT models are trained to predict the next token in a sequence, enabling powerful generative abilities.

## Core Architecture

GPT models use a decoder-only Transformer architecture with:
- **Causal self-attention**: Each token can only attend to itself and previous tokens
- **Autoregressive formulation**: Predict the next token based on all previous tokens
- **Scaled-up design**: Modern versions employ billions of parameters

## Evolution of GPT Models

1. **GPT-1** (2018): 12 layers, 117M parameters
2. **GPT-2** (2019): Up to 48 layers, 1.5B parameters
3. **GPT-3** (2020): 96 layers, 175B parameters
4. **GPT-4** (2023): Architecture details partially undisclosed, multimodal capabilities

Each generation brought significant improvements in generation quality, reasoning, and instruction following.

## Autoregressive Generation

GPT models generate text by repeatedly sampling the next token based on previously generated tokens:

```python
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained model and tokenizer
model_name = "gpt2"  # Options: "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set the model to evaluation mode
model.eval()

# Function for text generation
def generate_text(prompt, max_length=100, temperature=0.7, top_k=50, top_p=0.95):
    """
    Generate text using GPT-2 model
    
    Args:
        prompt: The input text to continue
        max_length: Maximum length of generated text (including prompt)
        temperature: Controls randomness (lower = more deterministic)
        top_k: Number of highest probability tokens to consider
        top_p: Cumulative probability threshold for nucleus sampling
    
    Returns:
        Generated text including the original prompt
    """
    # Encode the prompt
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    # Generate text
    output = model.generate(
        inputs,
        max_length=max_length,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=2
    )
    
    # Decode and return the generated text
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Example usage
prompt = "Artificial intelligence has transformed the way we"
generated_text = generate_text(prompt)
print(generated_text)
```

## Decoding Strategies

Several decoding strategies can be used during generation:

1. **Greedy decoding**: Always select the most probable next token
2. **Beam search**: Maintain multiple candidate sequences
3. **Top-K sampling**: Sample from K most likely tokens
4. **Nucleus (Top-p) sampling**: Sample from the smallest set of tokens whose cumulative probability exceeds p
5. **Temperature sampling**: Control randomness by scaling logits

```python
# Implement custom autoregressive generation with different decoding strategies
def custom_generate(prompt, strategy='top_p', max_length=50):
    # Encode prompt
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    # Initialize past for faster generation
    past = None
    
    # Generate tokens one by one
    for _ in range(max_length):
        with torch.no_grad():
            # Get model outputs
            outputs = model(input_ids, past_key_values=past)
            logits = outputs.logits[:, -1, :]  # Get logits for next token
            past = outputs.past_key_values  # Cache key/values for efficiency
            
            # Apply decoding strategy
            if strategy == 'greedy':
                # Greedy decoding - take most likely token
                next_token = torch.argmax(logits, dim=-1).unsqueeze(0)
            
            elif strategy == 'top_k':
                # Top-K sampling
                k = 50
                top_k_logits, top_k_indices = torch.topk(logits, k)
                
                # Apply temperature
                probs = F.softmax(top_k_logits / 0.7, dim=-1)
                
                # Sample from the distribution
                next_token_id = torch.multinomial(probs, num_samples=1)
                next_token = top_k_indices[:, next_token_id.item()].unsqueeze(0)
            
            elif strategy == 'top_p':
                # Top-p (nucleus) sampling
                p = 0.9
                
                # Sort logits in descending order
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                
                # Calculate cumulative probabilities
                sorted_probs = F.softmax(sorted_logits / 0.7, dim=-1)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                
                # Remove tokens with cumulative probability above the threshold
                sorted_indices_to_keep = cumulative_probs < p
                
                # Keep also the first token above the threshold
                sorted_indices_to_keep[..., 1:] = sorted_indices_to_keep[..., :-1].clone()
                sorted_indices_to_keep[..., 0] = 1
                
                # Get the indices of the kept tokens
                indices_to_keep = sorted_indices[sorted_indices_to_keep]
                
                # Sample from the reduced distribution
                probs = F.softmax(logits[:, indices_to_keep] / 0.7, dim=-1)
                next_token_id = torch.multinomial(probs, num_samples=1)
                next_token = indices_to_keep[next_token_id.item()].unsqueeze(0)
            
            # Add the chosen token to the input sequence
            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)
            
            # Stop if we generate an EOS token
            if next_token.item() == tokenizer.eos_token_id:
                break
    
    # Decode the generated sequence
    return tokenizer.decode(input_ids[0], skip_special_tokens=True)
```

## Fine-tuning GPT Models

Fine-tuning GPT models for specific tasks or domains can improve performance:

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments

def fine_tune_gpt(train_file_path, output_dir):
    """Fine-tune GPT-2 on custom text data"""
    # Load model and tokenizer
    model_name = "gpt2"
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    
    # Special tokens
    tokenizer.pad_token = tokenizer.eos_token
    
    # Load dataset
    train_dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=train_file_path,
        block_size=128
    )
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Not using masked language modeling
    )
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        save_steps=10_000,
        save_total_limit=2,
        prediction_loss_only=True,
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
    )
    
    # Train the model
    trainer.train()
    
    # Save the model
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    return model, tokenizer
```

## In-context Learning

A key innovation in GPT models is their ability to learn from examples provided in the prompt:

```python
# Few-shot learning with GPT
def few_shot_classification(examples, new_instance):
    """Use in-context learning for classification"""
    # Format the prompt with examples
    prompt = "Classify the text as positive or negative:\n\n"
    
    # Add few-shot examples
    for text, label in examples:
        prompt += f"Text: {text}\nLabel: {label}\n\n"
    
    # Add the new instance to classify
    prompt += f"Text: {new_instance}\nLabel:"
    
    # Generate the classification
    response = generate_text(prompt, max_length=len(prompt) + 10)
    
    # Extract the generated label
    return response[len(prompt):].strip()

# Example usage
examples = [
    ("I love this product, it works great!", "positive"),
    ("The quality is terrible and it broke right away.", "negative"),
    ("Amazing customer service, highly recommend!", "positive")
]

new_text = "I'm disappointed with my purchase."
prediction = few_shot_classification(examples, new_text)
print(f"Predicted label: {prediction}")
```

## Applications

1. **Creative writing**: Stories, poetry, scripts
2. **Content generation**: Articles, product descriptions, marketing copy
3. **Conversational AI**: Chatbots, virtual assistants
4. **Code generation**: Programming assistance
5. **Text summarization**: Condensing documents
6. **Translation**: Converting between languages

## Limitations and Ethical Considerations

1. **Hallucinations**: Generation of false or misleading information
2. **Bias amplification**: Replicating biases present in training data
3. **Lack of grounding**: No direct access to factual knowledge
4. **Context window limitations**: Limited memory of previous conversation
5. **Ethical concerns**: Potential for misuse in generating harmful content

GPT models have transformed NLP by demonstrating that large-scale unsupervised pre-training can yield powerful language models capable of complex generation, reasoning, and task adaptation through prompting, with minimal task-specific training.
