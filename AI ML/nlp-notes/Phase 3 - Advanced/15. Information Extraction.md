# Information Extraction

Information Extraction (IE) involves automatically extracting structured information from unstructured text. This includes identifying entities, relationships, events, and attributes to create structured data that can be used for various downstream applications.

## Core IE Tasks

1. **Named Entity Recognition (NER)**: Identifying and classifying entities (people, organizations, locations, etc.)
2. **Relation Extraction (RE)**: Determining relationships between entities
3. **Event Extraction**: Identifying events and their participants
4. **Coreference Resolution**: Resolving references to the same entity
5. **Temporal Information Extraction**: Extracting and normalizing time expressions

## Named Entity Recognition

NER identifies entities in text and classifies them into predefined categories:

```python
import spacy
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import numpy as np

def spacy_ner(text):
    """Extract entities using spaCy"""
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    
    entities = []
    for ent in doc.ents:
        entities.append({
            "text": ent.text,
            "start": ent.start_char,
            "end": ent.end_char,
            "type": ent.label_
        })
    
    return entities

def transformer_ner(text):
    """Extract entities using transformer model"""
    tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
    model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")
    
    # Tokenize input
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    
    # Forward pass
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Get predicted labels
    predictions = torch.argmax(outputs.logits, dim=2)
    token_predictions = [model.config.id2label[t.item()] for t in predictions[0]]
    
    # Align predictions with original text
    word_ids = inputs.word_ids()
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    
    # Extract entities
    entities = []
    current_entity = {"text": "", "type": "", "start": -1, "end": -1}
    
    for idx, (token, pred) in enumerate(zip(tokens, token_predictions)):
        # Skip special tokens
        if word_ids[idx] is None:
            continue
        
        # Handle B- (beginning of entity)
        if pred.startswith("B-"):
            # Save previous entity if exists
            if current_entity["text"]:
                entities.append(current_entity.copy())
            
            # Start new entity
            entity_type = pred[2:]  # Remove "B-" prefix
            token_text = token.replace("##", "")
            
            current_entity = {
                "text": token_text,
                "type": entity_type,
                "start": text.find(token_text),
                "end": text.find(token_text) + len(token_text)
            }
        
        # Handle I- (inside entity)
        elif pred.startswith("I-"):
            token_text = token.replace("##", "")
            current_entity["text"] += " " + token_text
            current_entity["end"] = current_entity["end"] + len(token_text) + 1
        
        # Handle O (outside entity)
        elif pred == "O" and current_entity["text"]:
            entities.append(current_entity.copy())
            current_entity = {"text": "", "type": "", "start": -1, "end": -1}
    
    # Add last entity if exists
    if current_entity["text"]:
        entities.append(current_entity)
    
    return entities

# Example usage
text = "Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976. The company is headquartered in Cupertino, California."
spacy_results = spacy_ner(text)
transformer_results = transformer_ner(text)

print("SpaCy NER Results:")
for entity in spacy_results:
    print(f"{entity['text']} - {entity['type']}")

print("\nTransformer NER Results:")
for entity in transformer_results:
    print(f"{entity['text']} - {entity['type']}")
```

## Relation Extraction

Relation extraction identifies semantic relationships between entities:

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import spacy
import itertools

class RelationExtractor:
    def __init__(self):
        # Load NER model for entity extraction
        self.nlp = spacy.load("en_core_web_sm")
        
        # Load relation classifier
        self.tokenizer = AutoTokenizer.from_pretrained("Rifky/relations-classification")
        self.model = AutoModelForSequenceClassification.from_pretrained("Rifky/relations-classification")
        
        # Relation labels
        self.id2label = {
            0: "no_relation",
            1: "org:founded_by",
            2: "per:employee_of",
            3: "org:alternate_names",
            4: "per:siblings",
            5: "per:spouse",
            # Add more relation types as needed
        }
    
    def extract_relations(self, text, threshold=0.5):
        """Extract relations from text"""
        # Extract entities
        doc = self.nlp(text)
        entities = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]
        
        # Create entity pairs
        entity_pairs = list(itertools.combinations(entities, 2))
        
        relations = []
        for entity1, entity2 in entity_pairs:
            # Format input for relation classifier
            e1_text, e1_type, e1_start, e1_end = entity1
            e2_text, e2_type, e2_start, e2_end = entity2
            
            # Mark entities in text
            marked_text = text[:e1_start] + f"<e1>{e1_text}</e1>" + text[e1_end:e2_start] + f"<e2>{e2_text}</e2>" + text[e2_end:]
            
            # Tokenize input
            inputs = self.tokenizer(marked_text, return_tensors="pt", padding=True, truncation=True)
            
            # Classify relation
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
            
            # Get predicted relation
            probs = torch.softmax(logits, dim=1)[0]
            pred_idx = torch.argmax(probs).item()
            confidence = probs[pred_idx].item()
            
            # Skip if no relation or low confidence
            if pred_idx == 0 or confidence < threshold:
                continue
            
            relation_type = self.id2label[pred_idx]
            relations.append({
                "head": e1_text,
                "head_type": e1_type,
                "relation": relation_type,
                "tail": e2_text,
                "tail_type": e2_type,
                "confidence": confidence
            })
        
        return relations

# Example usage
extractor = RelationExtractor()
text = "Apple Inc. was founded by Steve Jobs and Steve Wozniak in 1976. Tim Cook is the current CEO of Apple."
relations = extractor.extract_relations(text)

for relation in relations:
    print(f"{relation['head']} ({relation['head_type']}) {relation['relation']} {relation['tail']} ({relation['tail_type']})")
```

## Event Extraction

Event extraction identifies events and their arguments (participants, time, location):

```python
import spacy
from transformers import pipeline

class EventExtractor:
    def __init__(self):
        # Load spaCy for basic NLP
        self.nlp = spacy.load("en_core_web_sm")
        
        # Load QA pipeline for argument extraction
        self.qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")
    
    def extract_events(self, text):
        """Extract events and their arguments from text"""
        # Basic parsing
        doc = self.nlp(text)
        
        events = []
        # Look for sentences with action verbs
        for sent in doc.sents:
            # Find the main verb
            main_verbs = [token for token in sent if token.pos_ == "VERB" and token.dep_ in ("ROOT", "ccomp")]
            
            for verb in main_verbs:
                # Create event
                event = {
                    "trigger": verb.text,
                    "trigger_idx": verb.i,
                    "sentence": sent.text,
                    "arguments": []
                }
                
                # Extract subject (agent)
                subjects = [child for child in verb.children if child.dep_ in ("nsubj", "nsubjpass")]
                for subject in subjects:
                    # Get the full noun phrase
                    subject_span = self._get_span(subject)
                    event["arguments"].append({
                        "role": "Agent",
                        "text": subject_span.text
                    })
                
                # Extract object (patient)
                objects = [child for child in verb.children if child.dep_ in ("dobj", "pobj")]
                for obj in objects:
                    # Get the full noun phrase
                    object_span = self._get_span(obj)
                    event["arguments"].append({
                        "role": "Patient",
                        "text": object_span.text
                    })
                
                # Extract time expressions
                time_entities = [ent for ent in sent.ents if ent.label_ in ("DATE", "TIME")]
                for time_ent in time_entities:
                    event["arguments"].append({
                        "role": "Time",
                        "text": time_ent.text
                    })
                
                # Extract location
                loc_entities = [ent for ent in sent.ents if ent.label_ == "GPE"]
                for loc_ent in loc_entities:
                    event["arguments"].append({
                        "role": "Location",
                        "text": loc_ent.text
                    })
                
                # Use QA to extract missing arguments
                self._extract_missing_arguments(event)
                
                events.append(event)
        
        return events
    
    def _get_span(self, token):
        """Get the full noun phrase for a token"""
        if token.pos_ in ("NOUN", "PROPN"):
            # Find the head of the noun phrase
            head = token
            while head.head.pos_ in ("NOUN", "PROPN") and head.head.i < head.i:
                head = head.head
            
            # Get the full span
            span = doc[head.left_edge.i:token.right_edge.i+1]
            return span
        return token
    
    def _extract_missing_arguments(self, event):
        """Use QA to extract missing arguments"""
        # Check if we need to extract more arguments
        has_agent = any(arg["role"] == "Agent" for arg in event["arguments"])
        has_patient = any(arg["role"] == "Patient" for arg in event["arguments"])
        
        # Extract agent if missing
        if not has_agent:
            answer = self.qa_pipeline(
                question=f"Who {event['trigger']}?",
                context=event["sentence"]
            )
            if answer["score"] > 0.3:
                event["arguments"].append({
                    "role": "Agent",
                    "text": answer["answer"],
                    "confidence": answer["score"]
                })
        
        # Extract patient if missing
        if not has_patient:
            answer = self.qa_pipeline(
                question=f"What was {event['trigger']}?",
                context=event["sentence"]
            )
            if answer["score"] > 0.3:
                event["arguments"].append({
                    "role": "Patient",
                    "text": answer["answer"],
                    "confidence": answer["score"]
                })

# Example usage
event_extractor = EventExtractor()
text = "Apple announced its new iPhone model yesterday in Cupertino. Customers will be able to purchase it next month."
events = event_extractor.extract_events(text)

for event in events:
    print(f"Event: {event['trigger']}")
    for arg in event["arguments"]:
        print(f"  {arg['role']}: {arg['text']}")
```

## Applications

1. **Knowledge graph construction**: Building structured representations of information
2. **News analysis**: Extracting events and entities from news articles
3. **Biomedical research**: Extracting relationships between drugs, diseases, and genes
4. **Business intelligence**: Extracting competitor information and market trends
5. **Legal document analysis**: Extracting clauses, parties, and obligations

## Evaluation

IE systems are typically evaluated using:
- **Precision**: Proportion of extracted information that is correct
- **Recall**: Proportion of all correct information that was extracted
- **F1 Score**: Harmonic mean of precision and recall

Modern information extraction increasingly uses deep learning approaches, particularly transformer-based models, to achieve higher accuracy and better generalization across domains.
