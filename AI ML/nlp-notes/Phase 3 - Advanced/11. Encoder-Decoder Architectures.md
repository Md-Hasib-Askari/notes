# Encoder-Decoder Architectures

Encoder-decoder architectures form the backbone of many modern NLP systems, particularly for tasks where the input and output are sequences of different lengths. These architectures have been especially successful in machine translation, summarization, and other text generation tasks.

## Core Concept

The encoder-decoder framework consists of two main components:
1. **Encoder**: Processes the input sequence and compresses it into a context representation
2. **Decoder**: Takes the context representation and generates the output sequence

This design separates the understanding of input from the generation of output, allowing the model to handle variable-length sequences effectively.

## Evolution of Encoder-Decoder Architectures

### RNN-based Sequence-to-Sequence

The original encoder-decoder models used RNNs (often LSTMs or GRUs):

```python
import torch
import torch.nn as nn

class RNNEncoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        # src: [batch_size, src_len]
        embedded = self.dropout(self.embedding(src))
        # embedded: [batch_size, src_len, emb_dim]
        
        outputs, hidden = self.rnn(embedded)
        # outputs: [batch_size, src_len, hidden_dim]
        # hidden: [n_layers, batch_size, hidden_dim]
        
        return outputs, hidden

class RNNDecoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden):
        # input: [batch_size]
        # hidden: [n_layers, batch_size, hidden_dim]
        
        input = input.unsqueeze(1)  # [batch_size, 1]
        embedded = self.dropout(self.embedding(input))
        # embedded: [batch_size, 1, emb_dim]
        
        output, hidden = self.rnn(embedded, hidden)
        # output: [batch_size, 1, hidden_dim]
        # hidden: [n_layers, batch_size, hidden_dim]
        
        prediction = self.fc_out(output.squeeze(1))
        # prediction: [batch_size, output_dim]
        
        return prediction, hidden
```

### Attention-Enhanced Seq2Seq

Adding attention mechanisms greatly improved performance by allowing the decoder to focus on relevant parts of the input:

```python
class AttentionDecoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(emb_dim + hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout)
        
        # Attention layers
        self.attention = nn.Linear(hidden_dim * 2, 1)
        
    def forward(self, input, hidden, encoder_outputs):
        # input: [batch_size]
        # hidden: [n_layers, batch_size, hidden_dim]
        # encoder_outputs: [batch_size, src_len, hidden_dim]
        
        input = input.unsqueeze(1)  # [batch_size, 1]
        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, emb_dim]
        
        # Calculate attention weights
        batch_size = encoder_outputs.shape[0]
        src_len = encoder_outputs.shape[1]
        
        # Repeat decoder hidden state src_len times
        hidden_expanded = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)
        # hidden_expanded: [batch_size, src_len, hidden_dim]
        
        # Concatenate encoder outputs and decoder hidden state
        attention_input = torch.cat((encoder_outputs, hidden_expanded), dim=2)
        # attention_input: [batch_size, src_len, hidden_dim * 2]
        
        # Calculate attention scores
        attention_scores = self.attention(attention_input).squeeze(2)
        # attention_scores: [batch_size, src_len]
        
        # Apply softmax to get attention weights
        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(1)
        # attention_weights: [batch_size, 1, src_len]
        
        # Calculate context vector
        context = torch.bmm(attention_weights, encoder_outputs)
        # context: [batch_size, 1, hidden_dim]
        
        # Concatenate embedding and context vector
        rnn_input = torch.cat((embedded, context), dim=2)
        # rnn_input: [batch_size, 1, emb_dim + hidden_dim]
        
        # Pass through RNN
        output, hidden = self.rnn(rnn_input, hidden)
        # output: [batch_size, 1, hidden_dim]
        # hidden: [n_layers, batch_size, hidden_dim]
        
        # Prepare for final linear layer
        output = output.squeeze(1)  # [batch_size, hidden_dim]
        context = context.squeeze(1)  # [batch_size, hidden_dim]
        embedded = embedded.squeeze(1)  # [batch_size, emb_dim]
        
        # Concatenate all vectors for prediction
        prediction_input = torch.cat((output, context, embedded), dim=1)
        # prediction_input: [batch_size, hidden_dim * 2 + emb_dim]
        
        # Make prediction
        prediction = self.fc_out(prediction_input)
        # prediction: [batch_size, output_dim]
        
        return prediction, hidden, attention_weights
```

### Transformer-based Encoder-Decoder

Modern encoder-decoder architectures primarily use Transformer layers, which provide parallel processing and better capture long-range dependencies:

```python
import torch.nn as nn
from transformers import BertModel, BertConfig, BertTokenizer

class TransformerEncoderDecoder(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, 
                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):
        super().__init__()
        
        # Embedding layers
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, dropout)
        
        # Transformer layers
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout
        )
        
        # Output layer
        self.output_layer = nn.Linear(d_model, tgt_vocab_size)
        
    def forward(self, src, tgt, src_mask=None, tgt_mask=None,
                src_padding_mask=None, tgt_padding_mask=None,
                memory_key_padding_mask=None):
        # src: [batch_size, src_len]
        # tgt: [batch_size, tgt_len]
        
        # Create masks if not provided
        if src_mask is None or tgt_mask is None:
            src_mask, tgt_mask = self.create_masks(src, tgt)
        
        # Embedding and positional encoding
        src_emb = self.positional_encoding(self.src_embedding(src))
        tgt_emb = self.positional_encoding(self.tgt_embedding(tgt))
        
        # Permute dimensions for transformer (seq_len, batch, features)
        src_emb = src_emb.permute(1, 0, 2)
        tgt_emb = tgt_emb.permute(1, 0, 2)
        
        # Transformer forward pass
        output = self.transformer(
            src=src_emb,
            tgt=tgt_emb,
            src_mask=src_mask,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_padding_mask,
            tgt_key_padding_mask=tgt_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask
        )
        
        # Permute back to batch-first
        output = output.permute(1, 0, 2)
        
        # Project to vocabulary
        return self.output_layer(output)
    
    def create_masks(self, src, tgt):
        # Create masks for padding and future tokens
        src_seq_len = src.shape[1]
        tgt_seq_len = tgt.shape[1]
        
        # Source mask (all ones, no masking needed for encoder self-attention)
        src_mask = torch.ones((src_seq_len, src_seq_len)).to(src.device)
        
        # Target mask (lower triangular to prevent attending to future tokens)
        tgt_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len))).to(tgt.device)
        
        return src_mask, tgt_mask

# Positional encoding class
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)
```

## Pre-trained Encoder-Decoder Models

Modern NLP often uses pre-trained encoder-decoder models from libraries like Hugging Face:

```python
from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Load pre-trained model and tokenizer
model_name = "facebook/bart-large-cnn"  # BART model fine-tuned for summarization
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Example function for summarization using BART
def summarize_text(text, max_length=150, min_length=40):
    # Tokenize input
    inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)
    
    # Generate summary
    summary_ids = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        min_length=min_length,
        num_beams=4,
        length_penalty=2.0,
        early_stopping=True
    )
    
    # Decode and return summary
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Example text
article = """
Artificial intelligence (AI) has transformed numerous industries, from healthcare to finance. 
Recent advances in large language models have led to systems that can generate human-like text, 
translate between languages, and even write code. However, these systems also raise important 
ethical considerations regarding bias, privacy, and the potential for misuse. Researchers are 
working to address these challenges while continuing to push the boundaries of what AI can accomplish.
"""

summary = summarize_text(article)
print(f"Summary: {summary}")
```

## Popular Encoder-Decoder Models

1. **BART**: Bidirectional Auto-Regressive Transformer that combines BERT-style bidirectional encoder with GPT-style autoregressive decoder
2. **T5**: Text-to-Text Transfer Transformer that frames all NLP tasks as text-to-text problems
3. **mBART/mT5**: Multilingual versions of BART and T5
4. **PEGASUS**: Pre-trained model specifically optimized for summarization
5. **MarianMT**: Specialized for machine translation between many languages

## Applications

1. **Machine Translation**: Converting text between languages
2. **Summarization**: Condensing documents into shorter versions
3. **Question Answering**: Generating answers from context passages
4. **Data-to-Text Generation**: Converting structured data to natural language
5. **Dialogue Systems**: Generating contextually appropriate responses
6. **Code Generation**: Translating natural language to programming code

## Advantages of Encoder-Decoder Architecture

1. **Flexibility**: Handles variable-length input and output sequences
2. **Separation of concerns**: Encoder focuses on understanding, decoder on generation
3. **Transfer learning**: Pre-trained models can be fine-tuned for specific tasks
4. **Modularity**: Encoder and decoder can be optimized separately
5. **Cross-attention**: Decoder can focus on relevant parts of the input sequence

## Best Practices

1. **Use pre-trained models** when possible rather than training from scratch
2. **Task-specific fine-tuning** to adapt general models to specific domains
3. **Beam search decoding** for higher quality generation
4. **Length penalties** to control output verbosity
5. **Careful prompt engineering** to guide generation effectively

Encoder-decoder architectures remain central to modern NLP systems, providing a flexible framework for a wide range of sequence-to-sequence tasks and serving as the foundation for many state-of-the-art language models.
