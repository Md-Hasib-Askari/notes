# Question Answering Systems

Question Answering (QA) systems automatically answer questions posed in natural language. Modern QA systems leverage deep learning to understand questions and extract or generate answers from various sources of information.

## Types of QA Systems

1. **Extractive QA**: Extracts answer spans from a given context
2. **Abstractive QA**: Generates answers that might not appear verbatim in the context
3. **Knowledge-based QA**: Retrieves answers from structured knowledge bases
4. **Open-domain QA**: Answers questions without specific context, searching across large corpora

## Extractive QA with Transformers

The most common approach uses transformer models to identify answer spans in context:

```python
import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

def answer_question(question, context):
    """
    Extract an answer to a question from the provided context
    
    Args:
        question: The question to answer
        context: The text passage containing the answer
        
    Returns:
        The extracted answer text
    """
    # Load pre-trained model and tokenizer
    model_name = "deepset/roberta-base-squad2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    
    # Tokenize input
    inputs = tokenizer(
        question,
        context,
        add_special_tokens=True,
        return_tensors="pt"
    )
    
    # Get model prediction
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Get start and end positions
    answer_start = torch.argmax(outputs.start_logits)
    answer_end = torch.argmax(outputs.end_logits)
    
    # Convert token positions to character positions
    input_ids = inputs["input_ids"].tolist()[0]
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    # Get answer tokens
    answer = tokens[answer_start]
    
    # Join tokens if answer is multiple tokens
    for i in range(answer_start + 1, answer_end + 1):
        if tokens[i].startswith("##"):
            answer += tokens[i][2:]
        else:
            answer += " " + tokens[i]
    
    # Clean up answer
    answer = answer.replace("[CLS]", "").replace("[SEP]", "").strip()
    if answer.startswith("##"):
        answer = answer[2:]
    
    # Map to original text (improved accuracy)
    answer_mapping = tokenizer.decode(input_ids[answer_start:answer_end+1])
    
    return answer_mapping

# Example usage
context = """
The Transformer architecture was introduced in the paper "Attention Is All You Need" by 
Vaswani et al. in 2017. It revolutionized natural language processing by eliminating 
recurrence and convolutions entirely, relying instead on attention mechanisms. The 
Transformer has become the foundation for models like BERT and GPT, which have achieved 
state-of-the-art results on various NLP tasks.
"""

question = "When was the Transformer architecture introduced?"
answer = answer_question(question, context)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

## Fine-tuning a QA Model

To improve performance on domain-specific questions, fine-tune a pre-trained model:

```python
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset
import torch

def fine_tune_qa_model(train_file, validation_file, output_dir):
    """
    Fine-tune a QA model on custom data
    
    Args:
        train_file: Path to training data in SQuAD format
        validation_file: Path to validation data in SQuAD format
        output_dir: Directory to save the fine-tuned model
    """
    # Load dataset
    datasets = load_dataset('json', data_files={
        'train': train_file,
        'validation': validation_file
    })
    
    # Load base model and tokenizer
    model_name = "distilbert-base-uncased"
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Preprocessing function
    def preprocess_function(examples):
        questions = [q.strip() for q in examples["question"]]
        contexts = [c.strip() for c in examples["context"]]
        
        inputs = tokenizer(
            questions,
            contexts,
            max_length=384,
            truncation="only_second",
            stride=128,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length"
        )
        
        # Map original indices to features
        sample_map = inputs.pop("overflow_to_sample_mapping")
        offset_mapping = inputs.pop("offset_mapping")
        
        # Initialize answer positions
        start_positions = []
        end_positions = []
        
        for i, offset in enumerate(offset_mapping):
            sample_idx = sample_map[i]
            answer = examples["answers"][sample_idx]
            start_char = answer["answer_start"][0]
            end_char = start_char + len(answer["text"][0])
            
            # Find token indices that contain the answer
            sequence_ids = inputs.sequence_ids(i)
            context_start = 0
            while sequence_ids[context_start] != 1:
                context_start += 1
            context_end = len(sequence_ids) - 1
            while sequence_ids[context_end] != 1:
                context_end -= 1
                
            # If answer not in context due to truncation, use cls token position
            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
                start_positions.append(0)
                end_positions.append(0)
            else:
                # Find token indices for answer boundaries
                idx = context_start
                while idx <= context_end and offset[idx][0] <= start_char:
                    idx += 1
                start_positions.append(idx - 1)
                
                idx = context_end
                while idx >= context_start and offset[idx][1] >= end_char:
                    idx -= 1
                end_positions.append(idx + 1)
        
        inputs["start_positions"] = start_positions
        inputs["end_positions"] = end_positions
        return inputs
    
    # Apply preprocessing
    processed_datasets = datasets.map(
        preprocess_function,
        batched=True,
        remove_columns=datasets["train"].column_names
    )
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        learning_rate=3e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=3,
        weight_decay=0.01,
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=processed_datasets["train"],
        eval_dataset=processed_datasets["validation"],
        tokenizer=tokenizer,
    )
    
    # Train model
    trainer.train()
    
    # Save model
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    return model, tokenizer
```

## Open-Domain QA with Retrieval

For questions without provided context, implement a retrieval-based approach:

```python
from transformers import DPRQuestionEncoder, DPRContextEncoder, AutoTokenizer
from transformers import AutoModelForQuestionAnswering
import torch
import faiss
import numpy as np

class OpenDomainQA:
    def __init__(self, documents, chunk_size=200, overlap=50):
        """
        Initialize open-domain QA system
        
        Args:
            documents: List of text documents to search
            chunk_size: Size of document chunks
            overlap: Overlap between chunks
        """
        # Load DPR models
        self.question_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
        self.context_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
        self.tokenizer = AutoTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
        
        # Load QA model
        self.qa_tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
        self.qa_model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")
        
        # Create document chunks
        self.chunks = []
        for doc in documents:
            for i in range(0, len(doc), chunk_size - overlap):
                self.chunks.append(doc[i:i + chunk_size])
        
        # Encode chunks
        self.chunk_embeddings = self._encode_chunks()
        
        # Create FAISS index for fast similarity search
        self.index = faiss.IndexFlatIP(self.chunk_embeddings.shape[1])
        self.index.add(self.chunk_embeddings)
    
    def _encode_chunks(self):
        """Encode document chunks using DPR context encoder"""
        embeddings = []
        for i in range(0, len(self.chunks), 16):  # Process in batches
            batch = self.chunks[i:i+16]
            inputs = self.tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512)
            with torch.no_grad():
                outputs = self.context_encoder(**inputs)
            embeddings.append(outputs.pooler_output.numpy())
        
        return np.vstack(embeddings)
    
    def answer(self, question, k=3):
        """Answer a question using retrieval + reader approach"""
        # Encode question
        question_inputs = self.tokenizer(question, return_tensors="pt")
        with torch.no_grad():
            question_embedding = self.question_encoder(**question_inputs).pooler_output.numpy()
        
        # Retrieve relevant chunks
        scores, indices = self.index.search(question_embedding, k)
        contexts = [self.chunks[idx] for idx in indices[0]]
        
        # Get answers from each context
        answers = []
        for context in contexts:
            inputs = self.qa_tokenizer(question, context, return_tensors="pt")
            with torch.no_grad():
                outputs = self.qa_model(**inputs)
            
            answer_start = torch.argmax(outputs.start_logits)
            answer_end = torch.argmax(outputs.end_logits)
            
            input_ids = inputs.input_ids.tolist()[0]
            answer = self.qa_tokenizer.decode(input_ids[answer_start:answer_end+1])
            confidence = float(torch.max(outputs.start_logits) + torch.max(outputs.end_logits))
            
            answers.append((answer, confidence, context))
        
        # Sort by confidence
        answers.sort(key=lambda x: x[1], reverse=True)
        
        return {
            "answer": answers[0][0],
            "confidence": answers[0][1],
            "context": answers[0][2],
            "alternatives": [(a[0], a[1]) for a in answers[1:]]
        }
```

## Applications

1. **Search engine enhancements**: Direct answers to user queries
2. **Virtual assistants**: Answering factual questions
3. **Customer support**: Automated responses to common questions
4. **Educational tools**: Providing answers to student questions
5. **Information retrieval systems**: Extracting specific information from documents

## Evaluation Metrics

QA systems are typically evaluated using:
- **Exact Match (EM)**: Percentage of predictions matching ground truth exactly
- **F1 Score**: Overlap between predicted and ground truth answers
- **Mean Reciprocal Rank (MRR)**: For ranking-based evaluation
- **Human evaluation**: Assessing answer relevance and correctness

Modern QA systems combine powerful language models with retrieval mechanisms to provide accurate answers across a wide range of domains and question types.
