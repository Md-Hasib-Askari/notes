# Attention Mechanisms

Attention mechanisms revolutionized sequence modeling by allowing models to focus on different parts of the input sequence when generating each element of the output. This breakthrough addressed the information bottleneck in traditional sequence-to-sequence models and set the stage for modern NLP architectures like Transformers.

## Core Concept

Attention computes a weighted sum of all encoder hidden states for each decoding step, rather than relying solely on the final encoder state. These weights determine how much "attention" to pay to each input position when generating each output token.

## Types of Attention

### 1. Bahdanau (Additive) Attention

The original attention mechanism proposed by Bahdanau et al. (2014) calculates alignment scores using a small feed-forward network:

```
score(h_t, h̄_s) = v_a^T tanh(W_a[h_t; h̄_s])
```

Where:
- h_t is the decoder's current hidden state
- h̄_s is an encoder hidden state
- W_a and v_a are learnable parameters

### 2. Luong (Multiplicative) Attention

A simplified attention mechanism that uses dot products for scoring:

```
score(h_t, h̄_s) = h_t^T W_a h̄_s
```

With variants:
- Dot: score(h_t, h̄_s) = h_t^T h̄_s
- General: score(h_t, h̄_s) = h_t^T W_a h̄_s
- Concat: score(h_t, h̄_s) = v_a^T tanh(W_a[h_t; h̄_s])

### 3. Self-Attention

Allows a sequence to attend to itself, capturing relationships between different positions in the same sequence:

```
Attention(Q, K, V) = softmax(QK^T/√d_k)V
```

Where Q, K, and V are query, key, and value matrices derived from the input sequence.

## Attention Process

1. **Compute alignment scores**: Measure similarity between decoder state and each encoder state
2. **Apply softmax**: Convert scores to probabilities (attention weights)
3. **Create context vector**: Weighted sum of encoder states using attention weights
4. **Combine with decoder state**: Use context vector with current decoder state for prediction

## Implementation with PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BahdanauAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.W1 = nn.Linear(hidden_dim, hidden_dim)
        self.W2 = nn.Linear(hidden_dim, hidden_dim)
        self.V = nn.Linear(hidden_dim, 1)
        
    def forward(self, query, keys):
        # query: [batch_size, hidden_dim]
        # keys: [batch_size, src_len, hidden_dim]
        
        # Expand query to match keys dimension
        query = query.unsqueeze(1).repeat(1, keys.shape[1], 1)
        # query: [batch_size, src_len, hidden_dim]
        
        # Calculate energy
        energy = torch.tanh(self.W1(query) + self.W2(keys))
        # energy: [batch_size, src_len, hidden_dim]
        
        # Calculate attention weights
        attention = self.V(energy).squeeze(2)
        # attention: [batch_size, src_len]
        
        # Apply softmax to get probabilities
        attention_weights = F.softmax(attention, dim=1)
        # attention_weights: [batch_size, src_len]
        
        # Create context vector using attention weights
        context = torch.bmm(attention_weights.unsqueeze(1), keys)
        # context: [batch_size, 1, hidden_dim]
        
        return context.squeeze(1), attention_weights

class LuongAttention(nn.Module):
    def __init__(self, hidden_dim, method='general'):
        super().__init__()
        self.method = method
        
        if method == 'general':
            self.W = nn.Linear(hidden_dim, hidden_dim)
        elif method == 'concat':
            self.W = nn.Linear(hidden_dim * 2, hidden_dim)
            self.V = nn.Linear(hidden_dim, 1)
            
    def forward(self, query, keys):
        # query: [batch_size, hidden_dim]
        # keys: [batch_size, src_len, hidden_dim]
        
        if self.method == 'dot':
            # Simple dot product attention
            attention = torch.bmm(query.unsqueeze(1), keys.transpose(1, 2)).squeeze(1)
            
        elif self.method == 'general':
            # Linear transformation then dot product
            query_transformed = self.W(query)
            attention = torch.bmm(query_transformed.unsqueeze(1), keys.transpose(1, 2)).squeeze(1)
            
        elif self.method == 'concat':
            # Concatenation and feed-forward network
            query_expanded = query.unsqueeze(1).repeat(1, keys.shape[1], 1)
            energy = torch.tanh(self.W(torch.cat([query_expanded, keys], dim=2)))
            attention = self.V(energy).squeeze(2)
            
        # Apply softmax to get probabilities
        attention_weights = F.softmax(attention, dim=1)
        
        # Create context vector using attention weights
        context = torch.bmm(attention_weights.unsqueeze(1), keys).squeeze(1)
        
        return context, attention_weights

class SelfAttention(nn.Module):
    def __init__(self, hidden_dim, heads=8):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.heads = heads
        self.head_dim = hidden_dim // heads
        
        assert self.head_dim * heads == hidden_dim, "Hidden dimension must be divisible by number of heads"
        
        self.q_linear = nn.Linear(hidden_dim, hidden_dim)
        self.k_linear = nn.Linear(hidden_dim, hidden_dim)
        self.v_linear = nn.Linear(hidden_dim, hidden_dim)
        
        self.fc = nn.Linear(hidden_dim, hidden_dim)
        
    def forward(self, x, mask=None):
        # x: [batch_size, seq_len, hidden_dim]
        batch_size = x.shape[0]
        seq_len = x.shape[1]
        
        # Transform input into query, key, value projections
        Q = self.q_linear(x)  # [batch_size, seq_len, hidden_dim]
        K = self.k_linear(x)  # [batch_size, seq_len, hidden_dim]
        V = self.v_linear(x)  # [batch_size, seq_len, hidden_dim]
        
        # Reshape for multi-head attention
        Q = Q.view(batch_size, seq_len, self.heads, self.head_dim).permute(0, 2, 1, 3)
        K = K.view(batch_size, seq_len, self.heads, self.head_dim).permute(0, 2, 1, 3)
        V = V.view(batch_size, seq_len, self.heads, self.head_dim).permute(0, 2, 1, 3)
        # Q, K, V: [batch_size, heads, seq_len, head_dim]
        
        # Calculate attention scores
        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)
        # energy: [batch_size, heads, seq_len, seq_len]
        
        # Apply mask if provided (for padding tokens)
        if mask is not None:
            energy = energy.masked_fill(mask == 0, -1e10)
        
        # Apply softmax to get attention weights
        attention = F.softmax(energy, dim=-1)
        # attention: [batch_size, heads, seq_len, seq_len]
        
        # Apply attention weights to values
        output = torch.matmul(attention, V)
        # output: [batch_size, heads, seq_len, head_dim]
        
        # Reshape back to original dimensions
        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_len, self.hidden_dim)
        # output: [batch_size, seq_len, hidden_dim]
        
        # Final linear layer
        output = self.fc(output)
        
        return output, attention
```

## Attention Visualization

One of the benefits of attention is interpretability. We can visualize which input tokens the model focuses on when generating each output token:

```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def visualize_attention(source_tokens, target_tokens, attention_weights):
    """
    Visualize attention weights between source and target sequences
    
    Parameters:
    - source_tokens: List of source tokens
    - target_tokens: List of target tokens
    - attention_weights: 2D numpy array of shape [target_len, source_len]
    """
    plt.figure(figsize=(10, 8))
    sns.heatmap(attention_weights, xticklabels=source_tokens, yticklabels=target_tokens, cmap='viridis')
    plt.xlabel('Source')
    plt.ylabel('Target')
    plt.title('Attention Weights')
    plt.tight_layout()
    plt.show()

# Example usage
source = ["I", "love", "natural", "language", "processing", "."]
target = ["Ich", "liebe", "die", "Verarbeitung", "natürlicher", "Sprache", "."]
attention = np.random.rand(len(target), len(source))  # In practice, this would be model outputs
attention = attention / attention.sum(axis=1, keepdims=True)  # Normalize

visualize_attention(source, target, attention)
```

## Applications

1. **Machine Translation**: Focus on relevant words when translating
2. **Text Summarization**: Attend to key sentences when generating summaries
3. **Image Captioning**: Focus on different image regions when generating captions
4. **Speech Recognition**: Align audio segments with text output
5. **Question Answering**: Attend to relevant passage parts when answering

## Advantages of Attention

1. **Addresses bottleneck problem**: No need to compress all information into a single vector
2. **Improved handling of long sequences**: Can maintain information from distant positions
3. **Parallelization**: Enables more efficient training (especially self-attention)
4. **Interpretability**: Attention weights provide insight into model decisions
5. **Better gradient flow**: Helps mitigate vanishing gradient problems

## Evolution to Transformers

Attention mechanisms evolved into the Transformer architecture, which relies entirely on self-attention without recurrence or convolution. This paradigm shift led to models like BERT, GPT, and T5, which dominate modern NLP.

Attention mechanisms represent one of the most important advances in NLP, enabling models to handle longer sequences and capture complex dependencies that were previously impossible to model effectively.
