# T5 and Text-to-Text Frameworks

Text-to-Text Transfer Transformer (T5) represents a unified approach to NLP by framing all text-based language problems as text-to-text tasks. This paradigm shift simplifies the model architecture while achieving state-of-the-art results across various NLP tasks.

## Core Concept

The key insight of T5 is converting every NLP task into a text-to-text format:
- **Classification**: "classify: I loved this movie" → "positive"
- **Translation**: "translate English to French: Hello" → "Bonjour"
- **Summarization**: "summarize: [long text]" → "[summary]"
- **Question answering**: "question: [question] context: [passage]" → "[answer]"

This unified approach allows a single model to handle multiple tasks without task-specific architectures.

## Architecture

T5 uses a standard encoder-decoder Transformer architecture:
- **Encoder**: Processes the input text
- **Decoder**: Generates the output text
- **Shared vocabulary**: Same tokenization for input and output

## T5 Variants

- **T5-Small**: 60 million parameters
- **T5-Base**: 220 million parameters
- **T5-Large**: 770 million parameters
- **T5-3B**: 3 billion parameters
- **T5-11B**: 11 billion parameters

## Implementation with Hugging Face

```python
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load pre-trained model and tokenizer
model_name = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Example function to use T5 for various NLP tasks
def t5_process(task, input_text, max_length=100):
    """
    Process text using T5 for different NLP tasks
    
    Args:
        task: The NLP task to perform (e.g., 'summarize', 'translate', 'question')
        input_text: The input text to process
        max_length: Maximum length of generated output
        
    Returns:
        The model's output text
    """
    # Format input with task prefix
    if task == "summarize":
        input_text = f"summarize: {input_text}"
    elif task == "translate":
        input_text = f"translate English to German: {input_text}"
    elif task == "question":
        # For question answering, input should be formatted as: 
        # "question: [question] context: [context]"
        pass
    elif task == "classify":
        input_text = f"classify: {input_text}"
    
    # Tokenize input
    inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    
    # Generate output
    outputs = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)
    
    # Decode and return
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example usage for different tasks
text_to_summarize = """
The Transformer architecture has become the dominant approach in natural language processing. 
It relies entirely on attention mechanisms and eliminates recurrence and convolutions. 
This design allows for much more parallelization during training and has led to the development 
of pre-trained models that can be fine-tuned for specific tasks with remarkable results.
"""

# Summarization
summary = t5_process("summarize", text_to_summarize)
print(f"Summary: {summary}")

# Translation
translation = t5_process("translate", "The weather is nice today.")
print(f"Translation: {translation}")

# Classification
classification = t5_process("classify", "I absolutely loved this movie, it was fantastic!")
print(f"Classification: {classification}")
```

## Fine-tuning T5 for Custom Tasks

You can fine-tune T5 on specific datasets to improve performance:

```python
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from torch.utils.data import Dataset
import torch

# Custom dataset class
class TextToTextDataset(Dataset):
    def __init__(self, inputs, targets, tokenizer, max_input_length=512, max_target_length=64):
        self.inputs = inputs
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_target_length = max_target_length
    
    def __len__(self):
        return len(self.inputs)
    
    def __getitem__(self, idx):
        input_text = self.inputs[idx]
        target_text = self.targets[idx]
        
        # Tokenize inputs and targets
        input_encoding = self.tokenizer(
            input_text, 
            max_length=self.max_input_length, 
            padding="max_length", 
            truncation=True, 
            return_tensors="pt"
        )
        
        target_encoding = self.tokenizer(
            target_text, 
            max_length=self.max_target_length, 
            padding="max_length", 
            truncation=True, 
            return_tensors="pt"
        )
        
        # Replace padding token id with -100 for targets
        # This ensures that loss is not computed on padding tokens
        target_ids = target_encoding["input_ids"].clone()
        target_ids[target_ids == self.tokenizer.pad_token_id] = -100
        
        return {
            "input_ids": input_encoding["input_ids"].squeeze(),
            "attention_mask": input_encoding["attention_mask"].squeeze(),
            "labels": target_ids.squeeze()
        }

# Function to fine-tune T5
def fine_tune_t5(train_inputs, train_targets, val_inputs, val_targets, task_prefix, output_dir):
    """
    Fine-tune T5 for a custom task
    
    Args:
        train_inputs: List of input texts for training
        train_targets: List of target texts for training
        val_inputs: List of input texts for validation
        val_targets: List of target texts for validation
        task_prefix: Task prefix to add to inputs (e.g., 'summarize:', 'translate:')
        output_dir: Directory to save the fine-tuned model
    
    Returns:
        The fine-tuned model and tokenizer
    """
    # Load pre-trained model and tokenizer
    tokenizer = T5Tokenizer.from_pretrained("t5-base")
    model = T5ForConditionalGeneration.from_pretrained("t5-base")
    
    # Add task prefix to inputs
    train_inputs = [f"{task_prefix} {text}" for text in train_inputs]
    val_inputs = [f"{task_prefix} {text}" for text in val_inputs]
    
    # Create datasets
    train_dataset = TextToTextDataset(train_inputs, train_targets, tokenizer)
    val_dataset = TextToTextDataset(val_inputs, val_targets, tokenizer)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        eval_steps=100,
        save_steps=100,
        warmup_steps=100,
        prediction_loss_only=True,
        evaluation_strategy="steps",
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )
    
    # Train the model
    trainer.train()
    
    # Save the model and tokenizer
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    return model, tokenizer

# Example usage for sentiment classification
train_inputs = [
    "This movie was fantastic!",
    "I hated every minute of this show.",
    # ... more examples
]
train_targets = [
    "positive",
    "negative",
    # ... more examples
]

val_inputs = [
    "The performance was outstanding.",
    "The service was terrible.",
    # ... more examples
]
val_targets = [
    "positive",
    "negative",
    # ... more examples
]

model, tokenizer = fine_tune_t5(
    train_inputs, 
    train_targets, 
    val_inputs, 
    val_targets, 
    "classify:",
    "t5-sentiment-classifier"
)
```

## Multi-Task Learning

One of T5's strengths is multi-task learning. By mixing different tasks during fine-tuning, the model can improve on all tasks simultaneously:

```python
# Prepare multi-task data
classification_inputs = ["classify: " + text for text in classification_texts]
classification_targets = classification_labels

summarization_inputs = ["summarize: " + text for text in long_texts]
summarization_targets = summaries

qa_inputs = ["question: " + q + " context: " + c for q, c in zip(questions, contexts)]
qa_targets = answers

# Combine data from all tasks
all_inputs = classification_inputs + summarization_inputs + qa_inputs
all_targets = classification_targets + summarization_targets + qa_targets

# Use the same fine-tuning function as before
model, tokenizer = fine_tune_t5(
    all_inputs[:800],  # Training split
    all_targets[:800],
    all_inputs[800:],  # Validation split
    all_targets[800:],
    "",  # No additional prefix needed since it's in the data
    "t5-multitask"
)
```

## Applications

1. **Text summarization**: Condensing documents
2. **Machine translation**: Converting between languages
3. **Question answering**: Extracting answers from context
4. **Text classification**: Sentiment analysis, topic labeling
5. **Paraphrasing**: Rewriting text while preserving meaning
6. **Data-to-text generation**: Structured data to natural language

## Advantages of T5

1. **Unified framework**: Single model for multiple NLP tasks
2. **Simplified fine-tuning**: Consistent approach across different applications
3. **Transfer learning**: Knowledge transfers between related tasks
4. **Interpretable format**: Task specification is human-readable
5. **Strong performance**: State-of-the-art results on many benchmarks

## Limitations

1. **Input length constraints**: Like other Transformers, has maximum context length
2. **Computational requirements**: Larger versions require significant resources
3. **Task prefix sensitivity**: Performance can depend on exact prompt formulation
4. **Generation artifacts**: May produce repetitive or inconsistent text

The text-to-text framework pioneered by T5 represents an elegant unification of NLP tasks that has influenced the design of many subsequent models, demonstrating the power of consistent interfaces for language tasks.
