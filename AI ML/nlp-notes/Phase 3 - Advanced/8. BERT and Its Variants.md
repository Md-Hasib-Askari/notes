# BERT and Its Variants

BERT (Bidirectional Encoder Representations from Transformers), introduced by Google in 2018, revolutionized NLP by enabling deep bidirectional context understanding. BERT and its variants have achieved state-of-the-art results on numerous NLP tasks.

## Core Concept

BERT is a pre-trained Transformer encoder that learns bidirectional representations by jointly conditioning on both left and right context. Unlike previous models that processed text from left to right or right to left, BERT considers the entire context simultaneously.

## Pre-training Objectives

BERT uses two pre-training tasks:

1. **Masked Language Modeling (MLM)**: Randomly mask 15% of tokens and train the model to predict them based on context.
2. **Next Sentence Prediction (NSP)**: Train the model to predict whether two sentences follow each other in the original text.

## Architecture Variants

- **BERT-base**: 12 layers, 768 hidden units, 12 attention heads (110M parameters)
- **BERT-large**: 24 layers, 1024 hidden units, 16 attention heads (340M parameters)

## Input Representation

BERT's input combines three embeddings:
- **Token embeddings**: WordPiece tokens
- **Segment embeddings**: Distinguish between sentence pairs (A/B)
- **Position embeddings**: Capture token position in sequence

## Using BERT with Hugging Face

```python
import torch
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
import torch.nn.functional as F

# Load pre-trained model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Example: Getting embeddings from BERT
text = "BERT is a powerful language model."
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

with torch.no_grad():
    outputs = model(**inputs)

# Last hidden states (contextual embeddings)
last_hidden_states = outputs.last_hidden_state
print(f"Contextual embeddings shape: {last_hidden_states.shape}")

# [CLS] token embedding (often used for classification)
cls_embedding = last_hidden_states[:, 0, :]
print(f"CLS embedding shape: {cls_embedding.shape}")

# Example: Fine-tuning BERT for sentiment classification
num_labels = 2  # binary classification (positive/negative)
classifier_model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', 
    num_labels=num_labels
)

# Prepare inputs with labels
inputs = tokenizer(
    ["I love this movie!", "This film is terrible."],
    return_tensors="pt", 
    padding=True, 
    truncation=True
)
labels = torch.tensor([1, 0])  # 1 for positive, 0 for negative

# Forward pass with labels for training
outputs = classifier_model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits

# Get predictions
predictions = F.softmax(logits, dim=1)
print(f"Sentiment predictions: {predictions}")
```

## Fine-tuning Methodology

BERT excels at transfer learning - pre-train once, fine-tune for specific tasks:

```python
from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset

class SimpleDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)
        self.labels = labels
    
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    
    def __len__(self):
        return len(self.labels)

def fine_tune_bert(train_texts, train_labels, val_texts, val_labels, epochs=3):
    # Load model and tokenizer
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    # Create datasets and dataloaders
    train_dataset = SimpleDataset(train_texts, train_labels, tokenizer)
    val_dataset = SimpleDataset(val_texts, val_labels, tokenizer)
    
    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    # Prepare optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
    total_steps = len(train_dataloader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=0, 
        num_training_steps=total_steps
    )
    
    # Training loop
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        for batch in train_dataloader:
            optimizer.zero_grad()
            
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            
            loss = outputs.loss
            total_loss += loss.item()
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            scheduler.step()
        
        avg_train_loss = total_loss / len(train_dataloader)
        print(f"Epoch {epoch+1}, Average training loss: {avg_train_loss:.4f}")
        
        # Validation
        model.eval()
        val_accuracy = 0
        
        for batch in val_dataloader:
            with torch.no_grad():
                inputs = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**inputs)
                
                predictions = torch.argmax(outputs.logits, dim=1)
                val_accuracy += (predictions == inputs['labels']).sum().item()
        
        val_accuracy /= len(val_dataset)
        print(f"Validation accuracy: {val_accuracy:.4f}")
    
    return model, tokenizer

# Example usage
train_texts = ["I love this!", "Great movie!", "Terrible experience", "Waste of time"]
train_labels = [1, 1, 0, 0]  # 1 for positive, 0 for negative
val_texts = ["Awesome product", "Not recommended"]
val_labels = [1, 0]

model, tokenizer = fine_tune_bert(train_texts, train_labels, val_texts, val_labels)
```

## Popular BERT Variants

1. **RoBERTa** (Robustly Optimized BERT):
   - Removes NSP, uses dynamic masking, larger batches
   - More training data and compute
   - Often outperforms BERT on benchmarks

2. **DistilBERT**:
   - 40% smaller, 60% faster, retains 97% of BERT's performance
   - Uses knowledge distillation for compression
   - Ideal for resource-constrained environments

3. **ALBERT** (A Lite BERT):
   - Parameter reduction techniques with cross-layer parameter sharing
   - Factorized embedding parameterization
   - Achieves state-of-the-art results with fewer parameters

4. **SciBERT/BioBERT/ClinicalBERT**:
   - Domain-specific BERT variants trained on scientific, biomedical, or clinical text
   - Superior performance on domain-specific tasks

5. **ELECTRA**:
   - Replaced Token Detection instead of MLM
   - More efficient pre-training by learning to distinguish real tokens from replacements

## Applications

1. **Text Classification**: Sentiment analysis, topic categorization
2. **Named Entity Recognition**: Identifying entities in text
3. **Question Answering**: Extracting answers from context
4. **Natural Language Inference**: Determining entailment relationships
5. **Text Summarization**: When combined with generation components
6. **Token Classification**: POS tagging, chunking

## Best Practices

1. **Start with pre-trained models** rather than training from scratch
2. **Use specific variants** for domain-specific tasks (e.g., BioBERT for biomedical text)
3. **Fine-tune hyperparameters**:
   - Learning rate: 2e-5 to 5e-5
   - Batch size: 16 or 32
   - Epochs: 2-4 is often sufficient
4. **Gradient accumulation** for larger batch sizes on limited hardware
5. **Mixed precision training** to reduce memory requirements

BERT and its variants have transformed NLP by enabling powerful pre-trained representations that capture deep bidirectional context, setting new standards for a wide range of language understanding tasks.
