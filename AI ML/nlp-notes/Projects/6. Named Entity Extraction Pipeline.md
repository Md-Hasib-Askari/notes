# Named Entity Extraction Pipeline

This project demonstrates how to build a comprehensive named entity extraction pipeline that can identify and classify entities in text data.

## Project Overview

This named entity extraction pipeline will:
- Extract standard entities (people, organizations, locations, dates)
- Support custom entity types for domain-specific tasks
- Handle document batches efficiently
- Provide confidence scores for extracted entities
- Visualize entity relationships

## Implementation Steps

### 1. Building the Core NER Pipeline

Set up the basic NER pipeline with multiple models:

```python
import spacy
from transformers import pipeline
import en_core_web_sm

class NERPipeline:
    def __init__(self):
        # Load models
        self.spacy_model = en_core_web_sm.load()
        self.transformer_ner = pipeline("ner", model="dslim/bert-base-NER")
        
        # Define entity types
        self.entity_types = {
            # Standard types
            "PERSON": "Person",
            "ORG": "Organization",
            "GPE": "Location",
            "LOC": "Location",
            "DATE": "Date",
            "TIME": "Time",
            "MONEY": "Money",
            "PERCENT": "Percentage",
            
            # Transformer model types (CoNLL-2003)
            "B-PER": "Person",
            "I-PER": "Person",
            "B-ORG": "Organization",
            "I-ORG": "Organization",
            "B-LOC": "Location",
            "I-LOC": "Location",
            "B-MISC": "Miscellaneous",
            "I-MISC": "Miscellaneous",
        }
    
    def normalize_entities(self, entities):
        """Normalize entity types across different models"""
        normalized = []
        
        for entity in entities:
            entity_type = entity.get("type", entity.get("entity", ""))
            normalized_type = self.entity_types.get(entity_type, entity_type)
            
            normalized.append({
                "text": entity.get("text", entity.get("word", "")),
                "type": normalized_type,
                "start": entity.get("start", entity.get("start_pos", 0)),
                "end": entity.get("end", entity.get("end_pos", 0)),
                "score": entity.get("score", 1.0),
                "source": entity.get("source", "unknown")
            })
        
        return normalized
    
    def extract_entities_spacy(self, text):
        """Extract entities using SpaCy"""
        doc = self.spacy_model(text)
        entities = []
        
        for ent in doc.ents:
            entities.append({
                "text": ent.text,
                "type": ent.label_,
                "start": ent.start_char,
                "end": ent.end_char,
                "score": 1.0,  # SpaCy doesn't provide confidence scores
                "source": "spacy"
            })
        
        return entities
    
    def extract_entities_transformers(self, text):
        """Extract entities using transformers"""
        results = self.transformer_ner(text)
        entities = []
        
        # Group wordpieces into complete entities
        current_entity = None
        
        for item in results:
            entity_type = item["entity"]
            word = item["word"]
            score = item["score"]
            
            # Handle wordpiece tokens (##)
            if word.startswith("##"):
                if current_entity:
                    current_entity["text"] += word[2:]
                    current_entity["end"] = item["end"]
                continue
            
            # Check if it's a continuation (I-) or beginning (B-) of entity
            if entity_type.startswith("B-") or (current_entity and current_entity["type"] != entity_type):
                # New entity
                current_entity = {
                    "text": word,
                    "type": entity_type,
                    "start": item["start"],
                    "end": item["end"],
                    "score": score,
                    "source": "transformers"
                }
                entities.append(current_entity)
            elif entity_type.startswith("I-") and current_entity:
                # Continuation of current entity
                current_entity["text"] += " " + word
                current_entity["end"] = item["end"]
                # Update score (average)
                current_entity["score"] = (current_entity["score"] + score) / 2
        
        return entities
    
    def extract_entities(self, text, use_ensemble=True):
        """Extract entities using all available models"""
        spacy_entities = self.extract_entities_spacy(text)
        transformer_entities = self.extract_entities_transformers(text)
        
        if not use_ensemble:
            # Just combine and normalize
            all_entities = spacy_entities + transformer_entities
            return self.normalize_entities(all_entities)
        
        # Ensemble approach: merge overlapping entities, prefer higher confidence
        all_entities = spacy_entities + transformer_entities
        
        # Sort by position
        all_entities.sort(key=lambda x: (x["start"], x["end"]))
        
        # Merge overlapping entities
        merged_entities = []
        for entity in all_entities:
            # Check if overlaps with any entity in merged list
            overlap = False
            for i, merged_entity in enumerate(merged_entities):
                # Check for overlap
                if (entity["start"] <= merged_entity["end"] and 
                    entity["end"] >= merged_entity["start"]):
                    # Overlapping entities - keep one with higher confidence
                    if entity["score"] > merged_entity["score"]:
                        merged_entities[i] = entity
                    overlap = True
                    break
            
            if not overlap:
                merged_entities.append(entity)
        
        return self.normalize_entities(merged_entities)

# Usage
ner_pipeline = NERPipeline()
text = "Apple Inc. is planning to open a new store in New York City on January 15th, 2023. Tim Cook announced the plan yesterday."
entities = ner_pipeline.extract_entities(text)
for entity in entities:
    print(f"{entity['text']} - {entity['type']} ({entity['score']:.2f})")
```

### 2. Custom Entity Type Extension

Add support for domain-specific entity types:

```python
import re
from typing import List, Dict, Pattern, Any

class CustomEntityExtractor:
    def __init__(self):
        self.entity_patterns: Dict[str, List[Pattern[str]]] = {}
        self.custom_types = {}
    
    def add_entity_type(self, entity_type: str, description: str):
        """Add a new custom entity type"""
        if entity_type not in self.entity_patterns:
            self.entity_patterns[entity_type] = []
            self.custom_types[entity_type] = description
    
    def add_pattern(self, entity_type: str, pattern: str):
        """Add a regex pattern for a custom entity type"""
        if entity_type not in self.entity_patterns:
            self.add_entity_type(entity_type, entity_type)
        
        compiled_pattern = re.compile(pattern, re.IGNORECASE)
        self.entity_patterns[entity_type].append(compiled_pattern)
    
    def extract_custom_entities(self, text: str) -> List[Dict[str, Any]]:
        """Extract custom entities from text"""
        entities = []
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    entities.append({
                        "text": match.group(),
                        "type": entity_type,
                        "start": match.start(),
                        "end": match.end(),
                        "score": 1.0,  # Fixed score for regex matches
                        "source": "custom_regex"
                    })
        
        # Sort by position
        entities.sort(key=lambda x: (x["start"], x["end"]))
        
        return entities

# Extend NER pipeline with custom entity support
class ExtendedNERPipeline(NERPipeline):
    def __init__(self):
        super().__init__()
        self.custom_extractor = CustomEntityExtractor()
    
    def add_custom_entity_type(self, entity_type, description, patterns=None):
        """Add custom entity type with optional patterns"""
        self.custom_extractor.add_entity_type(entity_type, description)
        
        if patterns:
            for pattern in patterns:
                self.custom_extractor.add_pattern(entity_type, pattern)
    
    def extract_entities(self, text, use_ensemble=True, include_custom=True):
        """Extract entities using all available models including custom types"""
        # Get standard entities
        entities = super().extract_entities(text, use_ensemble)
        
        if include_custom:
            # Add custom entities
            custom_entities = self.custom_extractor.extract_custom_entities(text)
            
            # Combine and handle overlaps
            for custom_entity in custom_entities:
                # Check for overlap with existing entities
                overlap = False
                for i, entity in enumerate(entities):
                    # Check for overlap
                    if (custom_entity["start"] <= entity["end"] and 
                        custom_entity["end"] >= entity["start"]):
                        # For custom entities, always prefer them over standard types
                        entities[i] = custom_entity
                        overlap = True
                        break
                
                if not overlap:
                    entities.append(custom_entity)
            
            # Re-sort by position
            entities.sort(key=lambda x: (x["start"], x["end"]))
        
        return entities

# Usage with custom entities
extended_ner = ExtendedNERPipeline()

# Add custom entity types
extended_ner.add_custom_entity_type(
    "PRODUCT_CODE", 
    "Product identification code",
    [r'\b[A-Z]{2}-\d{4}-[A-Z0-9]{6}\b', r'\b[A-Z]{3}\d{5}\b']
)

extended_ner.add_custom_entity_type(
    "EMAIL", 
    "Email address",
    [r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b']
)

# Test with text containing custom entities
text = "Please contact support@company.com regarding product SKU12345 which was ordered on January 15th."
entities = extended_ner.extract_entities(text)
for entity in entities:
    print(f"{entity['text']} - {entity['type']} ({entity['source']})")
```

### 3. Batch Processing and Storage

Add batch processing and database storage:

```python
import pandas as pd
import sqlite3
from tqdm import tqdm
import json

class NERBatchProcessor:
    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.db_connection = None
    
    def connect_to_database(self, db_path):
        """Connect to SQLite database"""
        self.db_connection = sqlite3.connect(db_path)
        
        # Create tables if they don't exist
        cursor = self.db_connection.cursor()
        
        # Documents table
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id INTEGER PRIMARY KEY,
            document_id TEXT UNIQUE,
            text TEXT,
            metadata TEXT,
            processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')
        
        # Entities table
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS entities (
            id INTEGER PRIMARY KEY,
            document_id TEXT,
            text TEXT,
            entity_type TEXT,
            start_pos INTEGER,
            end_pos INTEGER,
            confidence REAL,
            source TEXT,
            FOREIGN KEY (document_id) REFERENCES documents (document_id)
        )
        ''')
        
        self.db_connection.commit()
    
    def process_documents(self, documents, batch_size=10, store_in_db=True):
        """Process a batch of documents"""
        results = []
        
        for i in tqdm(range(0, len(documents), batch_size)):
            batch = documents[i:i+batch_size]
            
            batch_results = []
            for doc in batch:
                doc_id = doc.get("id", f"doc_{i}")
                text = doc.get("text", "")
                metadata = doc.get("metadata", {})
                
                # Extract entities
                entities = self.pipeline.extract_entities(text)
                
                # Store in results
                batch_results.append({
                    "document_id": doc_id,
                    "text": text,
                    "metadata": metadata,
                    "entities": entities
                })
                
                # Store in database if connected
                if store_in_db and self.db_connection:
                    self._store_in_database(doc_id, text, metadata, entities)
            
            results.extend(batch_results)
        
        return results
    
    def _store_in_database(self, doc_id, text, metadata, entities):
        """Store document and entities in database"""
        cursor = self.db_connection.cursor()
        
        # Store document
        cursor.execute(
            "INSERT OR REPLACE INTO documents (document_id, text, metadata) VALUES (?, ?, ?)",
            (doc_id, text, json.dumps(metadata))
        )
        
        # Store entities
        for entity in entities:
            cursor.execute('''
            INSERT INTO entities 
            (document_id, text, entity_type, start_pos, end_pos, confidence, source)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                doc_id,
                entity["text"],
                entity["type"],
                entity["start"],
                entity["end"],
                entity["score"],
                entity["source"]
            ))
        
        self.db_connection.commit()
    
    def query_entities(self, entity_type=None, confidence_threshold=0.0, limit=100):
        """Query entities from database"""
        if not self.db_connection:
            raise ValueError("Database connection not established")
        
        query = '''
        SELECT e.document_id, e.text, e.entity_type, e.confidence, 
               d.text as document_text
        FROM entities e
        JOIN documents d ON e.document_id = d.document_id
        WHERE e.confidence >= ?
        '''
        
        params = [confidence_threshold]
        
        if entity_type:
            query += " AND e.entity_type = ?"
            params.append(entity_type)
        
        query += " ORDER BY e.confidence DESC LIMIT ?"
        params.append(limit)
        
        cursor = self.db_connection.cursor()
        cursor.execute(query, params)
        
        return cursor.fetchall()

# Usage
ner_pipeline = ExtendedNERPipeline()
batch_processor = NERBatchProcessor(ner_pipeline)
batch_processor.connect_to_database("ner_results.db")

# Sample documents
documents = [
    {
        "id": "doc1",
        "text": "Apple Inc. is planning to open a new store in New York City on January 15th, 2023. Tim Cook announced the plan yesterday.",
        "metadata": {"source": "news", "date": "2022-12-20"}
    },
    {
        "id": "doc2",
        "text": "Please contact support@company.com regarding product SKU12345 which was ordered on January 15th.",
        "metadata": {"source": "email", "date": "2022-12-21"}
    }
]

results = batch_processor.process_documents(documents)
```

### 4. Entity Visualization and API

Create an API and visualization tool:

```python
from flask import Flask, request, jsonify, render_template
import json
import spacy
from spacy import displacy
import pandas as pd

app = Flask(__name__)

# Initialize NER pipeline
ner_pipeline = ExtendedNERPipeline()
batch_processor = NERBatchProcessor(ner_pipeline)
batch_processor.connect_to_database("ner_results.db")

@app.route('/')
def index():
    return render_template('ner_tool.html')

@app.route('/extract', methods=['POST'])
def extract_entities():
    data = request.json
    text = data.get('text', '')
    
    if not text:
        return jsonify({'error': 'No text provided'}), 400
    
    # Extract entities
    entities = ner_pipeline.extract_entities(text)
    
    # Format for visualization
    doc_entities = []
    for entity in entities:
        doc_entities.append({
            "text": entity["text"],
            "start": entity["start"],
            "end": entity["end"],
            "label": entity["type"]
        })
    
    # Create visualization HTML
    visualization = create_entity_visualization(text, doc_entities)
    
    return jsonify({
        'entities': entities,
        'visualization': visualization
    })

@app.route('/batch', methods=['POST'])
def process_batch():
    data = request.json
    documents = data.get('documents', [])
    
    if not documents:
        return jsonify({'error': 'No documents provided'}), 400
    
    # Process batch
    results = batch_processor.process_documents(documents)
    
    return jsonify({
        'results': results
    })

@app.route('/stats', methods=['GET'])
def get_entity_stats():
    entity_type = request.args.get('type')
    confidence = float(request.args.get('confidence', 0.0))
    
    # Query entities
    entities = batch_processor.query_entities(entity_type, confidence)
    
    # Format results
    results = []
    for row in entities:
        doc_id, text, type, confidence, doc_text = row
        results.append({
            "document_id": doc_id,
            "entity_text": text,
            "entity_type": type,
            "confidence": confidence,
            "context": get_context(doc_text, text)
        })
    
    # Get entity type distribution
    type_counts = {}
    for result in results:
        entity_type = result["entity_type"]
        if entity_type in type_counts:
            type_counts[entity_type] += 1
        else:
            type_counts[entity_type] = 1
    
    return jsonify({
        'entities': results,
        'stats': {
            'total': len(results),
            'type_distribution': type_counts
        }
    })

def get_context(doc_text, entity_text, window=50):
    """Get context around entity"""
    start = max(0, doc_text.find(entity_text) - window)
    end = min(len(doc_text), doc_text.find(entity_text) + len(entity_text) + window)
    
    return doc_text[start:end]

def create_entity_visualization(text, entities):
    """Create HTML visualization of entities"""
    # Convert to spaCy's expected format
    doc_data = {"text": text, "ents": entities, "title": None}
    html = displacy.render([doc_data], style="ent", manual=True, page=False)
    return html

if __name__ == '__main__':
    app.run(debug=True)
```

HTML template (`templates/ner_tool.html`):

```html
<!DOCTYPE html>
<html>
<head>
    <title>Named Entity Extraction Tool</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 20px; }
        .container { max-width: 800px; margin: 0 auto; }
        .input-section { margin-bottom: 30px; }
        textarea { width: 100%; height: 150px; padding: 10px; margin-bottom: 10px; }
        button { padding: 10px 15px; background: #4285f4; color: white; border: none; }
        .results { margin-top: 20px; }
        .entity-list { margin-top: 20px; }
        table { width: 100%; border-collapse: collapse; margin-top: 10px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .visualization { margin-top: 20px; }
        
        /* Entity styles */
        .entity { padding: 2px 5px; border-radius: 3px; margin: 0 1px; }
        .Person { background: #fca; }
        .Organization { background: #afa; }
        .Location { background: #aaf; }
        .Date { background: #faa; }
        .Time { background: #aff; }
        .Money { background: #faf; }
        .Percentage { background: #ff9; }
        .PRODUCT_CODE { background: #9cf; }
        .EMAIL { background: #c9f; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Named Entity Extraction Tool</h1>
        
        <div class="input-section">
            <h2>Input Text</h2>
            <textarea id="inputText" placeholder="Enter text to analyze..."></textarea>
            <button id="analyzeBtn">Extract Entities</button>
        </div>
        
        <div class="results" id="results" style="display: none;">
            <h2>Results</h2>
            
            <div class="visualization" id="visualization"></div>
            
            <div class="entity-list">
                <h3>Extracted Entities</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Text</th>
                            <th>Type</th>
                            <th>Confidence</th>
                            <th>Source</th>
                        </tr>
                    </thead>
                    <tbody id="entityTable"></tbody>
                </table>
            </div>
        </div>
    </div>

    <script>
        document.getElementById('analyzeBtn').addEventListener('click', async function() {
            const text = document.getElementById('inputText').value.trim();
            
            if (!text) {
                alert('Please enter some text to analyze');
                return;
            }
            
            try {
                const response = await fetch('/extract', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ text })
                });
                
                const result = await response.json();
                
                // Display visualization
                document.getElementById('visualization').innerHTML = result.visualization;
                
                // Display entity table
                const tableBody = document.getElementById('entityTable');
                tableBody.innerHTML = '';
                
                result.entities.forEach(entity => {
                    const row = document.createElement('tr');
                    
                    const textCell = document.createElement('td');
                    textCell.textContent = entity.text;
                    
                    const typeCell = document.createElement('td');
                    typeCell.textContent = entity.type;
                    
                    const scoreCell = document.createElement('td');
                    scoreCell.textContent = entity.score.toFixed(2);
                    
                    const sourceCell = document.createElement('td');
                    sourceCell.textContent = entity.source;
                    
                    row.appendChild(textCell);
                    row.appendChild(typeCell);
                    row.appendChild(scoreCell);
                    row.appendChild(sourceCell);
                    
                    tableBody.appendChild(row);
                });
                
                document.getElementById('results').style.display = 'block';
            } catch (error) {
                console.error(error);
                alert('Error processing text');
            }
        });
    </script>
</body>
</html>
```

## Further Enhancements

- Implement entity linking to knowledge bases
- Add relationship extraction between entities
- Support entity co-reference resolution
- Implement active learning for improving custom entity extraction
- Add export functionality (CSV, JSON, XML)
