# Text Classifier for News Articles

This project implements a news article classifier that categorizes articles into predefined topics like politics, sports, technology, and entertainment. The implementation demonstrates both traditional ML and transformer-based approaches.

## Project Overview

- **Objective**: Build a classifier to automatically categorize news articles
- **Input**: News article text
- **Output**: Topic/category prediction
- **Technologies**: scikit-learn, NLTK, Hugging Face Transformers

## Implementation

### 1. Data Collection and Preparation

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Load dataset - you can use BBC News, AG News, or 20 Newsgroups
# Option 1: AG News dataset via Hugging Face
from datasets import load_dataset
dataset = load_dataset("ag_news")

# Option 2: Load from CSV
# df = pd.read_csv("news_articles.csv")
# texts = df['text'].tolist()
# labels = df['category'].tolist()

# Extract data and prepare for training
train_texts = [item['text'] for item in dataset['train']]
train_labels = [item['label'] for item in dataset['train']]
test_texts = [item['text'] for item in dataset['test']]
test_labels = [item['label'] for item in dataset['test']]

# Map label IDs to readable names
id2label = {0: "World", 1: "Sports", 2: "Business", 3: "Science/Technology"}
print(f"Dataset loaded with {len(train_texts)} training examples")
```

### 2. Traditional ML Approach

```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Text preprocessing
def preprocess_text(text):
    # Tokenize
    tokens = nltk.word_tokenize(text.lower())
    
    # Remove stopwords and non-alphabetic tokens
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]
    
    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    return ' '.join(tokens)

# Create ML pipeline
def create_classifier(classifier_type='nb'):
    if classifier_type == 'nb':
        classifier = MultinomialNB()
    elif classifier_type == 'lr':
        classifier = LogisticRegression(max_iter=1000)
    elif classifier_type == 'svm':
        classifier = LinearSVC()
    else:
        raise ValueError("Invalid classifier type")
    
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(preprocessor=preprocess_text, 
                                  ngram_range=(1, 2),
                                  max_features=50000)),
        ('classifier', classifier)
    ])
    
    return pipeline

# Train and evaluate
nb_classifier = create_classifier('nb')
nb_classifier.fit(train_texts[:10000], train_labels[:10000])  # Using a subset for speed

# Evaluate
predictions = nb_classifier.predict(test_texts[:1000])
print(classification_report(test_labels[:1000], predictions, 
                           target_names=[id2label[i] for i in range(4)]))
```

### 3. Transformer-Based Approach

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset

# Custom dataset class
class NewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and model
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=4
)

# Create datasets
train_dataset = NewsDataset(train_texts[:10000], train_labels[:10000], tokenizer)
test_dataset = NewsDataset(test_texts[:1000], test_labels[:1000], tokenizer)

# Training arguments
training_args = TrainingArguments(
    output_dir="./news_classifier",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    learning_rate=5e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Train the model
trainer.train()

# Evaluate
results = trainer.evaluate()
print(f"Evaluation results: {results}")
```

### 4. Building the Classifier API

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

# Load the trained model
@app.route('/predict', methods=['POST'])
def predict():
    # Get input data
    data = request.json
    text = data.get('text', '')
    
    if not text:
        return jsonify({'error': 'No text provided'}), 400
    
    # Make prediction
    # For traditional ML
    # category_id = nb_classifier.predict([text])[0]
    
    # For transformer model
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        predicted_class_id = outputs.logits.argmax().item()
    
    # Map to category name
    category = id2label[predicted_class_id]
    
    return jsonify({
        'category': category,
        'category_id': predicted_class_id
    })

if __name__ == '__main__':
    app.run(debug=True)
```

## Advanced Features

1. **Multi-label classification**: For articles that belong to multiple categories
2. **Confidence scores**: Return probability for each category
3. **Explainability**: Highlight words that influenced the classification
4. **Active learning**: Interface for users to correct misclassifications

## Evaluation Metrics

- Accuracy: Overall correctness
- Precision/Recall/F1: Performance on each category
- Confusion matrix: Identify common misclassifications

## Deployment Considerations

- Containerize with Docker for easy deployment
- Consider model quantization for faster inference
- Implement caching for frequently requested articles
- Set up monitoring for model drift detection

This news article classifier provides a practical demonstration of text classification techniques, from traditional ML approaches to state-of-the-art transformers, with a complete pipeline from data preparation to API deployment.
