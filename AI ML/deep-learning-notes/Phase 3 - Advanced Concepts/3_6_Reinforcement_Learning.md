
## ğŸ“˜ 3.6: **Reinforcement Learning (RL)**

### ğŸ¯ Goal

Train agents to make sequential decisions by **interacting with an environment** to **maximize rewards**.

---

## ğŸ§  1. Core Concepts

| Concept         | Description                            |
| --------------- | -------------------------------------- |
| **Agent**       | Learner or decision-maker              |
| **Environment** | What the agent interacts with          |
| **State (s)**   | A snapshot of the environment          |
| **Action (a)**  | Decision taken by the agent            |
| **Reward (r)**  | Feedback from the environment          |
| **Policy (Ï€)**  | Strategy mapping states to actions     |
| **Value (V)**   | Expected long-term return from a state |

---

## ğŸ” 2. The RL Loop

```plaintext
Agent â†’ takes action â†’ Environment â†’ returns reward & new state â†’ Agent â€¦
```

### ğŸ”¹ Objective:

Maximize cumulative reward (expected return) over time:


<p align="center">
  <img src="https://github.com/user-attachments/assets/46ab6c20-7de0-4854-8fb6-c86ff322952f " alt=" " />
</p>

Where **Î³** in [0,1] is the **discount factor**.

---

## ğŸ§® 3. Categories of RL Algorithms

### ğŸ”¹ Value-Based Methods

Learn a **value function** to evaluate states or state-action pairs.

#### ğŸ”¸ Q-Learning

* Learn Q(s, a): expected reward for taking action `a` in state `s`
* **Update rule**:

<p align="center">
  <img src="https://github.com/user-attachments/assets/be92efd4-7712-4a9d-83d0-b6fe76e5539f " alt=" " />
</p>

---

### ğŸ”¹ Policy-Based Methods

Directly learn the **policy function** Ï€(a|s) without using value functions.

#### ğŸ”¸ Policy Gradient

* Updates the policy parameters in the direction that improves performance:

<p align="center">
  <img src="https://github.com/user-attachments/assets/57797bd7-1a6f-41ba-9368-ca6ba7975528 " alt=" " />
</p>

---

### ğŸ”¹ Actor-Critic Methods

Combines both:

* **Actor**: Updates the policy Ï€
* **Critic**: Estimates value function V(s) or Q(s, a)

---

## ğŸ§  4. Deep Q Networks (DQN)

> Use **neural networks** to approximate Q-values.

### ğŸ”¸ Key Ideas:

* Replay Buffer: store transitions and sample mini-batches
* Target Network: stabilize training by delaying Q-target updates

### ğŸ”§ Simple DQN Flow:

1. Input: state
2. Output: Q-values for each action
3. Choose action: `argmax(Q)`
4. Update with Bellman equation

---

## ğŸ“š Libraries & Tools

* `Gymnasium` (OpenAI Gym): RL environment simulation
* `Stable-Baselines3`: High-level RL algorithms
* `RLlib`, `CleanRL`: Research & production-ready frameworks

---

## ğŸ§ª Exercises

### âœ… Conceptual

1. Whatâ€™s the difference between policy-based and value-based methods?
2. Why is experience replay useful in DQN?

### âœ… Practical

* Implement Q-learning on FrozenLake (from `gymnasium`)
* Train a DQN agent on CartPole-v1
* Try PPO (Proximal Policy Optimization) using `Stable-Baselines3`
