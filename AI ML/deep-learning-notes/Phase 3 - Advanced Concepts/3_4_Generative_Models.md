
## ğŸ“˜ 3.4: **Generative Models** (VAEs + GANs)

### ğŸ¯ Goal

Understand how models can learn to **generate new data** â€” like images, audio, or text â€” that resemble training samples.

---

## ğŸ§  What Are Generative Models?

Generative models learn to capture the **underlying distribution** of data and can **sample new data points** from it.

### ğŸ”„ Contrast with Discriminative Models:

| Discriminative               | Generative                          |
| ---------------------------- | ----------------------------------- |
| Learn **P(y \| x)**          | Learn **P(x)** or **P(x \| y)**     |
| Focus on classification      | Focus on data generation            |
| Example: Logistic Regression | Example: GAN, VAE                   |

---

## ğŸ”® 1. **Variational Autoencoders (VAEs)**

### ğŸ”¹ Structure

* **Encoder**: Maps input â†’ latent space (mean + std)
* **Latent space**: A distribution (not a point)
* **Decoder**: Reconstructs data from latent vector

### ğŸ§  Why "Variational"?

It learns a probability **distribution** over latent variables rather than fixed encodings.

### ğŸ” Key Loss Function:

```math
  Loss = Reconstruction  Loss + KL  Divergence
```


### ğŸ”§ PyTorch Sketch:

```python
z = mu + sigma * torch.randn_like(sigma)  # Reparameterization trick
```

### âœ… Use Cases

* Denoising images
* Anomaly detection
* Generative sampling (less sharp than GANs)

---

## ğŸ§¨ 2. **Generative Adversarial Networks (GANs)**

### ğŸ”¹ Core Idea: A two-player game

| Component         | Role                              |
| ----------------- | --------------------------------- |
| **Generator**     | Tries to generate fake data       |
| **Discriminator** | Tries to distinguish real vs fake |

They compete, and both improve.

### ğŸ¯ Objective:

<p align="center">
  <img src="https://github.com/user-attachments/assets/91121f23-ea57-4b16-87b8-d0400e6ab0cf " alt="Objective " />
</p>

### ğŸ”§ GAN Training Pitfalls:

* Mode collapse
* Training instability
* Needs careful tuning (learning rate, architecture)

### âœ… Use Cases

* Image synthesis (e.g., StyleGAN)
* Super-resolution
* Image-to-image translation (Pix2Pix, CycleGAN)

---

## ğŸ“Š Comparison: VAE vs GAN

| Feature        | VAE                       | GAN                      |
| -------------- | ------------------------- | ------------------------ |
| Stability      | More stable training      | Can be unstable          |
| Output Quality | Blurry                    | Sharp                    |
| Latent Space   | Structured, interpretable | Often less interpretable |
| Use Case       | Encoding + generation     | Pure generation          |

---

## ğŸ§ª Exercises

### âœ… Conceptual

1. Why do GANs need a discriminator?
2. What does KL divergence enforce in VAEs?

### âœ… Practical

* Train a VAE on MNIST and visualize the latent space.
* Train a simple GAN on Fashion-MNIST.
* Modify a GAN to generate hand-written digits from noise.

