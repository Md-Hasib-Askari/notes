

## ğŸ§ª 4.1: **Model Interpretability**

### ğŸ¯ Goal

Make deep learning models more understandable and **transparent** â€” especially important in sensitive domains (healthcare, finance, etc.).

---

## ğŸ” Why Interpretability Matters

| Purpose            | Use Case Example                               |
| ------------------ | ---------------------------------------------- |
| **Debugging**      | Identify why the model fails on edge cases     |
| **Trust**          | Show end-users how decisions were made         |
| **Compliance**     | Required in regulated industries (GDPR, HIPAA) |
| **Bias Detection** | Detect unethical or unwanted behavior          |

---

## ğŸ› ï¸ 1. Feature Importance Methods

### ğŸ”¹ **SHAP (SHapley Additive exPlanations)**

* Based on game theory (Shapley values)
* Measures each featureâ€™s contribution to predictions
* **Works with any model**
* Tools: `shap` Python package

**Example Plot:**

* SHAP summary plot showing global feature influence
* SHAP force plot showing local explanation for one prediction

### ğŸ”¹ **LIME (Local Interpretable Model-Agnostic Explanations)**

* Perturbs input data locally to understand the effect of each feature
* Trains a simple, interpretable model (like linear regression) locally

**Use Case:** Explain why a classifier predicted "spam" for a specific email.

---

## ğŸ§  2. Model-Specific Interpretability

| Model Type         | Techniques                           |
| ------------------ | ------------------------------------ |
| **CNNs**           | Grad-CAM, Guided Backpropagation     |
| **Tabular Models** | Feature importance, SHAP, LIME       |
| **Transformers**   | Attention visualization              |
| **RNNs**           | Hidden state analysis, saliency maps |

---

## ğŸ§ª 3. Exercises

### âœ… Theory

* Whatâ€™s the difference between SHAP and LIME?
* When would Grad-CAM be more useful than SHAP?

### âœ… Practice

* Use `shap.Explainer` on a trained XGBoost or PyTorch model
* Visualize Grad-CAM on a CNN trained on CIFAR-10
* Apply LIME on a sentiment analysis classifier

