## ğŸŸ¡ Intermediate Level â€“ Step 8: **Evaluation Metrics for GANs**

### ğŸ¯ Goal:

Learn how to **quantify GAN performance**, especially the *quality* and *diversity* of generated samples.

---

## ğŸ“ Why Evaluate?

GANs donâ€™t use loss as a true performance metric. So we need:

* **Image quality** assessment
* **Sample diversity** check
* **Similarity to real data**

---

## ğŸ”¥ Core Metrics

---

### 1. **Inception Score (IS)**

**Measures**:

* *Quality*: are generated images sharp and recognizable?
* *Diversity*: does the model produce various samples?

**How it works**:

* Run generated images through **Inception v3**
* Calculate:

  $$
  IS = \exp\left(\mathbb{E}_x [KL(p(y|x) \| p(y))]\right)
  $$

Where:

* $p(y|x)$ = predicted label distribution for image $x$
* $p(y)$ = average label distribution over many samples

**Higher IS = better**

* Ideal range (CIFAR-10): 7â€“9
* Baseline real data: \~11

**Limitations**:

* Doesnâ€™t compare to real images
* Not suitable for low-res/simple datasets like MNIST

---

### 2. **Frechet Inception Distance (FID)** âœ… **Most Popular**

**Measures**:

* Distance between real and fake image **feature distributions**

$$
FID = \| \mu_r - \mu_g \|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
$$

Where:

* $\mu_r, \Sigma_r$ = mean and covariance of real images
* $\mu_g, \Sigma_g$ = same for generated images
* Features are from **Inception-v3â€™s** intermediate layer

**Lower FID = better**

| Dataset  | Good FID |
| -------- | -------- |
| MNIST    | < 10     |
| CIFAR-10 | < 30     |
| CelebA   | < 10â€“20  |

**Advantages**:

* Compares to real data
* Captures both quality and diversity

---

### 3. **Precision & Recall for GANs**

**Idea**:

* **Precision**: how many samples are *realistic*?
* **Recall**: how much of the *real data distribution* is covered?

âœ… Better than IS alone
âœ… Can separate quality vs. diversity trade-off

**Visual**: Often plotted as a 2D graph

> Requires computing embeddings using Inception-v3 or similar.

---

### 4. **MS-SSIM (Multi-Scale Structural Similarity)**

**Purpose**: Measure **diversity** between generated images.

* Compute pairwise similarity scores in a batch.
* **Low MS-SSIM = more diversity**
* **High MS-SSIM = mode collapse**

---

## ğŸ§ª Code Example (FID with TorchMetrics)

```bash
pip install torchmetrics
```

```python
from torchmetrics.image.fid import FrechetInceptionDistance

fid = FrechetInceptionDistance(feature=64).to(device)

# During training:
fid.update(real_imgs, real=True)
fid.update(fake_imgs, real=False)

print("FID:", fid.compute().item())
```

> For MNIST, use `feature=64` instead of 2048 (due to lower resolution)

---

## âœ… Summary

| Metric               | Use Case                         | Higher/Lower           |
| -------------------- | -------------------------------- | ---------------------- |
| Inception Score (IS) | ImageNet-like datasets           | Higher is better       |
| FID                  | Most robust, real vs fake dist.  | Lower is better        |
| Precision/Recall     | Separates quality from diversity | Balanced               |
| MS-SSIM              | Detects mode collapse            | Lower = more diversity |
