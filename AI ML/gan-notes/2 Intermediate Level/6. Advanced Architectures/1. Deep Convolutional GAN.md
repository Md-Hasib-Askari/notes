## üü° Intermediate Level ‚Äì Step 6 (Part 1): **Deep Convolutional GAN (DCGAN)**

### üéØ Goal:

Improve GAN output quality by replacing fully connected layers with **convolutional layers** ‚Äî more suitable for images.

---

## üß† Why DCGAN?

The original GAN had fully connected layers ‚Üí bad for images (no spatial structure).
**DCGAN** uses:

* **ConvTranspose2d** in the Generator
* **Conv2d** in the Discriminator
* **BatchNorm**, **ReLU**, and **LeakyReLU** activations

> Result: smoother, more realistic images ‚Äî especially on datasets like CelebA or CIFAR-10.

---

## üîß Architectural Guidelines (from the DCGAN paper)

| Rule                                                   | Detail |
| ------------------------------------------------------ | ------ |
| Generator uses `ConvTranspose2d`, not `Linear`         |        |
| Discriminator uses `Conv2d`, not `Linear`              |        |
| **BatchNorm** in both G and D (except final layer)     |        |
| **ReLU** in G, **LeakyReLU** in D                      |        |
| **Tanh** as final activation in G (output in \[-1, 1]) |        |
| **Sigmoid** as final activation in D                   |        |

---

## üì¶ Generator (DCGAN Version)

```python
class DCGenerator(nn.Module):
    def __init__(self, latent_dim=100, img_channels=1, feature_maps=64):
        super().__init__()
        self.model = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, feature_maps * 8, 4, 1, 0),  # 1x1 -> 4x4
            nn.BatchNorm2d(feature_maps * 8),
            nn.ReLU(True),

            nn.ConvTranspose2d(feature_maps * 8, feature_maps * 4, 4, 2, 1),  # 4x4 -> 8x8
            nn.BatchNorm2d(feature_maps * 4),
            nn.ReLU(True),

            nn.ConvTranspose2d(feature_maps * 4, feature_maps * 2, 4, 2, 1),  # 8x8 -> 16x16
            nn.BatchNorm2d(feature_maps * 2),
            nn.ReLU(True),

            nn.ConvTranspose2d(feature_maps * 2, img_channels, 4, 2, 1),  # 16x16 -> 32x32
            nn.Tanh()
        )

    def forward(self, z):
        return self.model(z)
```

---

## üì¶ Discriminator (DCGAN Version)

```python
class DCDiscriminator(nn.Module):
    def __init__(self, img_channels=1, feature_maps=64):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(img_channels, feature_maps, 4, 2, 1),  # 32x32 -> 16x16
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1),  # 16x16 -> 8x8
            nn.BatchNorm2d(feature_maps * 2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1),  # 8x8 -> 4x4
            nn.BatchNorm2d(feature_maps * 4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_maps * 4, 1, 4, 1, 0),  # 4x4 -> 1x1
            nn.Sigmoid()
        )

    def forward(self, img):
        return self.model(img).view(-1, 1)
```

---

## ‚öôÔ∏è Inputs

* **Noise vector z** must be reshaped as `(batch_size, latent_dim, 1, 1)`

  ```python
  z = torch.randn(batch_size, 100, 1, 1).to(device)
  ```

* **Output image size**: `(batch_size, 1, 32, 32)` (or 64√ó64 for CelebA)

---

## üß™ Exercise: Build DCGAN

1. Replace your vanilla GAN networks with the DCGAN versions.
2. Train on **MNIST**, **CIFAR-10**, or **CelebA**.
3. Save sample images every few epochs and plot the loss.

---

## üß† Optional Tweaks

* Try **CelebA 64√ó64** for better image resolution.
* Add **Dropout** to the Discriminator for regularization.
* Replace `ReLU` in Generator with `GELU` and observe effects.

---

## ‚úÖ Summary

| Vanilla GAN            | DCGAN                          |
| ---------------------- | ------------------------------ |
| Fully connected layers | Conv layers                    |
| Slower convergence     | Faster convergence             |
| No batch norm          | Batch norm                     |
| Poor visual results    | Sharper, more realistic images |
