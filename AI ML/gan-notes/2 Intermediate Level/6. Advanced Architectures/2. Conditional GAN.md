## 🟡 Intermediate Level – Step 6 (Part 2): **Conditional GAN (cGAN)**

### 🎯 Goal:

Learn how to condition the GAN on **class labels**, so you can generate a **specific class** (e.g., digit "3" or image of a "cat").

---

## 🧠 Why cGAN?

In standard GANs, there’s no control over what kind of image the Generator produces.

**Conditional GAN** allows us to:

* **Guide the Generator** to produce a specific category.
* **Help the Discriminator** distinguish both *class + real/fake*.

---

## 🧩 Core Idea

Both the Generator and Discriminator get **class label** information.

```txt
Generator:
    Input: [noise vector z, label y] → output: image for label y

Discriminator:
    Input: [image, label y] → output: real or fake
```

---

## 🧱 Architecture Overview

### 1. **Label Embedding**

* Convert class label (0–9 for MNIST) to a dense vector.

### 2. **Concatenate Label + Input**

* `G(z, y)` → append label embedding to noise `z`
* `D(x, y)` → concatenate label info to image input

---

## 🧪 PyTorch Code: cGAN on MNIST

```python
class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim=100, num_classes=10, img_dim=784):
        super().__init__()
        self.label_embed = nn.Embedding(num_classes, num_classes)
        self.model = nn.Sequential(
            nn.Linear(latent_dim + num_classes, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, img_dim),
            nn.Tanh()
        )

    def forward(self, z, labels):
        label_embedding = self.label_embed(labels)
        x = torch.cat((z, label_embedding), dim=1)
        return self.model(x)


class ConditionalDiscriminator(nn.Module):
    def __init__(self, num_classes=10, img_dim=784):
        super().__init__()
        self.label_embed = nn.Embedding(num_classes, num_classes)
        self.model = nn.Sequential(
            nn.Linear(img_dim + num_classes, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x, labels):
        label_embedding = self.label_embed(labels)
        x = torch.cat((x, label_embedding), dim=1)
        return self.model(x)
```

---

## ⚙️ Training Loop (Changes)

```python
# Assume z is shape [batch_size, latent_dim]
# labels is shape [batch_size]
z = torch.randn(batch_size, latent_dim).to(device)
labels = torch.randint(0, 10, (batch_size,)).to(device)

# Generator forward
fake_imgs = G(z, labels)

# Discriminator forward
real_preds = D(real_imgs.view(batch_size, -1), labels)
fake_preds = D(fake_imgs.detach(), labels)

# Losses
real_loss = criterion(real_preds, torch.ones_like(real_preds))
fake_loss = criterion(fake_preds, torch.zeros_like(fake_preds))
d_loss = real_loss + fake_loss

# Generator loss
gen_preds = D(fake_imgs, labels)
g_loss = criterion(gen_preds, torch.ones_like(gen_preds))
```

---

## 📸 Visualizing Conditioned Samples

Generate digits `0–9` in a grid:

```python
z = torch.randn(10, latent_dim).to(device)
labels = torch.arange(0, 10).to(device)
samples = G(z, labels).view(-1, 1, 28, 28).detach().cpu()

# Plot
import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 10, figsize=(15, 2))
for i in range(10):
    axs[i].imshow(samples[i][0], cmap="gray")
    axs[i].axis('off')
plt.show()
```

---

## ✅ Summary

| Feature                    | GAN | cGAN |
| -------------------------- | --- | ---- |
| Random output              | ✅   | ❌    |
| Controlled generation      | ❌   | ✅    |
| Label used during training | ❌   | ✅    |
