## 🟡 Intermediate Level – Step 10: **Hyperparameter Tuning for GANs**

### 🎯 Goal:

Master the critical hyperparameters that control **training stability**, **speed**, and **output quality** in GANs.

---

## 🛠️ Key Hyperparameters to Tune

---

### 1. **Learning Rate (LR)**

* **Default for GANs:**

  * Generator: `0.0001 – 0.0002`
  * Discriminator: `0.0004` (slightly higher)

* **Why:**
  Discriminator often trains faster → can overpower G
  Use **Two-Time-Scale Update Rule (TTUR)** to balance:

```python
optimizer_G = Adam(G.parameters(), lr=1e-4, betas=(0.5, 0.999))
optimizer_D = Adam(D.parameters(), lr=4e-4, betas=(0.5, 0.999))
```

---

### 2. **Batch Size**

* Common values: `64`, `128`, `256`
* **Effects**:

  * Larger batch size → smoother gradients, more stable
  * Smaller batch size → faster, but more noise
* **Recommendation**:

  * Start with `64` or `128`
  * Avoid very small values (`<32`) unless memory-constrained

---

### 3. **Optimizer Choice**

* **Adam** is the standard for GANs

```python
Adam(..., betas=(0.5, 0.999))  # Classic GAN setting
```

* For WGAN, **RMSprop** or **Adam with low beta1**:

```python
Adam(..., betas=(0.0, 0.9))  # More stable in WGAN-GP
```

---

### 4. **Latent Vector Size (`z_dim`)**

* Common: `100`
* Larger values = more latent capacity, but harder to train
* Tune between `64 – 256`

---

### 5. **Number of D Updates per G Update**

* Default: `1:1`
* Better for stability: `n_critic = 5` (for WGAN)

```python
for _ in range(n_critic):
    train_discriminator()
train_generator()
```

---

### 6. **Weight Initialization**

* Use this to avoid dead neurons or vanishing gradients:

```python
def weights_init(m):
    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):
        nn.init.normal_(m.weight.data, 0.0, 0.02)
```

```python
G.apply(weights_init)
D.apply(weights_init)
```

---

### 7. **Normalization Layers**

* **Generator**: Use `BatchNorm`
* **Discriminator**: Use `SpectralNorm` or **no** BatchNorm for stability

---

## 🧪 Mini Experiment Plan

Try the following:

| Trial | Batch Size | LR\_G  | LR\_D  | Notes                |
| ----- | ---------- | ------ | ------ | -------------------- |
| A     | 64         | 0.0002 | 0.0002 | Baseline             |
| B     | 128        | 0.0001 | 0.0004 | TTUR                 |
| C     | 64         | 0.0001 | 0.0001 | WGAN-GP setup        |
| D     | 64         | 0.0002 | 0.0002 | + Spectral Norm in D |

---

## ✅ Summary Table

| Hyperparameter | Good Starting Point    |
| -------------- | ---------------------- |
| LR (G, D)      | 0.0002 / 0.0004        |
| Batch Size     | 64–128                 |
| Optimizer      | Adam with (0.5, 0.999) |
| Latent Dim     | 100                    |
| D\:G Steps     | 1:1 (or 5:1 for WGAN)  |
