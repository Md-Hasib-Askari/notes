## üü° Intermediate Level ‚Äì Step 7: **GAN Training Tricks**

### üéØ Goal:

Understand practical techniques to make GAN training more stable, robust, and effective.

---

## ‚öîÔ∏è Common Challenges These Tricks Fix

* Mode collapse
* Discriminator overpowering Generator
* Vanishing gradients
* Non-convergence

---

## üß† Core Tricks

---

### 1. **Label Smoothing**

**What it is:**
Instead of using `1` for real and `0` for fake labels, use:

* Real = `0.9`
* Fake = `0`

**Why:**
Prevents D from becoming **overconfident** and helps G learn.

```python
real_labels = torch.full((batch_size, 1), 0.9).to(device)
```

---

### 2. **One-Sided Label Flipping**

**What it is:**
Randomly flip **some real labels to 0** during training (not vice versa).

**Why:**
Keeps D from overpowering G by adding noise to training.

```python
if random.random() < 0.05:
    real_labels = torch.zeros_like(real_labels)  # flip real to fake
```

---

### 3. **Feature Matching (from Improved GANs)**

**What it is:**
Instead of directly trying to fool D, the Generator tries to match the **intermediate feature statistics** of real vs. fake samples from D.

**Why:**
Improves stability and diversity.

```python
# In D, extract intermediate features f(x)
# G_loss = MSE(f(real), f(fake))
```

> Requires modifying the Discriminator to expose internal features.

---

### 4. **Noise Injection**

**What it is:**
Add small noise to input images of D during training.

**Why:**
Acts like data augmentation to prevent D from overfitting.

```python
noisy_real = real_imgs + 0.05 * torch.randn_like(real_imgs)
```

---

### 5. **Spectral Normalization**

**What it is:**
Constrain the Lipschitz constant of each layer in D.

**Why:**
Improves stability by preventing exploding gradients.

```python
from torch.nn.utils import spectral_norm

self.conv = spectral_norm(nn.Conv2d(...))
```

> Common in **WGAN-GP**, **SNGAN**, **BigGAN**, etc.

---

### 6. **Two-Time-Scale Update Rule (TTUR)**

**What it is:**
Use different learning rates for G and D (e.g., G slower).

**Why:**
Prevents D from dominating too early.

```python
opt_G = Adam(G.parameters(), lr=0.0001)
opt_D = Adam(D.parameters(), lr=0.0004)
```

---

### 7. **Minibatch Discrimination**

**What it is:**
Let D evaluate multiple samples together to detect low diversity (e.g., same output every time).

**Why:**
Reduces mode collapse.

> Needs custom implementation in D: compute pairwise distances between samples in a minibatch.

---

### 8. **Use LeakyReLU (not ReLU)**

**Why:**
Avoids dying ReLU problem in Discriminator ‚Äî keeps gradient flow alive.

---

## üß™ Quick Experiments You Can Try

| Trick                | Try This                               |
| -------------------- | -------------------------------------- |
| Label smoothing      | Use 0.9 for real labels                |
| Spectral norm        | Wrap `nn.Conv2d` with `spectral_norm`  |
| Noise injection      | Add Gaussian noise to real/fake images |
| One-sided label flip | Flip 5% of real labels randomly to 0   |

---

## ‚úÖ Summary

| Trick            | Helps With                   |
| ---------------- | ---------------------------- |
| Label smoothing  | D domination                 |
| Spectral norm    | Gradient stability           |
| Feature matching | Mode collapse                |
| TTUR             | Training balance             |
| Label flipping   | Discriminator overconfidence |
