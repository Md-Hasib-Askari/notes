## 🟢 Beginner Level – Step 5: **Basic GAN Evaluation Techniques**

### 🎯 Goal:

Understand how to **quantify** the quality and diversity of GAN-generated samples.

---

## 📸 1. **Visual Inspection (Most Common)**

### 🔍 What to Look For:

* Are the generated samples sharp or blurry?
* Are they **diverse**, or just repeating?
* Do they resemble **real data**?

**Pros:** Fast, intuitive
**Cons:** Subjective, no quantitative comparison

> ✅ Always save generated samples every few epochs to observe changes.

---

## 📈 2. **Loss Curves Analysis**

Track and plot:

* **Generator loss**
* **Discriminator loss**

This helps diagnose:

* Mode collapse (G loss ↓ too fast, D loss flat)
* Vanishing gradients (loss stuck)
* Training instability (losses oscillate wildly)

> ✅ Plot loss after training to check convergence and balance.

---

## 📊 3. **Inception Score (IS)**

* Measures **quality + diversity** of generated images.
* Uses a pre-trained **Inception v3** network to compute probabilities of image classes.
* High-quality images = **confident predictions** (low entropy).
* Diverse images = **many different predictions** (high entropy over samples).

$$
\text{IS}(G) = \exp(\mathbb{E}_x[KL(p(y|x) \| p(y))])
$$

**Problems:**

* Not reliable for non-ImageNet-like data (like MNIST).
* Can be gamed by overfitting a small number of high-quality images.

> ✅ Use only when generating realistic natural images (e.g., CIFAR-10, CelebA).

---

## 📉 4. **Frechet Inception Distance (FID)**

* Measures the distance between **real** and **generated** image distributions.
* Uses features from **Inception v3** intermediate layer.

$$
\text{FID}(x, g) = \| \mu_x - \mu_g \|^2 + \text{Tr}(\Sigma_x + \Sigma_g - 2(\Sigma_x \Sigma_g)^{1/2})
$$

Where:

* $\mu_x, \Sigma_x$ = mean & covariance of real images
* $\mu_g, \Sigma_g$ = mean & covariance of generated images

**Lower FID = better.**

> ✅ FID is more robust than IS and is the **current standard** for GAN evaluation.

---

## ⚒️ 5. **Diversity Metrics (Bonus)**

* **Mode score:** Encourages mode coverage
* **Precision & Recall for GANs:** Separates quality (precision) from coverage (recall)
* **MS-SSIM (Structural Similarity):** Used to measure diversity (low MS-SSIM = more diverse)

---

## 🧪 Practical Exercise

### Visual Evaluation + Loss Plot

Add this to your training loop:

```python
g_losses, d_losses = [], []

# Inside training loop
g_losses.append(g_loss.item())
d_losses.append(d_loss.item())

# After training
import matplotlib.pyplot as plt
plt.plot(g_losses, label='G Loss')
plt.plot(d_losses, label='D Loss')
plt.legend()
plt.title("Loss Curve")
plt.show()
```

### Bonus (Optional):

Try computing **FID** using [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/image/frechet_inception_distance.html):

```python
from torchmetrics.image.fid import FrechetInceptionDistance
fid = FrechetInceptionDistance(feature=64).to(device)

# Add real and generated batches
fid.update(real_imgs, real=True)
fid.update(fake_imgs, real=False)

print("FID:", fid.compute())
```

> You can install TorchMetrics with `pip install torchmetrics`

---

### ✅ Recap

| Technique                  | What It Measures                                |
| -------------------------- | ----------------------------------------------- |
| Visual inspection          | Sample realism & diversity                      |
| Loss curves                | Training health                                 |
| Inception Score (IS)       | Confidence + diversity                          |
| FID Score                  | Distance between real & generated distributions |
| MS-SSIM / Precision-Recall | Sample diversity & quality                      |
