## 🟢 Beginner Level – Step 1: **Basic Concepts of GANs**

### 🎯 Goal:

Understand what GANs are, how they work, and their main components.

---

### 🔍 What is a GAN?

A **Generative Adversarial Network (GAN)** is a framework where **two neural networks compete**:

* **Generator (G):** Creates fake data.
* **Discriminator (D):** Judges if data is real (from dataset) or fake (from Generator).

They’re trained **together** in a game-theoretic setup:

* Generator tries to fool the Discriminator.
* Discriminator tries not to be fooled.

---

### 🧠 Key Terms:

| Term              | Description                                                    |
| ----------------- | -------------------------------------------------------------- |
| Generator         | Neural net that generates fake samples (e.g., images).         |
| Discriminator     | Neural net that distinguishes real vs. fake samples.           |
| Minimax Game      | Training objective where G minimizes the loss, D maximizes it. |
| Latent Vector (z) | Random noise input to the Generator.                           |
| Loss Function     | Binary cross-entropy (BCE) is commonly used.                   |

---

### ⚙️ How It Works

1. Sample real data: `x ~ pdata`
2. Sample noise: `z ~ pz` (usually Gaussian or uniform)
3. Generator makes fake: `G(z)`
4. Discriminator gets both real (`x`) and fake (`G(z)`) and outputs:

   * `D(x)` → Probability real
   * `D(G(z))` → Probability fake
5. Update:

   * **Discriminator** maximizes `log(D(x)) + log(1 - D(G(z)))`
   * **Generator** minimizes `log(1 - D(G(z)))` *(or maximizes `log(D(G(z)))` for stability)*

---

### 📊 Loss Functions

* **Discriminator Loss**:

  $$
  \mathcal{L}_D = -[\log D(x) + \log(1 - D(G(z)))]
  $$

* **Generator Loss**:

  $$
  \mathcal{L}_G = -\log D(G(z))
  $$

---

### 🧪 Real-World Use Cases

* Generate synthetic images (e.g., faces, anime, art)
* Data augmentation for ML
* Image super-resolution
* Image-to-image translation (e.g., day → night)

---

### 🧰 Tools to Learn

* Python
* PyTorch or TensorFlow (pick one — we’ll use **PyTorch** in this roadmap)
* Google Colab for training with GPU

---

### 💻 Exercise

1. Watch a video or animation explaining how GANs work (e.g., [3Blue1Brown GAN video](https://www.youtube.com/watch?v=8L11aMN5KY8)).
2. Install PyTorch and open Colab.
3. Write a Python function to generate 100 samples of random noise:

   ```python
   import torch
   z = torch.randn(100, 100)  # batch of 100 noise vectors of size 100
   print(z.shape)
   ```
4. Reflect:

   * What role does the noise vector play?
   * Why must the generator learn to turn it into realistic samples?
