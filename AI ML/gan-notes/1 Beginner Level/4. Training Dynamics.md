## 🟢 Beginner Level – Step 4: **Training Dynamics & Common Problems**

### 🎯 Goal:

Understand why GANs are hard to train and how to deal with problems like mode collapse, instability, and vanishing gradients.

---

## ⚠️ Why Training GANs Is Hard

Unlike typical supervised learning, GANs involve **two competing networks**. This can create a **non-stationary** optimization target and unstable feedback loops.

---

## 🔄 Common GAN Training Problems

### 1. **Mode Collapse**

* Generator produces **very limited types** of outputs (e.g., same face over and over).
* It finds a trick that fools the Discriminator too well.

**Symptoms**:

* Generated samples look nearly identical.
* Generator loss is low, but sample diversity is gone.

**Fixes**:

* Use **Wasserstein loss** (WGAN).
* Use **minibatch discrimination**.
* Add **noise to labels** (label smoothing).

---

### 2. **Vanishing Gradients**

* Discriminator becomes too good early.
* Generator stops learning because gradients are close to 0.

**Symptoms**:

* Generator loss doesn’t change much.
* D quickly reaches \~0.99 accuracy.

**Fixes**:

* Use **WGAN or LSGAN** for stable gradients.
* Use **one-sided label smoothing** (real labels = 0.9).
* Don’t overtrain D.

---

### 3. **Non-Convergence**

* G and D keep oscillating, never reaching stability.

**Symptoms**:

* Generator loss fluctuates wildly.
* Images vary a lot epoch to epoch.

**Fixes**:

* Try using **TTUR (Two Time-scale Update Rule)**: smaller learning rate for G.
* Monitor losses over time and visualize outputs frequently.
* Balance training of G and D (e.g., 1 step G for every 1–5 steps D).

---

### 4. **Discriminator Dominance**

* Discriminator wins the game and G can’t catch up.

**Symptoms**:

* D loss is near zero
* G loss increases

**Fixes**:

* Reduce D capacity (fewer layers/neurons)
* Add noise to real inputs
* Use **Dropout** in D

---

## 📉 What Loss Curves Should Look Like

* **D loss**: Starts high, drops, then plateaus with mild fluctuation
* **G loss**: Starts high, drops, fluctuates moderately
* Consistent reduction and slight bouncing is healthy

---

## 🧪 Mini Exercise

In your current GAN training loop:

1. **Plot generator and discriminator losses over time**

   ```python
   g_losses, d_losses = [], []

   # In training loop
   g_losses.append(g_loss.item())
   d_losses.append(d_loss.item())
   ```

2. **Plot after training**

   ```python
   plt.plot(g_losses, label="G Loss")
   plt.plot(d_losses, label="D Loss")
   plt.legend()
   plt.show()
   ```

3. Inspect your generated outputs:

   * Are they improving every 10 epochs?
   * Are they diverse or repetitive?

---

## 🔧 Tweaks to Try

* Label smoothing: replace `real_labels = 1.0` with `0.9`
* One-sided label flipping: sometimes label real as 0 and fake as 1 randomly
* Try changing the learning rate: `0.0001` for G and `0.0004` for D
