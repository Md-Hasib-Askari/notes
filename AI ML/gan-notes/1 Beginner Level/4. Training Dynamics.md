## ğŸŸ¢ Beginner Level â€“ Step 4: **Training Dynamics & Common Problems**

### ğŸ¯ Goal:

Understand why GANs are hard to train and how to deal with problems like mode collapse, instability, and vanishing gradients.

---

## âš ï¸ Why Training GANs Is Hard

Unlike typical supervised learning, GANs involve **two competing networks**. This can create a **non-stationary** optimization target and unstable feedback loops.

---

## ğŸ”„ Common GAN Training Problems

### 1. **Mode Collapse**

* Generator produces **very limited types** of outputs (e.g., same face over and over).
* It finds a trick that fools the Discriminator too well.

**Symptoms**:

* Generated samples look nearly identical.
* Generator loss is low, but sample diversity is gone.

**Fixes**:

* Use **Wasserstein loss** (WGAN).
* Use **minibatch discrimination**.
* Add **noise to labels** (label smoothing).

---

### 2. **Vanishing Gradients**

* Discriminator becomes too good early.
* Generator stops learning because gradients are close to 0.

**Symptoms**:

* Generator loss doesnâ€™t change much.
* D quickly reaches \~0.99 accuracy.

**Fixes**:

* Use **WGAN or LSGAN** for stable gradients.
* Use **one-sided label smoothing** (real labels = 0.9).
* Donâ€™t overtrain D.

---

### 3. **Non-Convergence**

* G and D keep oscillating, never reaching stability.

**Symptoms**:

* Generator loss fluctuates wildly.
* Images vary a lot epoch to epoch.

**Fixes**:

* Try using **TTUR (Two Time-scale Update Rule)**: smaller learning rate for G.
* Monitor losses over time and visualize outputs frequently.
* Balance training of G and D (e.g., 1 step G for every 1â€“5 steps D).

---

### 4. **Discriminator Dominance**

* Discriminator wins the game and G canâ€™t catch up.

**Symptoms**:

* D loss is near zero
* G loss increases

**Fixes**:

* Reduce D capacity (fewer layers/neurons)
* Add noise to real inputs
* Use **Dropout** in D

---

## ğŸ“‰ What Loss Curves Should Look Like

* **D loss**: Starts high, drops, then plateaus with mild fluctuation
* **G loss**: Starts high, drops, fluctuates moderately
* Consistent reduction and slight bouncing is healthy

---

## ğŸ§ª Mini Exercise

In your current GAN training loop:

1. **Plot generator and discriminator losses over time**

   ```python
   g_losses, d_losses = [], []

   # In training loop
   g_losses.append(g_loss.item())
   d_losses.append(d_loss.item())
   ```

2. **Plot after training**

   ```python
   plt.plot(g_losses, label="G Loss")
   plt.plot(d_losses, label="D Loss")
   plt.legend()
   plt.show()
   ```

3. Inspect your generated outputs:

   * Are they improving every 10 epochs?
   * Are they diverse or repetitive?

---

## ğŸ”§ Tweaks to Try

* Label smoothing: replace `real_labels = 1.0` with `0.9`
* One-sided label flipping: sometimes label real as 0 and fake as 1 randomly
* Try changing the learning rate: `0.0001` for G and `0.0004` for D
