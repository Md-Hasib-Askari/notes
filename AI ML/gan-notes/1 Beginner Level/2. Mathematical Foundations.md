## 🟢 Beginner Level – Step 2: **Mathematical Foundations for GANs**

### 🎯 Goal:

Understand the core math concepts behind how GANs learn and train, especially around probability, divergence, and gradients.

---

### 🧮 1. **Probability Distributions**

GANs try to make the **generated data distribution (pg)** match the **real data distribution (pdata)**.

* **Real data**: Comes from the actual dataset (e.g., MNIST, CelebA).
* **Fake data**: Generated by sampling noise `z` and transforming it with `G(z)`.

> Goal: Make `pg ≈ pdata`

---

### 📏 2. **Distance Between Distributions**

GANs don’t just generate random stuff—they try to *minimize the difference* between `pdata` and `pg`.

#### Common Measures:

| Measure           | Meaning                                          |
| ----------------- | ------------------------------------------------ |
| **KL Divergence** | Penalizes missing real modes.                    |
| **JS Divergence** | More symmetric and stable. Used in vanilla GANs. |

**JS Divergence** (Jensen-Shannon):

$$
\text{JS}(P \parallel Q) = \frac{1}{2} \text{KL}(P \parallel M) + \frac{1}{2} \text{KL}(Q \parallel M)
$$

Where $M = \frac{1}{2}(P + Q)$

> GANs minimize this indirectly through adversarial loss.

---

### 🔄 3. **Loss Functions – Binary Cross Entropy**

Used for both Generator and Discriminator as it's a binary classification problem:

#### Discriminator:

$$
\mathcal{L}_D = -[ \log(D(x)) + \log(1 - D(G(z))) ]
$$

#### Generator:

$$
\mathcal{L}_G = -\log(D(G(z)))
$$

> Generator wants to “fool” D into thinking fake samples are real.

---

### 🔁 4. **Backpropagation & Gradient Descent**

* GANs use **stochastic gradient descent** or **Adam optimizer**.
* Gradients flow **through the discriminator into the generator**.
* This allows the generator to **learn how to improve** based on D’s judgment.

---

### 💡 Summary

| Concept              | Why It Matters                                 |
| -------------------- | ---------------------------------------------- |
| Probability          | GANs are probability matchers                  |
| Divergence           | Tells us how “different” two distributions are |
| Binary Cross-Entropy | Core loss function for D and G                 |
| Gradient Descent     | How the networks improve                       |

---

### 💻 Exercise

**Task**: Implement Binary Cross Entropy manually to understand it.

```python
import torch
import torch.nn.functional as F

# Assume discriminator prediction on real (D(x)) = 0.9
# and fake (D(G(z))) = 0.1
real_pred = torch.tensor([0.9])
fake_pred = torch.tensor([0.1])

# Discriminator loss
loss_real = F.binary_cross_entropy(real_pred, torch.ones_like(real_pred))
loss_fake = F.binary_cross_entropy(fake_pred, torch.zeros_like(fake_pred))
disc_loss = loss_real + loss_fake

# Generator loss (tries to make D(G(z)) → 1)
gen_loss = F.binary_cross_entropy(fake_pred, torch.ones_like(fake_pred))

print(f"Discriminator Loss: {disc_loss.item():.4f}")
print(f"Generator Loss: {gen_loss.item():.4f}")
```

> 🔍 Try changing the prediction values and watch how the loss changes.
