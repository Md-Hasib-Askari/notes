## ğŸ”´ Advanced Level â€“ Step 14: **Unsupervised & Self-Supervised GANs**

### ğŸ¯ Goal:

Learn how to build GANs that can:

* Discover structure in data without labels
* Cluster images by content
* Leverage contrastive learning for better representation

---

## ğŸ§  1. **Contrastive GANs**

### ğŸ” Concept:

Use **contrastive learning** within the GAN to improve the discriminatorâ€™s understanding of semantic similarity.

#### ğŸ”§ How it works:

* The Discriminator learns **feature embeddings** of images.
* Instead of just real/fake, it uses a **contrastive loss**:

  * Pulls together **real-real** or **fake-fake** samples
  * Pushes apart **real-fake** pairs

#### ğŸ“Œ Common Loss: **InfoNCE**

$$
\mathcal{L}_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(x_i, x_j)/\tau)}{\sum_k \exp(\text{sim}(x_i, x_k)/\tau)}
$$

> $\text{sim}(a,b)$ is cosine similarity, $\tau$ is temperature.

#### âœ… Benefits:

* Better discriminator features
* **Improved Generator quality** (even if G isnâ€™t directly optimized with contrastive loss)

---

## ğŸ§  2. **Unsupervised Image Clustering with GANs**

### ğŸ” Goal:

Use a GAN to **discover classes or clusters** in data â€” without labels.

#### ğŸ§© Techniques:

* Cluster in latent space of `G(z)`
* Use **InfoGAN** or **DiscoGAN** to identify interpretable factors
* Apply **k-means** on feature embeddings from Discriminator or Q-network

#### âœ… Example:

* Cluster MNIST digits without labels
* Find semantic categories in CelebA (e.g., hair color, expression)

---

## ğŸ§  3. **Semi-Supervised GANs**

### ğŸ” Use a small set of labeled data to **guide learning** while the majority of training data is unlabeled.

#### ğŸ”§ Approach:

* Modify **Discriminator** to output `K + 1` classes:

  * `K` real classes (e.g., digits 0â€“9)
  * `+1` = fake samples

#### ğŸ§ª Loss:

* Cross-entropy for labeled data
* GAN loss for fake data

#### âœ… Results:

* Strong classifiers with **<10% labeled data**
* Used in **Improved GANs** (Salimans et al.)

---

## ğŸ§  4. **Self-Supervised GAN Training**

### ğŸ” Use **pretext tasks** to improve GAN performance:

* **Rotation prediction** (predict angle the image is rotated)
* **Jigsaw puzzles**, **colorization**, **inpainting**

These tasks train the **Discriminator** to learn **useful representations** even without labels.

> Example: BigBiGAN uses self-supervised encoders + GAN loss

---

## âœ… Summary Table

| Method                | Description                | Supervision         |
| --------------------- | -------------------------- | ------------------- |
| Contrastive GAN       | Contrastive loss in D      | Unsupervised        |
| InfoGAN               | Discover latent factors    | Unsupervised        |
| Semi-Supervised GAN   | Class + GAN loss           | Partial supervision |
| Self-Supervised Tasks | Auxiliary training signals | Unsupervised        |

---

## ğŸ§ª Project Ideas

| Project             | Dataset            | Goal                             |
| ------------------- | ------------------ | -------------------------------- |
| InfoGAN clustering  | MNIST              | Group digits w/o labels          |
| Contrastive GAN     | CIFAR-10           | Improve diversity + realism      |
| Semi-Supervised GAN | 10% labeled CelebA | Learn attributes                 |
| Self-Supervised GAN | STL-10             | Train D with rotation prediction |
