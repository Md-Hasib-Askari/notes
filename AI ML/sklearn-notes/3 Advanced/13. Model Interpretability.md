## ðŸ”µ Advanced Level

### 13. Model Interpretability

Model interpretability helps you understand **why** your model made a prediction. Itâ€™s essential for debugging, trust, and complianceâ€”especially in fields like finance, healthcare, and law.

---

### ðŸ“Œ 1. Feature Importance Visualization

Most tree-based models (e.g., Random Forest, Gradient Boosting) have a built-in `.feature_importances_` attribute.

```python
import matplotlib.pyplot as plt
import numpy as np

model = RandomForestClassifier()
model.fit(X_train, y_train)

importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.title("Feature Importances")
plt.show()
```

---

### ðŸ“Œ 2. Partial Dependence Plots (PDP)

Shows how one or two features affect the predicted outcome, marginalizing over others.

```python
from sklearn.inspection import PartialDependenceDisplay

model.fit(X_train, y_train)
PartialDependenceDisplay.from_estimator(model, X_train, ['age', 'income'])
```

âœ… Great for understanding **nonlinear** feature influence.

---

### ðŸ“Œ 3. Integration with SHAP (SHapley Additive exPlanations)

Provides local and global model explainability based on cooperative game theory.

```python
import shap

model.fit(X_train, y_train)
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test)
```

âœ… Works with tree-based, linear, and even deep models
âœ… Explains **individual predictions**

---

### ðŸ“Œ 4. Integration with LIME (Local Interpretable Model-Agnostic Explanations)

LIME explains individual predictions by approximating the model locally with a simple one.

```python
from lime.lime_tabular import LimeTabularExplainer

explainer = LimeTabularExplainer(X_train.values, feature_names=X.columns, class_names=['No', 'Yes'], mode='classification')
exp = explainer.explain_instance(X_test.iloc[0].values, model.predict_proba)
exp.show_in_notebook()
```

âœ… Useful for debugging specific predictions or edge cases.

---

### ðŸ“Œ 5. Which to Use When?

| Method             | Global/Local | Best For                      |
| ------------------ | ------------ | ----------------------------- |
| Feature Importance | Global       | Tree models, quick overview   |
| PDP                | Global       | Single feature effect         |
| SHAP               | Both         | Most comprehensive/explained  |
| LIME               | Local        | Single prediction explanation |
