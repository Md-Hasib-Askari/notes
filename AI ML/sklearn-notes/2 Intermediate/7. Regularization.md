## ðŸŸ¡ Intermediate Level

### 7. Regularization

**Regularization** is a technique to prevent overfitting by adding a penalty to large model coefficients. Scikit-learn supports regularization in regression models like Ridge, Lasso, and ElasticNet.

---

### ðŸ“Œ 1. Why Regularization?

* Prevents model from becoming too complex.
* Encourages simpler models that generalize better.
* Useful when you have many features or multicollinearity.

---

### ðŸ“Œ 2. Ridge Regression (L2 Regularization)

* Penalizes the **squared magnitude** of coefficients.
* Shrinks coefficients but never sets them to zero.

```python
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
y_pred = ridge.predict(X_test)
```

---

### ðŸ“Œ 3. Lasso Regression (L1 Regularization)

* Penalizes the **absolute magnitude** of coefficients.
* Can shrink some coefficients exactly to **zero** â†’ Feature selection.

```python
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
y_pred = lasso.predict(X_test)
```

---

### ðŸ“Œ 4. ElasticNet (L1 + L2 Regularization)

* Combines both L1 and L2 penalties.
* Balances between Ridge and Lasso.

```python
from sklearn.linear_model import ElasticNet

enet = ElasticNet(alpha=0.1, l1_ratio=0.5)
enet.fit(X_train, y_train)
y_pred = enet.predict(X_test)
```

* `l1_ratio=0.0` â†’ Ridge
* `l1_ratio=1.0` â†’ Lasso

---

### ðŸ“Œ 5. Comparing Models

```python
from sklearn.metrics import r2_score

print("Ridge RÂ²:", r2_score(y_test, ridge.predict(X_test)))
print("Lasso RÂ²:", r2_score(y_test, lasso.predict(X_test)))
print("ElasticNet RÂ²:", r2_score(y_test, enet.predict(X_test)))
```

---

### ðŸ“Œ 6. Regularization in Logistic Regression

Also supports L1 and L2 regularization:

```python
from sklearn.linear_model import LogisticRegression

# L2 regularization (default)
log_reg = LogisticRegression(penalty='l2', C=1.0)

# L1 regularization
log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)
```

Note: `C` is the **inverse** of regularization strength.
Lower `C` = Stronger regularization.

