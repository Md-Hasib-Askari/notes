## ðŸŸ¡ Intermediate Level

### 9. Ensemble Methods

**Ensemble methods** combine multiple models to produce better predictive performance than any individual model. Scikit-learn offers several powerful ensemble techniques.

---

### ðŸ“Œ 1. Why Use Ensembles?

* Reduce **variance** (overfitting)
* Reduce **bias** (underfitting)
* Improve **accuracy** and **robustness**

---

### ðŸ“Œ 2. Bagging (Bootstrap Aggregating)

**Key Idea:** Train several base learners on different random subsets of data and average their predictions.

#### âœ… Random Forest (for classification/regression)

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
```

* Handles high-dimensional data
* Less prone to overfitting than decision trees

---

### ðŸ“Œ 3. Boosting

**Key Idea:** Train models sequentially, each correcting the errors of the previous one.

#### âœ… AdaBoost

```python
from sklearn.ensemble import AdaBoostClassifier

ab = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)
ab.fit(X_train, y_train)
y_pred = ab.predict(X_test)
```

---

#### âœ… Gradient Boosting

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)
gb.fit(X_train, y_train)
y_pred = gb.predict(X_test)
```

* Learns slowly and adjusts residuals at each step.

---

### ðŸ“Œ 4. Voting Classifier

**Key Idea:** Combine different models and take a majority vote (classification) or average (regression).

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

voting_clf = VotingClassifier(estimators=[
    ('lr', LogisticRegression()),
    ('svc', SVC(probability=True)),
    ('dt', DecisionTreeClassifier())
], voting='soft')

voting_clf.fit(X_train, y_train)
y_pred = voting_clf.predict(X_test)
```

---

### ðŸ“Œ 5. Stacking

**Key Idea:** Train multiple base models and use their outputs as features for a meta-model.

```python
from sklearn.ensemble import StackingClassifier

stacking = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression()),
        ('dt', DecisionTreeClassifier())
    ],
    final_estimator=RandomForestClassifier()
)
stacking.fit(X_train, y_train)
```

---

### ðŸ“Œ 6. Comparison Table

| Method   | Handles Overfitting | Accuracy | Complexity |
| -------- | ------------------- | -------- | ---------- |
| Bagging  | âœ… High              | âœ…        | Medium     |
| Boosting | âœ… High              | âœ…âœ…       | High       |
| Voting   | Medium              | Medium   | Low        |
| Stacking | âœ…âœ…                  | âœ…âœ…       | High       |

