# 1.2 Core Concepts of Vector Databases

## What are Vector Databases?

Vector databases are specialized database systems designed to store, index, and query vector embeddings efficiently.

### Traditional Databases vs Vector Databases

| Traditional Databases | Vector Databases |
|------------------------|-------------------|
| Store structured data (tables, documents) | Store vector embeddings |
| Exact match queries | Similarity-based queries |
| SQL or NoSQL interfaces | Vector search APIs |
| B-trees, hash indices | ANN algorithms (HNSW, IVF, etc.) |
| ACID properties focus | Performance and recall focus |

### When to Use Vector Databases

Vector databases excel in scenarios requiring:
- Semantic search beyond keyword matching
- Recommendation systems based on similarity
- Image, audio, or video similarity search
- Natural language processing applications
- Pattern recognition in high-dimensional data

Key decision factors:
- Data volume (vectors + metadata)
- Query performance requirements
- Need for specialized distance metrics
- Integration with ML/AI workflows

### Real-world Applications and Use Cases

1. **Search Systems**:
   - Semantic document search
   - Image similarity search
   - Multi-modal search experiences

2. **Recommendation Engines**:
   - Content-based filtering
   - Item-to-item recommendations
   - Personalized content discovery

3. **AI/ML Applications**:
   - Retrieval-Augmented Generation (RAG)
   - Face recognition systems
   - Anomaly detection
   - Duplicate detection

4. **Domain-Specific Applications**:
   - Drug discovery (molecular similarity)
   - Genomic sequence matching
   - Financial fraud detection
   - Geospatial analysis

### Vector Embeddings Explained

Vector embeddings are numerical representations of data in a continuous vector space:

- **Definition**: Dense vectors where semantic similarity in the original data is preserved as proximity in vector space
- **Dimensionality**: Typically ranges from 100-4096 dimensions
- **Properties**:
  - Similar items have similar vectors (smaller distances)
  - Vector arithmetic can reveal semantic relationships
  - Dimension reduction preserves most important information

## Vector Representations

### Text Embeddings

- **Word2Vec**:
  - Creates word vectors based on context
  - Models: Skip-gram and CBOW (Continuous Bag of Words)
  - Captures semantic relationships (king - man + woman â‰ˆ queen)
  
- **GloVe**:
  - Global Vectors for Word Representation
  - Combines global matrix factorization with local context
  - Pre-trained on large corpora

- **Modern Approaches**:
  - Transformer-based (BERT, GPT)
  - Sentence-level embeddings
  - Fine-tuned for specific domains

### Image Embeddings Concepts

- **Approaches**:
  - CNN feature extraction
  - Pre-trained models (ResNet, EfficientNet)
  - Self-supervised learning (CLIP, DALL-E)
  
- **Properties**:
  - Capture visual features hierarchically
  - Transfer learning from large datasets
  - Can be domain-specific or general

### Audio Embeddings Overview

- **Techniques**:
  - Spectrograms + CNNs
  - Wav2Vec, AudioCLIP
  - MFCCs (Mel-frequency cepstral coefficients)
  
- **Applications**:
  - Music similarity
  - Speaker recognition
  - Audio classification
  - Speech-to-text systems

### Numerical Feature Vectors

- **Tabular Data**:
  - Normalized numerical features
  - One-hot encoded categorical features
  
- **Time Series**:
  - Statistical features
  - Frequency domain features
  - Wavelet coefficients
  
- **Sensor Data**:
  - Processed sensor readings
  - Feature extraction from raw signals
  - Dimensionality reduction
