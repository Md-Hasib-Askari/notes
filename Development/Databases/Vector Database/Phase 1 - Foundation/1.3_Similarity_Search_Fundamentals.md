# 1.3 Similarity Search Fundamentals

## Distance Metrics

Distance metrics quantify the similarity or dissimilarity between vectors. Choosing the right metric is crucial for effective vector search.

### Euclidean Distance

- **Formula**: d(x,y) = √(Σᵢ(xᵢ-yᵢ)²)
- **Properties**:
  - Intuitive "as-the-crow-flies" distance
  - Sensitive to magnitude differences
  - Satisfies all metric axioms
- **Best For**:
  - Low to medium dimensional data
  - When magnitude matters
  - Data with similar scales across dimensions
- **Limitations**:
  - Suffers from "curse of dimensionality"
  - Less effective for sparse data

### Manhattan Distance (L1 Norm)

- **Formula**: d(x,y) = Σᵢ|xᵢ-yᵢ|
- **Properties**:
  - "City block" or "taxicab" distance
  - Sum of absolute differences
  - Less affected by outliers than Euclidean
- **Best For**:
  - Grid-like structures
  - Robust feature comparisons
  - When routes follow axes
- **Limitations**:
  - Doesn't account for correlation between dimensions

### Cosine Similarity

- **Formula**: cos(θ) = (x·y)/(||x||·||y||)
- **Properties**:
  - Measures angle between vectors
  - Range: [-1,1] (often normalized to [0,1])
  - Ignores magnitude, focuses on direction
- **Best For**:
  - Text embeddings and sparse vectors
  - When relative orientation matters more than magnitude
  - High-dimensional data
- **Limitations**:
  - Not a true distance metric (triangle inequality)
  - Insensitive to vector magnitude

### Hamming Distance

- **Formula**: Count of positions where bits differ
- **Properties**:
  - Only applicable to equal-length strings/binary vectors
  - Each position contributes 0 or 1 to distance
  - Integer values only
- **Best For**:
  - Binary vectors and hash codes
  - Error detection
  - Binary similarity search
- **Limitations**:
  - Only for binary/discrete data
  - No concept of "degree" of difference

### When to Use Each Metric

Selection criteria:
1. **Data Distribution**:
   - Normalized data → Cosine or Euclidean
   - Binary data → Hamming
   - Sparse data → Cosine

2. **Semantic Requirements**:
   - Direction matters, magnitude doesn't → Cosine
   - Absolute distances matter → Euclidean
   - Ordinal differences matter → Manhattan

3. **Computational Considerations**:
   - Manhattan: Fastest, no square roots
   - Euclidean: Standard but requires square root
   - Cosine: Requires normalization and dot product

4. **Domain-Specific Requirements**:
   - Text search → Typically cosine
   - Image search → Often Euclidean
   - Genomic data → Special metrics (Levenshtein, etc.)

## Basic Search Algorithms

### Brute Force Search

- **Approach**:
  - Compare query vector with every vector in the database
  - Calculate distance/similarity for each comparison
  - Sort results by distance/similarity
  - Return top-k nearest neighbors

- **Properties**:
  - 100% recall (finds exact nearest neighbors)
  - Simple implementation
  - Works with any distance metric

- **Limitations**:
  - O(nd) time complexity (n = dataset size, d = dimensions)
  - Impractical for large datasets
  - Doesn't scale well

### K-Nearest Neighbors (KNN)

- **Approach**:
  - Find k closest vectors to the query vector
  - Return these vectors as the result
  - Can use any distance metric

- **Properties**:
  - Fundamental algorithm for similarity search
  - No training phase
  - Results are interpretable

- **Variants**:
  - Weighted KNN
  - KNN with specialized data structures
  - Approximate KNN (ANN)

### Time Complexity Considerations

- **Brute Force**:
  - Query time: O(nd)
  - Memory: O(nd)
  - Build time: O(1)

- **Tree-based methods** (not covered in detail here):
  - Query time: O(log n) best case, O(n) worst case
  - Build time: O(n log n)
  - Effectiveness degrades in high dimensions

- **Graph-based methods** (e.g., HNSW):
  - Query time: O(log n) empirically
  - Build time: O(n log n)
  - Excellent performance in high dimensions

- **Performance Factors**:
  - Vector dimensionality
  - Dataset size
  - Hardware (CPU/GPU/memory)
  - Distance metric computation cost
  - Required accuracy (exact vs. approximate)
