# 2.2 Indexing Algorithms

## Hierarchical Navigable Small Worlds (HNSW)

HNSW is a state-of-the-art graph-based algorithm for approximate nearest neighbor search, offering excellent performance-accuracy trade-offs.

### Algorithm Principles

- **Graph-Based Approach**:
  - Creates a navigable small-world graph
  - Multiple interconnected layers with decreasing density
  - Logarithmic search complexity
  - Greedy routing with shortcuts

- **Key Innovations**:
  - Hierarchical structure for efficient navigation
  - Skip-list-like architecture
  - Proximity graph with long-range connections
  - Balance between exploration and exploitation

### Construction Process

1. **Layer Assignment**:
   - Each vector assigned a maximum layer randomly
   - Probability decreases exponentially with layer height
   - Top layer has fewest points

2. **Graph Building**:
   - Start from top layer
   - For each new element:
     - Find entry point (nearest neighbor in layer above)
     - Perform greedy search to find M nearest neighbors
     - Connect bidirectionally with these neighbors
   - Repeat for each layer down to base layer

3. **Edge Management**:
   - Maintain fixed maximum connections per node (M)
   - Heuristic neighbor selection to preserve graph properties
   - Pruning strategies to maintain quality connections

### Search Process

1. **Entry Point Selection**:
   - Start at entry point in top layer
   - Typically fixed or randomly selected

2. **Layer-by-Layer Descent**:
   - Greedy search in current layer to find closest neighbors
   - Descend to lower layer using best candidates
   - Expand search neighborhood at each step

3. **Priority Queue Navigation**:
   - Maintain priority queue of candidates
   - Explore ef closest neighbors at each layer
   - Return k closest elements from base layer

### Parameters Tuning

- **M** (connections per node):
  - Controls graph connectivity
  - Typical values: 12-64
  - Higher values: better recall, more memory

- **efConstruction** (search width during construction):
  - Influences build quality
  - Typical values: 100-500
  - Higher values: better recall, slower build

- **ef** (search width during query):
  - Controls search accuracy vs speed
  - Typical values: 50-200
  - Higher values: better recall, slower queries

## Locality Sensitive Hashing (LSH)

LSH approximates similarity search by hashing similar items to the same buckets with high probability.

### Hash Families

- **Definition**: Collection of hash functions with similarity-preserving properties
- **Requirements**:
  - Similar items hash to same bucket with high probability
  - Dissimilar items hash to different buckets with high probability

- **Common Hash Families**:
  - Random projections (for cosine similarity)
  - MinHash (for Jaccard similarity)
  - p-stable distributions (for Lp distances)

### Random Projections

- **Technique**:
  - Generate random unit vectors
  - Project data onto these vectors
  - Apply threshold to binarize result

- **Mathematical Basis**:
  - Johnson-Lindenstrauss lemma
  - Hyperplane partitioning of space
  - Angular distance preservation

- **Implementation**:
  - Hash function: h(v) = sign(rÂ·v) where r is random vector
  - Concatenate multiple hashes to form signature
  - Group vectors by matching signatures

### MinHash for Sets

- **Technique**:
  - Apply random permutations to universe of elements
  - For each set, hash is index of first element present
  - Multiple permutations create signature

- **Properties**:
  - Probability of hash collision equals Jaccard similarity
  - Efficient for sparse binary data
  - Can be implemented with single pass over data

### Implementation Considerations

- **Multi-probe LSH**:
  - Query multiple adjacent buckets
  - Reduces false negatives
  - Improves recall at cost of query time

- **Amplification techniques**:
  - AND-construction (stricter matching)
  - OR-construction (more permissive matching)
  - Balancing precision and recall

- **Memory-accuracy tradeoffs**:
  - Number of hash functions
  - Signature length
  - Number of hash tables

## Inverted File Index (IVF)

IVF uses a clustering-based approach for efficient vector search.

### Clustering-based Approaches

- **Core Concept**:
  - Partition vectors into clusters
  - Search only within relevant clusters
  - Trade accuracy for speed

- **Implementation**:
  - K-means or other clustering algorithm
  - Assign vectors to nearest centroid
  - Query searches only in nearest clusters

- **Performance factors**:
  - Number of clusters (nlist)
  - Number of clusters to search (nprobe)
  - Cluster size distribution

### Quantization Concepts

- **Vector Quantization**:
  - Approximate vectors with representative codewords
  - Reduce memory footprint
  - Trade accuracy for storage efficiency

- **Techniques**:
  - Scalar quantization (per component)
  - Vector quantization (whole vector)
  - Residual quantization (iterative refinement)

### Product Quantization Basics

- **Principle**:
  - Split high-dimensional vector into subvectors
  - Quantize each subvector independently
  - Combine quantized subvectors for final representation

- **Advantages**:
  - Exponentially larger codebook without exponential storage
  - Adjustable compression-accuracy trade-off
  - Efficient distance computation (lookup tables)

- **Implementations**:
  - IVF-PQ (inverted file with product quantization)
  - Combining coarse quantization with product quantization
  - Distance computation approximations
