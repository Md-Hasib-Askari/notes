# 2.3 Embedding Generation

## Text Embeddings

### Sentence Transformers

- **Overview**:
  - Framework for generating sentence/document embeddings
  - Built on transformer models
  - Optimized for semantic similarity tasks

- **Key Features**:
  - Mean pooling of token embeddings
  - Siamese/triplet network training
  - Contrastive learning objectives
  - Specialized for similarity comparison

- **Usage**:
  ```python
  from sentence_transformers import SentenceTransformer
  model = SentenceTransformer('all-MiniLM-L6-v2')
  embeddings = model.encode(["Vector search is powerful", 
                           "Embeddings enable semantic search"])
  ```

### BERT and Variants

- **BERT Architecture**:
  - Bidirectional Encoder Representations from Transformers
  - Pre-trained on masked language modeling
  - Context-aware embeddings

- **Embedding Strategies**:
  - [CLS] token embedding (classification tasks)
  - Mean/max pooling of token embeddings
  - Layer selection (typically last 4 layers)

- **Notable Variants**:
  - RoBERTa: Improved training methodology
  - DistilBERT: Smaller, faster model
  - ALBERT: Parameter-efficient version
  - Domain-specific BERTs (BioBERT, SciBERT, etc.)

### OpenAI Embeddings

- **Models**:
  - text-embedding-ada-002 (OpenAI's standard embedding model)
  - text-embedding-3-small/large (newer generation)

- **Characteristics**:
  - High-quality general-purpose embeddings
  - Good performance across diverse tasks
  - Standardized dimensionality (1536 for ada-002)
  - Excellent text retrieval performance

- **Integration**:
  ```python
  from openai import OpenAI
  client = OpenAI()
  response = client.embeddings.create(
    model="text-embedding-ada-002",
    input="Your text here"
  )
  embedding = response.data[0].embedding
  ```

### Custom Fine-tuning Basics

- **Transfer Learning Approaches**:
  - Start with pre-trained model
  - Fine-tune on domain-specific data
  - Use specialized training objectives

- **Training Objectives**:
  - Contrastive learning (SimCSE, etc.)
  - Triplet loss
  - Multiple negatives ranking loss
  - Knowledge distillation

- **Data Considerations**:
  - Curated pairs/triplets
  - Domain-specific corpus
  - Synthetic data generation
  - Data augmentation techniques

## Multimodal Embeddings

### CLIP for Image-Text

- **Architecture**:
  - Contrastive Language-Image Pre-training
  - Dual encoder (vision + text)
  - Joint embedding space

- **Capabilities**:
  - Zero-shot classification
  - Cross-modal retrieval
  - Open-vocabulary image understanding

- **Implementation**:
  ```python
  from transformers import CLIPProcessor, CLIPModel
  model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
  processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
  inputs = processor(text=["a photo of a cat"], 
                   images=image, return_tensors="pt")
  outputs = model(**inputs)
  ```

### Audio Embeddings

- **Approaches**:
  - Self-supervised audio models (wav2vec 2.0)
  - Audio spectrogram transformers
  - Contrastive audio-text models (Audio-CLIP)

- **Applications**:
  - Music retrieval
  - Sound classification
  - Voice recognition
  - Audio search

- **Techniques**:
  - Mel-spectrogram features
  - Self-supervised pre-training
  - Task-specific fine-tuning

### Video Embeddings

- **Architectures**:
  - 3D CNNs (C3D, I3D)
  - Video transformers
  - Frame-based approaches with temporal pooling

- **Challenges**:
  - Temporal dynamics
  - Computational efficiency
  - Long-range dependencies

- **Applications**:
  - Video retrieval
  - Action recognition
  - Video recommendation
  - Scene understanding

### Cross-modal Search

- **Unified Embedding Spaces**:
  - Align different modalities in same vector space
  - Enable cross-modal retrieval (text→image, image→text)
  - Leverage semantic relationships across modalities

- **Training Strategies**:
  - Contrastive learning across modalities
  - Joint embedding alignment
  - Teacher-student architectures

- **Implementation Patterns**:
  - Dual encoders with shared projection
  - Late fusion of modality-specific embeddings
  - Multimodal transformers
